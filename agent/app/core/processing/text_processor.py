"""
í…ìŠ¤íŠ¸ ì²˜ë¦¬ ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ
RAG ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©ë˜ëŠ” í…ìŠ¤íŠ¸ ì²˜ë¦¬, ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±, ì¤‘ë³µ ì œê±° ë“±ì˜ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""
import re
import logging
from typing import List, Dict, Any

from app.models.schemas import SearchResult

logger = logging.getLogger(__name__)


class TextProcessor:
    """í…ìŠ¤íŠ¸ ì²˜ë¦¬ ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    # í•œêµ­ì–´ ë¶ˆìš©ì–´ ëª©ë¡
    KOREAN_STOPWORDS = {
        'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë¡œ', 'ì™€', 'ê³¼',
        'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ë¼ê³ ', 'í•˜ê³ '
    }
    
    # ì˜ì–´ ë¶ˆìš©ì–´ ëª©ë¡
    ENGLISH_STOPWORDS = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 
        'to', 'for', 'of', 'with', 'by', 'from', 'as', 'is', 'was'
    }
    
    def build_context(self, search_results: List[SearchResult]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not search_results:
            return ""
        
        context_parts = []
        for i, result in enumerate(search_results, 1):
            metadata = result.metadata
            
            # ì»¨í…ìŠ¤íŠ¸ í—¤ë” êµ¬ì„±
            context_header = f"[ë¬¸ì„œ {i}"
            if metadata.get("page"):
                context_header += f", í˜ì´ì§€ {metadata['page']}"
            if metadata.get("type"):
                context_header += f", {metadata['type']}"
            if metadata.get("file_path"):
                file_name = metadata["file_path"].split("/")[-1]
                context_header += f", ì¶œì²˜: {file_name}"
            context_header += "]"
            
            # ë‚´ìš© ì¶”ê°€
            content = result.content.strip()
            if len(content) > 1000:  # ê¸´ ë‚´ìš©ì€ ìš”ì•½
                content = content[:1000] + "..."
            
            context_part = f"{context_header}\n{content}\n"
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    def remove_duplicate_content(self, answer: str) -> str:
        """ë‹µë³€ì—ì„œ ì¤‘ë³µëœ ë‚´ìš©ì„ ì œê±°í•˜ê³  í‘œë¥¼ ì¬í¬ë§·íŒ…"""
        # ë¨¼ì € í‘œ ì¬í¬ë§·íŒ…
        answer = self._reformat_markdown_table(answer)
        
        # ì„¹ì…˜ë³„ë¡œ ë‚˜ëˆ„ê¸° (ì˜ˆ: **ê°„ë‹¨í•œ ë‹µë³€:**, **ìì„¸í•œ ì„¤ëª…:** ë“±)
        sections = re.split(r'(\*\*.*?\*\*:)', answer)
        
        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì§‘í•©
        seen_lines = set()
        cleaned_sections = []
        
        for section in sections:
            if section.startswith('**') and section.endswith('**:'):
                # ì„¹ì…˜ í—¤ë”ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                cleaned_sections.append(section)
                seen_lines.clear()  # ì„¹ì…˜ ë°”ë€” ë•Œë§ˆë‹¤ ì´ˆê¸°í™”
            else:
                # ì„¹ì…˜ ë‚´ìš©ì—ì„œ ì¤‘ë³µ ë¼ì¸ ì œê±°
                lines = section.split('\n')
                unique_lines = []
                for line in lines:
                    line_stripped = line.strip()
                    if line_stripped and line_stripped not in seen_lines:
                        unique_lines.append(line)
                        seen_lines.add(line_stripped)
                    elif not line_stripped:
                        unique_lines.append(line)  # ë¹ˆ ì¤„ì€ ìœ ì§€
                cleaned_sections.append('\n'.join(unique_lines))
        
        return ''.join(cleaned_sections).strip()

    def _reformat_markdown_table(self, text: str) -> str:
        """Markdown í‘œë¥¼ ì°¾ì•„ì„œ ì •ë ¬ëœ í‘œë¡œ ì¬í¬ë§·íŒ…"""
        lines = text.split('\n')
        table_start = -1
        table_end = -1
        
        for i, line in enumerate(lines):
            if line.strip().startswith('|') and '|' in line:
                if table_start == -1:
                    table_start = i
                table_end = i
            elif table_start != -1 and not line.strip().startswith('|'):
                break
        
        if table_start == -1 or table_end - table_start < 2:
            return text  # í‘œê°€ ì—†ê±°ë‚˜ ë„ˆë¬´ ì‘ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        
        # í‘œ ë¼ì¸ë“¤ ì¶”ì¶œ
        table_lines = lines[table_start:table_end + 1]
        
        # ê° ì—´ì˜ ë‚´ìš© ì¶”ì¶œ
        columns = []
        for line in table_lines:
            cells = [cell.strip() for cell in line.split('|')[1:-1]]
            columns.append(cells)
        
        if not columns:
            return text
        
        # ê° ì—´ì˜ ìµœëŒ€ ë„ˆë¹„ ê³„ì‚°
        max_widths = []
        for col_idx in range(len(columns[0])):
            max_width = 0
            for row in columns:
                if col_idx < len(row):
                    max_width = max(max_width, len(row[col_idx]))
            max_widths.append(max_width)
        
        # í‘œ ì¬êµ¬ì„±
        formatted_lines = []
        for row_idx, row in enumerate(columns):
            formatted_cells = []
            for col_idx, cell in enumerate(row):
                if col_idx < len(max_widths):
                    formatted_cells.append(cell.center(max_widths[col_idx]))
                else:
                    formatted_cells.append(cell)
            
            formatted_line = '| ' + ' | '.join(formatted_cells) + ' |'
            formatted_lines.append(formatted_line)
            
            # í—¤ë” ë‹¤ìŒì— êµ¬ë¶„ì„  ì¶”ê°€
            if row_idx == 0:
                separator_cells = ['-' * max_widths[col_idx] for col_idx in range(len(max_widths))]
                separator_line = '| ' + ' | '.join(separator_cells) + ' |'
                formatted_lines.append(separator_line)
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ì— ì¬ì‚½ì…
        new_lines = lines[:table_start] + formatted_lines + lines[table_end + 1:]
        return '\n'.join(new_lines)

    def parse_document_sections(self, context: str) -> List[Dict[str, str]]:
        """ë¬¸ì„œ ì„¹ì…˜ íŒŒì‹±"""
        sections = []
        lines = context.strip().split('\n')
        
        current_header = ""
        current_content = []
        
        for line in lines:
            if line.startswith('[ë¬¸ì„œ'):
                if current_header and current_content:
                    sections.append({
                        'header': current_header,
                        'content': '\n'.join(current_content).strip()
                    })
                current_header = line.strip('[]')
                current_content = []
            elif line.strip():
                current_content.append(line)
        
        if current_header and current_content:
            sections.append({
                'header': current_header,
                'content': '\n'.join(current_content).strip()
            })
        
        return sections

    def extract_keywords(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œêµ­ì–´ ì§€ì›)"""
        # í•œêµ­ì–´, ì˜ì–´, ìˆ«ì ì¡°í•© ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text.lower())
        
        # ë¶ˆìš©ì–´ ì œê±°
        all_stopwords = self.KOREAN_STOPWORDS | self.ENGLISH_STOPWORDS
        keywords = [word for word in words if len(word) > 1 and word not in all_stopwords]
        
        return list(set(keywords))  # ì¤‘ë³µ ì œê±°
    
    def calculate_text_relevance(self, text: str, keywords: List[str]) -> float:
        """í…ìŠ¤íŠ¸ì™€ í‚¤ì›Œë“œ ê°„ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not keywords:
            return 0.0
        
        text_lower = text.lower()
        matches = 0
        
        for keyword in keywords:
            if keyword in text_lower:
                matches += text_lower.count(keyword)
        
        # ë§¤ì¹˜ ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ ê¸¸ì´ë¡œ ì •ê·œí™”
        return matches / max(len(text.split()), 1)
    
    def create_direct_document_answer(self, question: str, context: str) -> str:
        """ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ êµ¬ì¡°í™”í•˜ì—¬ ë‹µë³€ ìƒì„±"""
        if not context.strip():
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."
        
        # ë¬¸ì„œ ì„¹ì…˜ íŒŒì‹±
        document_sections = self.parse_document_sections(context)
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ë†’ì€ ë‚´ìš© ìš°ì„  ë°°ì¹˜
        question_keywords = self.extract_keywords(question)
        scored_sections = []
        
        for section in document_sections:
            relevance_score = self.calculate_text_relevance(
                section['content'], 
                question_keywords
            )
            scored_sections.append((relevance_score, section))
        
        # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
        scored_sections.sort(key=lambda x: x[0], reverse=True)
        
        # ë‹µë³€ êµ¬ì„±
        answer_parts = [
            f"**'{question}'** ì§ˆë¬¸ê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n"
        ]
        
        for i, (score, section) in enumerate(scored_sections[:5]):
            answer_parts.append(f"**{section['header']}**")
            content = section['content']
            if len(content) > 1200:
                sentences = content.split('.')
                if len(sentences) > 3:
                    content = '. '.join(sentences[:int(len(sentences)*0.7)]) + "...\n\n(ì¶”ê°€ ë‚´ìš© ìˆìŒ)"
                else:
                    content = content[:1200] + "..."
            answer_parts.append(content)
            answer_parts.append("")
        
        if len(scored_sections) > 5:
            answer_parts.append(f"ğŸ“‹ ì¶”ê°€ë¡œ {len(scored_sections) - 5}ê°œì˜ ê´€ë ¨ ë¬¸ì„œê°€ ë” ìˆìŠµë‹ˆë‹¤.")
        
        answer_parts.append("ğŸ’¡ **AI ë¶„ì„ì´ ì¼ì‹œì ìœ¼ë¡œ ì œí•œë˜ì–´ ì›ë³¸ ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ ì œê³µí–ˆìŠµë‹ˆë‹¤.**")
        
        return "\n".join(answer_parts)
    
    def create_llm_free_summary(self, question: str, context: str) -> str:
        """LLM ì—†ì´ ì§ˆë¬¸ì— ë§ëŠ” ë¬¸ì„œ ìš”ì•½ (ìµœì¢… fallback)"""
        if not context.strip():
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        sections = self.parse_document_sections(context)
        
        answer_parts = [
            f"# ğŸ“„ '{question}' ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©\n",
            "âš ï¸ **AI ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í•˜ì—¬ ì›ë³¸ ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ ì œê³µí•©ë‹ˆë‹¤.**\n"
        ]
        
        for i, section in enumerate(sections[:5], 1):
            answer_parts.append(f"## {i}. {section['header']}")
            
            content = section['content']
            if len(content) > 1000:
                content = content[:1000] + "\n\n... (ë‚´ìš©ì´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë¨)"
            
            answer_parts.append(content)
            answer_parts.append("")
        
        if len(sections) > 5:
            answer_parts.append(f"ğŸ“‹ **ì¶”ê°€ë¡œ {len(sections) - 5}ê°œì˜ ë¬¸ì„œ ì„¹ì…˜ì´ ë” ìˆìŠµë‹ˆë‹¤.**")
        
        answer_parts.append("---")
        answer_parts.append("ğŸ’¡ **ë” ì •í™•í•œ ë‹µë³€ì„ ì›í•˜ì‹œë©´ êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.**")
        
        return self.remove_duplicate_content("\n".join(answer_parts))


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
text_processor = TextProcessor()
