import logging
from typing import List, Dict, Any, Optional
import asyncio
import time

from app.core.embedding_manager import embedding_manager
from app.core.vector_store import vector_store
from app.core.gemini_service import gemini_service
from app.models.schemas import QueryRequest, QueryResponse, SearchResult
from app.models.enums import EmbeddingModelType
from app.core.config import settings

logger = logging.getLogger(__name__)


class RAGEngine:
    """RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ ì—”ì§„ - ê²€ìƒ‰ê³¼ ìƒì„±ì„ í†µí•© ê´€ë¦¬"""
    
    def __init__(self):
        self.embedding_manager = embedding_manager
        self.vector_store = vector_store
        self.gemini_service = gemini_service
        self._initialized = False
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ (ë©”ëª¨ë¦¬ ê¸°ë°˜, ì‹¤ì œ ìš´ì˜ì‹œì—ëŠ” Redis/DB ì‚¬ìš© ê¶Œì¥)
        self.conversation_history: Dict[str, List[Dict[str, Any]]] = {}
    
    async def initialize(self):
        """RAG ì—”ì§„ ì´ˆê¸°í™”"""
        if self._initialized:
            return
        
        logger.info("RAG ì—”ì§„ ì´ˆê¸°í™” ì‹œì‘...")
        
        try:
            # ëª¨ë“  ì„œë¹„ìŠ¤ ë³‘ë ¬ ì´ˆê¸°í™” (GeminiëŠ” ì—°ê²° í…ŒìŠ¤íŠ¸ ë¹„í™œì„±í™”)
            await asyncio.gather(
                self.embedding_manager.initialize(),
                self.vector_store.initialize(),
                self.gemini_service.initialize(test_connection=False)
            )
            
            self._initialized = True
            logger.info("RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"RAG ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def query(self, request: QueryRequest) -> QueryResponse:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        conversation_key = f"{request.user_id}_{request.conversation_id or 'default'}"
        if conversation_key not in self.conversation_history:
            self.conversation_history[conversation_key] = []
        
        try:
            if not self._initialized:
                await self.initialize()
            
            logger.info(f"RAG ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: '{request.question[:50]}...' (ì‚¬ìš©ì: {request.user_id})")
            
            # 1. ëŒ€í™” ë§¥ë½ ë¶„ì„ ë° ì§ˆë¬¸ ë³´ì™„
            context_aware_question, is_contextual = await self._analyze_question_context(
                request.question, conversation_key
            )
            
            # 2. ë©”íƒ€ ì§ˆë¬¸ ê°ì§€ ë° ì²˜ë¦¬ (ë§¥ë½ ì¸ì‹ëœ ì§ˆë¬¸ìœ¼ë¡œ)
            meta_response = await self._handle_meta_questions(context_aware_question, request.user_id)
            if meta_response:
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                self._add_to_history(conversation_key, request.question, meta_response, [])
                return QueryResponse(
                    answer=meta_response,
                    sources=[],
                    confidence=0.9,
                    processing_time=time.time() - start_time
                )
            
            # 3. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜ (ë§¥ë½ ì¸ì‹ëœ ì§ˆë¬¸ ì‚¬ìš©)
            question_embedding = await self._embed_question(context_aware_question)
            
            # 4. ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
            search_results = await self._vector_search(
                question_embedding=question_embedding,
                user_id=request.user_id,
                limit=request.max_results,
                score_threshold=request.score_threshold
            )
            
            # 5. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¡œ ì „í™˜
            if not search_results:
                return await self._handle_general_conversation(request.question, time.time() - start_time)
            
            # 6. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(search_results)
            
            # 7. Geminië¥¼ í™œìš©í•œ ì§€ëŠ¥ì  ë‹µë³€ ìƒì„± (ì›ë³¸ ì§ˆë¬¸ê³¼ ë§¥ë½, íˆìŠ¤í† ë¦¬ ëª¨ë‘ ì „ë‹¬)
            answer = await self._generate_intelligent_answer(
                request.question, 
                context, 
                context_aware_question,
                self._get_recent_history(conversation_key)
            )
            
            # 8. ì‘ë‹µ êµ¬ì„±
            processing_time = time.time() - start_time
            confidence = self._calculate_confidence(search_results)
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self._add_to_history(conversation_key, request.question, answer, search_results)
            
            response = QueryResponse(
                answer=answer,
                sources=search_results,
                confidence=confidence,
                processing_time=processing_time
            )
            
            logger.info(f"RAG ì¿¼ë¦¬ ì™„ë£Œ: {processing_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {confidence:.2f}")
            return response
            
        except Exception as e:
            logger.error(f"RAG ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return QueryResponse(
                answer="ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                sources=[],
                confidence=0.0,
                processing_time=time.time() - start_time
            )
    
    async def _handle_meta_questions(self, question: str, user_id: str) -> Optional[str]:
        """ë©”íƒ€ ì§ˆë¬¸ ë° ì¼ë°˜ ëŒ€í™”ë¥¼ LLMìœ¼ë¡œ íŒë‹¨ í›„ ì²˜ë¦¬"""
        try:
            # LLMìœ¼ë¡œ ì§ˆë¬¸ ìœ í˜• ë¶„ì„
            classification = await self._classify_question_with_llm(question)
            
            if classification == "GREETING":
                return await self._handle_greeting_with_llm(question)
            elif classification == "DOCUMENT_LIST":
                return await self._handle_document_list_request(user_id)
            elif classification == "SYSTEM_STATUS":
                return await self._handle_system_status_request()
            else:
                return None  # RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì§„í–‰
                
        except Exception as e:
            logger.warning(f"ë©”íƒ€ ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None  # ì˜¤ë¥˜ ì‹œ RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì§„í–‰
    
    async def _classify_question_with_llm(self, question: str) -> str:
        """LLMìœ¼ë¡œ ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
        prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

"{question}"

ë¶„ë¥˜ ê¸°ì¤€:
- GREETING: ë‹¨ìˆœí•œ ì¸ì‚¬, ì•ˆë¶€ ë“± (ì˜ˆ: ì•ˆë…•, í•˜ì´, ì˜ ì§€ë‚´?)
- DOCUMENT_LIST: ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ ìš”ì²­ (ì˜ˆ: ë¬¸ì„œ ëª©ë¡, ì–´ë–¤ íŒŒì¼ë“¤ì´ ìˆì–´?)  
- SYSTEM_STATUS: ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ìš”ì²­ (ì˜ˆ: ì‹œìŠ¤í…œ ìƒíƒœ, ì •ìƒ ì‘ë™?)
- DOCUMENT_QUERY: ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•œ ì§ˆë¬¸ (ì˜ˆ: ì—¬ë¹„ ê·œì •, ì¼ë¹„ ì–¼ë§ˆ?)

ìœ„ 4ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”: GREETING, DOCUMENT_LIST, SYSTEM_STATUS, DOCUMENT_QUERY"""

        try:
            if hasattr(self.gemini_service, 'model') and self.gemini_service.model:
                def classify():
                    response = self.gemini_service.model.generate_content(prompt)
                    result = response.text.strip().upper()
                    # ìœ íš¨í•œ ë¶„ë¥˜ë§Œ ë°˜í™˜
                    valid_types = ["GREETING", "DOCUMENT_LIST", "SYSTEM_STATUS", "DOCUMENT_QUERY"]
                    for valid_type in valid_types:
                        if valid_type in result:
                            return valid_type
                    return "DOCUMENT_QUERY"  # ê¸°ë³¸ê°’
                
                return await asyncio.to_thread(classify)
        except Exception as e:
            logger.warning(f"ì§ˆë¬¸ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
        
        return "DOCUMENT_QUERY"  # LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
    
    async def _handle_greeting_with_llm(self, question: str) -> str:
        """LLMìœ¼ë¡œ ì¸ì‚¬ ì‘ë‹µ ìƒì„±"""
        prompt = f"""ì‚¬ìš©ìê°€ "{question}"ë¼ê³  ë§í–ˆìŠµë‹ˆë‹¤.

RAG ë¬¸ì„œ ê²€ìƒ‰ AI ì–´ì‹œìŠ¤í„´íŠ¸ë¡œì„œ ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
- 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ
- ë¬¸ì„œ ì—…ë¡œë“œì™€ ì§ˆë¬¸ì„ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„
- ê³¼ë„í•˜ê²Œ ê¸¸ê±°ë‚˜ ë³µì¡í•˜ì§€ ì•Šê²Œ"""

        try:
            if hasattr(self.gemini_service, 'model') and self.gemini_service.model:
                def generate():
                    response = self.gemini_service.model.generate_content(prompt)
                    return response.text.strip()
                
                return await asyncio.to_thread(generate)
        except Exception as e:
            logger.warning(f"ì¸ì‚¬ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # LLM ì‹¤íŒ¨ ì‹œ ìµœì†Œí•œì˜ ê¸°ë³¸ ì‘ë‹µ
        return "ì•ˆë…•í•˜ì„¸ìš”! ë¬¸ì„œ ê´€ë ¨ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ë„ì™€ë“œë¦´ê²Œìš”."
    
    async def _handle_document_list_request(self, user_id: str) -> str:
        """ë¬¸ì„œ ëª©ë¡ ìš”ì²­ ì²˜ë¦¬"""
        try:
            collections = await self.vector_store.list_user_documents(user_id)
            if not collections:
                return "í˜„ì¬ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            
            doc_list = []
            for i, doc in enumerate(collections, 1):
                doc_list.append(f"{i}. {doc.get('file_name', 'Unknown')}")
            
            return f"ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡:\n" + "\n".join(doc_list)
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return "ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    async def _handle_system_status_request(self) -> str:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ìš”ì²­ ì²˜ë¦¬"""
        try:
            status = await self.health_check()
            if status.get('rag_engine') == 'healthy':
                return "âœ… RAG ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤."
            else:
                return f"âš ï¸ ì‹œìŠ¤í…œ ìƒíƒœ: {status.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        except Exception as e:
            return f"âŒ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}"

    async def _embed_question(self, question: str) -> List[float]:
        """ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        try:
            return await self.embedding_manager.embed_text(
                question, 
                EmbeddingModelType.KOREAN
            )
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _vector_search(
        self,
        question_embedding: List[float],
        user_id: str,
        limit: int,
        score_threshold: float
    ) -> List[SearchResult]:
        """ë‹¤ì¤‘ ì „ëµ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            collection_name = f"documents_{user_id}"
            
            # ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼
            primary_results = await self.vector_store.search_similar(
                collection_name=collection_name,
                query_vector=question_embedding,
                limit=limit,
                score_threshold=score_threshold
            )
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€ ê²€ìƒ‰ ì „ëµ ì ìš©
            if len(primary_results) < 3:
                # ë” ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ì¶”ê°€ ê²€ìƒ‰
                additional_results = await self.vector_store.search_similar(
                    collection_name=collection_name,
                    query_vector=question_embedding,
                    limit=limit * 2,
                    score_threshold=max(0.3, score_threshold - 0.2)
                )
                
                # ì¤‘ë³µ ì œê±°í•˜ë©° ê²°ê³¼ í•©ì¹˜ê¸°
                seen_ids = {r.chunk_id for r in primary_results}
                for result in additional_results:
                    if result.chunk_id not in seen_ids and len(primary_results) < limit:
                        primary_results.append(result)
                        seen_ids.add(result.chunk_id)
            
            return primary_results
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _build_context(self, search_results: List[SearchResult]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not search_results:
            return ""
        
        context_parts = []
        for i, result in enumerate(search_results, 1):
            metadata = result.metadata
            
            # ì»¨í…ìŠ¤íŠ¸ í—¤ë” êµ¬ì„±
            context_header = f"[ë¬¸ì„œ {i}"
            if metadata.get("page"):
                context_header += f", í˜ì´ì§€ {metadata['page']}"
            if metadata.get("type"):
                context_header += f", {metadata['type']}"
            if metadata.get("file_path"):
                file_name = metadata["file_path"].split("/")[-1]
                context_header += f", ì¶œì²˜: {file_name}"
            context_header += "]"
            
            # ë‚´ìš© ì¶”ê°€
            content = result.content.strip()
            if len(content) > 1000:  # ê¸´ ë‚´ìš©ì€ ìš”ì•½
                content = content[:1000] + "..."
            
            context_part = f"{context_header}\n{content}\n"
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    async def _generate_intelligent_answer(
        self, 
        original_question: str, 
        context: str, 
        enhanced_question: str = None, 
        history: List[Dict[str, Any]] = None
    ) -> str:
        """LLMì´ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ì¶”ë¡  ê°•í™”ëœ ì§€ëŠ¥ì  ë‹µë³€ ìƒì„±"""
        
        # 1ë‹¨ê³„: ë§¥ë½ì´ ìˆëŠ” Gemini API í˜¸ì¶œ
        try:
            # íˆìŠ¤í† ë¦¬ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            history_context = ""
            if history:
                history_context = f"\n\nì´ì „ ëŒ€í™”:\n{self._format_history_for_prompt(history)}"
            
            # ì§ˆë¬¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            question_to_use = enhanced_question or original_question
            context_note = ""
            if enhanced_question and enhanced_question != original_question:
                context_note = f"\n\nì›ë˜ ì§ˆë¬¸: '{original_question}'\në§¥ë½ ë³´ì™„ëœ ì§ˆë¬¸: '{enhanced_question}'"
            
            # ì¶”ë¡  ê°•í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
            enhanced_prompt = self._create_reasoning_prompt(
                question_to_use, context, history_context + context_note
            )
            
            answer = await self.gemini_service.generate_answer(
                question=enhanced_prompt,
                context="",  # í”„ë¡¬í”„íŠ¸ì— ì´ë¯¸ í¬í•¨ë¨
                max_tokens=3000,
                temperature=0.2
            )
            
            # ë‹µë³€ í’ˆì§ˆ ê²€ì¦
            if self._is_answer_complete(answer, original_question):
                return self._remove_duplicate_content(answer)
            else:
                logger.warning("Gemini ë‹µë³€ì´ ë¶ˆì™„ì „í•¨, ë³´ì™„ëœ ë‹µë³€ ìƒì„±")
                enhanced_answer = await self._create_enhanced_gemini_answer(
                    original_question, context, answer, history_context + context_note
                )
                return self._remove_duplicate_content(enhanced_answer)
                
        except Exception as e:
            logger.warning(f"ë§¥ë½ ì¸ì‹ Gemini API ì‹¤íŒ¨: {e}")
            
            # 2ë‹¨ê³„: ë‹¨ìˆœ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
            try:
                simple_answer = await self._generate_simple_gemini_answer(question_to_use, context)
                if len(simple_answer.strip()) > 50:
                    return self._remove_duplicate_content(simple_answer)
            except Exception as e2:
                logger.warning(f"Gemini API 2ì°¨ ì‹¤íŒ¨: {e2}")
            
            # 3ë‹¨ê³„: LLM ê¸°ë°˜ êµ¬ì¡°í™”ëœ fallback
            fallback_answer = await self._create_llm_guided_fallback(question_to_use, context)
            return self._remove_duplicate_content(fallback_answer)
    
    def _create_reasoning_prompt(self, question: str, context: str, additional_context: str = "") -> str:
        """ì¶”ë¡  ê°•í™” í”„ë¡¬í”„íŠ¸ ìƒì„± - ì¼ë°˜ì¸ ì¹œí™”ì  ë‹µë³€"""
        return f"""ë‹¹ì‹ ì€ ì¹œì ˆí•œ íšŒì‚¬ ê·œì • ì•ˆë‚´ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ë³µì¡í•œ ê·œì •ì„ ì¼ë°˜ ì§ì›ë“¤ì´ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ëª…í•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. **ì •ë³´ ìˆ˜ì§‘**: ë¬¸ì„œì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ëª¨ë“  ì •ë³´ë¥¼ ì°¾ì•„ë³´ì„¸ìš”.
2. **ì—°ê²° ë¶„ì„**: ì„œë¡œ ë‹¤ë¥¸ ë¶€ë¶„ì— ìˆëŠ” ì •ë³´ë“¤ì„ ì—°ê²°í•´ì„œ ë¶„ì„í•´ë³´ì„¸ìš”.
3. **ì‰¬ìš´ ì„¤ëª…**: ë³µì¡í•œ ê·œì • ìš©ì–´ë¥¼ ì¼ìƒ ì–¸ì–´ë¡œ ë°”ê¿”ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
4. **ëª…í™•í•œ ê²°ë¡ **: ì§ˆë¬¸ìê°€ ì›í•˜ëŠ” ë‹µì„ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:
{context}

{additional_context}

ë‹µë³€í•  ë•Œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
âœ… **ì‰¬ìš´ ì–¸ì–´ ì‚¬ìš©**: ë²•ë¥  ìš©ì–´ë‚˜ ë³µì¡í•œ í‘œí˜„ ëŒ€ì‹  ì¼ìƒ ì–¸ì–´ë¡œ ì„¤ëª…
âœ… **êµ¬ì²´ì ì¸ ì˜ˆì‹œ**: ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ìƒí™© ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…
âœ… **í•µì‹¬ë§Œ ê°„ë‹¨íˆ**: ë¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ì€ ìƒëµí•˜ê³  í•µì‹¬ë§Œ ì „ë‹¬
âœ… **í™•ì‹¤í•œ ì •ë³´ë§Œ**: ì¶”ì¸¡ì´ë‚˜ ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ ëª…ì‹œ
âœ… **ì¹œê·¼í•œ í†¤**: ë”±ë”±í•œ ê³µì‹ ë¬¸ì„œ í†¤ì´ ì•„ë‹Œ ì¹œê·¼í•œ ì„¤ëª… í†¤ ì‚¬ìš©
âœ… **ì¤‘ë³µ ë°©ì§€**: ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , í•œ ë²ˆì— ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ì´ì „ ë‹µë³€ê³¼ ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì€ í”¼í•˜ì„¸ìš”.
âœ… **ì¼ê´€ì„± ìœ ì§€**: í•˜ë‚˜ì˜ ë‹µë³€ìœ¼ë¡œ ì™„ì„±í•˜ì„¸ìš”. ì—¬ëŸ¬ ë²„ì „ì˜ ë‹µë³€ì„ ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.
âœ… **ì™„ì „í•œ ë‹µë³€**: ì§ˆë¬¸ì˜ ìœ í˜•ì— ë”°ë¼ í•„ìš”í•œ ëª¨ë“  ì„¸ë¶€ì‚¬í•­(ì¼ìˆ˜, ì ˆì°¨, ì¡°ê±´, ì„œë¥˜ ë“±)ì„ í¬í•¨í•´ì„œ ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.
âœ… **í‘œ í¬ë§·íŒ…**: í‘œë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” Markdown í‘œ í˜•ì‹ì„ ì •í™•íˆ ì‚¬ìš©í•˜ê³ , ê° ì—´ì˜ ë„ˆë¹„ë¥¼ ë°ì´í„°ì— ë§ê²Œ ì¡°ì •í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”. í—¤ë”ì™€ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ê³µë°±ì„ ì¶”ê°€í•˜ì„¸ìš”.

ë‹µë³€ í˜•ì‹:
**ê°„ë‹¨í•œ ë‹µë³€:**
[ì§ˆë¬¸ì— ëŒ€í•œ í•µì‹¬ ë‹µë³€ì„ 1-2ë¬¸ì¥ìœ¼ë¡œ]

**ìì„¸í•œ ì„¤ëª…:**
[ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•œ ë‚´ìš©]

**ì˜ˆì‹œ:** (í•´ë‹¹ë˜ëŠ” ê²½ìš°)
[êµ¬ì²´ì ì¸ ìƒí™© ì˜ˆì‹œ]

**ì£¼ì˜ì‚¬í•­:** (í•„ìš”í•œ ê²½ìš°)
[ì•Œì•„ë‘ë©´ ì¢‹ì„ ì¶”ê°€ ì •ë³´]"""
    
    def _is_answer_complete(self, answer: str, question: str) -> bool:
        """ë‹µë³€ ì™„ì„±ë„ ê²€ì¦"""
        if not answer or len(answer.strip()) < 100:
            return False
        
        # ì¤‘ê°„ì— ëŠì–´ì§„ ê²ƒ ê°™ì€ íŒ¨í„´ ì²´í¬
        if answer.strip().endswith(('*', ':', '(', '-', ',', 'ë°')):
            return False
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œì™€ ë‹µë³€ ê´€ë ¨ì„± ì²´í¬
        question_keywords = question.lower().split()
        answer_lower = answer.lower()
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¤‘ ì¼ë¶€ë¼ë„ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        keyword_match = any(keyword in answer_lower for keyword in question_keywords if len(keyword) > 2)
        
        return keyword_match and len(answer.strip()) > 100
    
    async def _generate_simple_gemini_answer(self, question: str, context: str) -> str:
        """ë‹¨ìˆœí•œ í”„ë¡¬í”„íŠ¸ë¡œ Gemini ë‹µë³€ ìƒì„±"""
        simple_prompt = f"""ì§ˆë¬¸: {question}

ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:
{context[:2000]}

ìœ„ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”."""

        def generate():
            response = self.model.generate_content(
                simple_prompt,
                generation_config=self.gemini_service._create_generation_config(2000, 0.3)
            )
            return response.text
        
        return self._remove_duplicate_content(await asyncio.to_thread(generate))
    
    async def _create_enhanced_gemini_answer(
        self, 
        question: str, 
        context: str, 
        partial_answer: str, 
        additional_context: str = ""
    ) -> str:
        """ë¶ˆì™„ì „í•œ Gemini ë‹µë³€ì„ ë³´ì™„ (ëŒ€í™” ë§¥ë½ í¬í•¨)"""
        try:
            enhancement_prompt = f"""ë‹¤ìŒì€ ì§ˆë¬¸ì— ëŒ€í•œ ë¶€ë¶„ì ì¸ ë‹µë³€ì…ë‹ˆë‹¤. ì´ë¥¼ ì™„ì„±í•˜ê³  ë³´ì™„í•´ì„œ ì™„ì „í•œ ë‹µë³€ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {question}
ë¶€ë¶„ ë‹µë³€: {partial_answer}

ì¶”ê°€ ë¬¸ì„œ ë‚´ìš©:
{context}

{additional_context}

ì¤‘ìš” ì§€ì‹œì‚¬í•­:
- ë¶€ë¶„ ë‹µë³€ì˜ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ê³ , ë¶€ì¡±í•œ ë¶€ë¶„ë§Œ ì±„ì›Œì„œ ì™„ì „í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”
- ì´ë¯¸ í¬í•¨ëœ ì •ë³´ëŠ” ìƒëµí•˜ê³  ìƒˆë¡œìš´ ì •ë³´ë§Œ ì¶”ê°€í•˜ì„¸ìš”
- í•˜ë‚˜ì˜ ì¼ê´€ëœ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. ì—¬ëŸ¬ ë²„ì „ì˜ ë‹µë³€ì´ë‚˜ ì¤‘ë³µëœ ì†Œê°œë¥¼ í”¼í•˜ì„¸ìš”
- ì¹œì ˆí•˜ê³  ì¼ê´€ëœ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”"""

            enhanced = await self.gemini_service.generate_with_system_prompt(
                system_prompt="""ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¶ˆì™„ì „í•œ ë‹µë³€ì„ ì™„ì„±í•˜ëŠ” ê²ƒì´ ì„ë¬´ì…ë‹ˆë‹¤.
ì¤‘ìš”: ì¤‘ë³µì„ í”¼í•˜ê³ , í•˜ë‚˜ì˜ ì™„ì „í•œ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. ë¶€ë¶„ ë‹µë³€ì˜ ë‚´ìš©ì„ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”.""",
                user_message=enhancement_prompt,
                max_tokens=2000,
                temperature=0.1
            )
            
            return enhanced  # ë³´ì™„ëœ ë‹µë³€ë§Œ ë°˜í™˜
            
        except Exception as e:
            logger.warning(f"ë‹µë³€ ë³´ì™„ ì‹¤íŒ¨: {e}")
            return partial_answer
    
    async def _create_llm_guided_fallback(self, question: str, context: str) -> str:
        """LLM ê°€ì´ë“œ ê¸°ë°˜ fallback ë‹µë³€"""
        try:
            # ë§ˆì§€ë§‰ ì‹œë„: ë§¤ìš° ê°„ë‹¨í•œ ì§€ì‹œë¬¸ìœ¼ë¡œ
            fallback_prompt = f"""ë¬¸ì„œì—ì„œ '{question}' ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ë‹µë³€í•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context[:1500]}

ë‹µë³€:"""
            
            # ì§ì ‘ ëª¨ë¸ í˜¸ì¶œ (ì„œë¹„ìŠ¤ ìš°íšŒ)
            if hasattr(self.gemini_service, 'model') and self.gemini_service.model:
                def generate():
                    response = self.gemini_service.model.generate_content(fallback_prompt)
                    return response.text
                
                result = await asyncio.to_thread(generate)
                if len(result.strip()) > 30:
                    return self._remove_duplicate_content(result)
                    
        except Exception as e:
            logger.warning(f"LLM ê°€ì´ë“œ fallback ì‹¤íŒ¨: {e}")
        
        # ìµœì¢… fallback: LLMì´ ì™„ì „íˆ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©ë˜ëŠ” ë‹¨ìˆœí•œ ë¬¸ì„œ í‘œì‹œ
        return await self._create_llm_free_summary(question, context)
    
    async def _create_llm_free_summary(self, question: str, context: str) -> str:
        """LLM ì—†ì´ ì§ˆë¬¸ì— ë§ëŠ” ë¬¸ì„œ ìš”ì•½ (ìµœì¢… fallback)"""
        if not context.strip():
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì»¨í…ìŠ¤íŠ¸ íŒŒì‹±
        sections = self._parse_document_sections(context)
        
        answer_parts = [
            f"# ğŸ“„ '{question}' ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©\n",
            "âš ï¸ **AI ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í•˜ì—¬ ì›ë³¸ ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ ì œê³µí•©ë‹ˆë‹¤.**\n"
        ]
        
        # ëª¨ë“  ì„¹ì…˜ì„ ìˆœì„œëŒ€ë¡œ í‘œì‹œ (ìµœëŒ€ 5ê°œ)
        for i, section in enumerate(sections[:5], 1):
            answer_parts.append(f"## {i}. {section['header']}")
            
            # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ì ì ˆíˆ ìë¥´ê¸°
            content = section['content']
            if len(content) > 1000:
                content = content[:1000] + "\n\n... (ë‚´ìš©ì´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë¨)"
            
            answer_parts.append(content)
            answer_parts.append("")
        
        # ë” ë§ì€ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ì•ˆë‚´
        if len(sections) > 5:
            answer_parts.append(f"ğŸ“‹ **ì¶”ê°€ë¡œ {len(sections) - 5}ê°œì˜ ë¬¸ì„œ ì„¹ì…˜ì´ ë” ìˆìŠµë‹ˆë‹¤.**")
        
        answer_parts.append("---")
        answer_parts.append("ğŸ’¡ **ë” ì •í™•í•œ ë‹µë³€ì„ ì›í•˜ì‹œë©´ êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”.**")
        
        return self._remove_duplicate_content("\n".join(answer_parts))
    
    def _parse_document_sections(self, context: str) -> list:
        """ë¬¸ì„œ ì„¹ì…˜ íŒŒì‹±"""
        sections = []
        lines = context.strip().split('\n')
        
        current_header = ""
        current_content = []
        
        for line in lines:
            if line.startswith('[ë¬¸ì„œ'):
                if current_header and current_content:
                    sections.append({
                        'header': current_header,
                        'content': '\n'.join(current_content).strip()
                    })
                current_header = line.strip('[]')
                current_content = []
            elif line.strip():
                current_content.append(line)
        
        if current_header and current_content:
            sections.append({
                'header': current_header,
                'content': '\n'.join(current_content).strip()
            })
        
        return sections

    def _create_enhanced_document_answer(self, question: str, context: str) -> str:
        """í–¥ìƒëœ ë¬¸ì„œ ê¸°ë°˜ ì§ì ‘ ë‹µë³€ ìƒì„± (LLM ì‹¤íŒ¨ ì‹œì—ë§Œ ì‚¬ìš©)"""
        if not context.strip():
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”."
        
        # LLMì´ ì™„ì „íˆ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©ë˜ëŠ” ê°„ë‹¨í•œ fallback
        return self._create_smart_document_summary(question, context)

    async def _generate_answer_with_fallback(self, question: str, context: str) -> str:
        """Gemini ìš°ì„ , ì‹¤íŒ¨ ì‹œ ì¦‰ì‹œ ë¬¸ì„œ ë‚´ìš© ê¸°ë°˜ ë‹µë³€ ì œê³µ"""
        try:
            # ë¨¼ì € Gemini APIë¡œ ë‹µë³€ ìƒì„± ì‹œë„ (ë” ê¸´ ë‹µë³€ì„ ìœ„í•´ í† í° ì¦ê°€)
            gemini_answer = await self.gemini_service.generate_answer(
                question=question,
                context=context,
                max_tokens=2000,
                temperature=0.1
            )
            
            # ë‹µë³€ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ëŠì–´ì§„ ê²ƒ ê°™ìœ¼ë©´ fallback ì‚¬ìš©
            if len(gemini_answer.strip()) < 100 or gemini_answer.strip().endswith(('*', ':', '(', '-', '.')):
                logger.warning(f"Gemini ë‹µë³€ì´ ë¶ˆì™„ì „í•¨ (ê¸¸ì´: {len(gemini_answer)}), fallback ì‚¬ìš©")
                return self._create_direct_document_answer(question, context)
            
            return gemini_answer
            
        except Exception as e:
            logger.warning(f"Gemini API ì‹¤íŒ¨, ë¬¸ì„œ ê¸°ë°˜ ì§ì ‘ ë‹µë³€ ì œê³µ: {e}")
            
            # ì¦‰ì‹œ êµ¬ì¡°í™”ëœ ë¬¸ì„œ ë‚´ìš© ì œê³µ
            return self._create_direct_document_answer(question, context)
    
    def _create_direct_document_answer(self, question: str, context: str) -> str:
        """ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ êµ¬ì¡°í™”í•˜ì—¬ ë‹µë³€ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ íŒŒì‹±
        context_lines = context.strip().split('\n')
        document_sections = []
        
        current_doc = ""
        current_content = []
        
        for line in context_lines:
            if line.startswith('[ë¬¸ì„œ'):
                # ì´ì „ ë¬¸ì„œ ë‚´ìš© ì €ì¥
                if current_doc and current_content:
                    content_text = '\n'.join(current_content).strip()
                    if content_text:
                        document_sections.append({
                            'header': current_doc,
                            'content': content_text
                        })
                
                # ìƒˆ ë¬¸ì„œ ì‹œì‘
                current_doc = line.strip('[]')
                current_content = []
            elif line.strip():
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ë¬¸ì„œ ì €ì¥
        if current_doc and current_content:
            content_text = '\n'.join(current_content).strip()
            if content_text:
                document_sections.append({
                    'header': current_doc,
                    'content': content_text
                })
        
        # ì§ˆë¬¸ í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ë†’ì€ ë‚´ìš© ìš°ì„  ë°°ì¹˜
        question_keywords = self._extract_keywords(question)
        scored_sections = []
        
        for section in document_sections:
            relevance_score = self._calculate_text_relevance(
                section['content'], 
                question_keywords
            )
            scored_sections.append((relevance_score, section))
        
        # ê´€ë ¨ì„± ìˆœìœ¼ë¡œ ì •ë ¬
        scored_sections.sort(key=lambda x: x[0], reverse=True)
        
        # ë‹µë³€ êµ¬ì„±
        answer_parts = [
            f"**'{question}'** ì§ˆë¬¸ê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n"
        ]
        
        for i, (score, section) in enumerate(scored_sections[:5]):  # ìƒìœ„ 5ê°œë¡œ ì¦ê°€
            answer_parts.append(f"**{section['header']}**")
            # ë‚´ìš© ê¸¸ì´ ì œí•œì„ ëŠ˜ë¦¬ê³  ë” ë§ì€ ì •ë³´ ì œê³µ
            content = section['content']
            if len(content) > 1200:  # 800ì—ì„œ 1200ìœ¼ë¡œ ì¦ê°€
                # ì¤‘ìš”í•œ ë¶€ë¶„ì„ ì°¾ì•„ì„œ ë” ì§€ëŠ¥ì ìœ¼ë¡œ ìë¥´ê¸°
                sentences = content.split('.')
                if len(sentences) > 3:
                    content = '. '.join(sentences[:int(len(sentences)*0.7)]) + "...\n\n(ì¶”ê°€ ë‚´ìš© ìˆìŒ)"
                else:
                    content = content[:1200] + "..."
            answer_parts.append(content)
            answer_parts.append("")  # êµ¬ë¶„ì„ 
        
        # ì¶”ê°€ ë¬¸ì„œê°€ ìˆìœ¼ë©´ í‘œì‹œ
        if len(scored_sections) > 3:
            answer_parts.append(f"ğŸ“‹ ì¶”ê°€ë¡œ {len(scored_sections) - 3}ê°œì˜ ê´€ë ¨ ë¬¸ì„œê°€ ë” ìˆìŠµë‹ˆë‹¤.")
        
        answer_parts.append("ğŸ’¡ **AI ë¶„ì„ì´ ì¼ì‹œì ìœ¼ë¡œ ì œí•œë˜ì–´ ì›ë³¸ ë¬¸ì„œ ë‚´ìš©ì„ ì§ì ‘ ì œê³µí–ˆìŠµë‹ˆë‹¤.**")
        
        return "\n".join(answer_parts)
    
    def _extract_keywords(self, text: str) -> list:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (í•œêµ­ì–´ ì§€ì›)"""
        import re
        
        # í•œêµ­ì–´, ì˜ì–´, ìˆ«ì ì¡°í•© ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text.lower())
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì˜', 'ë¡œ', 'ì™€', 'ê³¼', 
                    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with'}
        
        keywords = [word for word in words if len(word) > 1 and word not in stopwords]
        return list(set(keywords))  # ì¤‘ë³µ ì œê±°
    
    def _calculate_text_relevance(self, text: str, keywords: list) -> float:
        """í…ìŠ¤íŠ¸ì™€ í‚¤ì›Œë“œ ê°„ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        if not keywords:
            return 0.0
        
        text_lower = text.lower()
        matches = 0
        
        for keyword in keywords:
            if keyword in text_lower:
                matches += text_lower.count(keyword)
        
        # ë§¤ì¹˜ ìˆ˜ë¥¼ í…ìŠ¤íŠ¸ ê¸¸ì´ë¡œ ì •ê·œí™”
        return matches / max(len(text.split()), 1)

    async def _generate_answer(self, question: str, context: str) -> str:
        """Geminië¥¼ ì‚¬ìš©í•´ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        try:
            return await self.gemini_service.generate_answer(
                question=question,
                context=context,
                max_tokens=1000,
                temperature=0.1  # ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ì„ ìœ„í•´ ë‚®ì€ ê°’
            )
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # Gemini API ì‹¤íŒ¨ ì‹œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ì ‘ ë‹µë³€ ìƒì„±
            if "í• ë‹¹ëŸ‰" in str(e) or "quota" in str(e).lower():
                return self._create_fallback_answer_with_context(question, context, "API í• ë‹¹ëŸ‰ ì´ˆê³¼")
            else:
                return self._create_fallback_answer_with_context(question, context, "ë‹µë³€ ìƒì„± ì˜¤ë¥˜")
    
    def _create_fallback_answer_with_context(self, question: str, context: str, error_type: str) -> str:
        """Gemini ì‹¤íŒ¨ ì‹œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ëŒ€ì²´ ë‹µë³€ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        context_lines = context.strip().split('\n')
        formatted_content = []
        
        current_doc = ""
        current_content = []
        
        for line in context_lines:
            if line.startswith('[ë¬¸ì„œ'):
                # ì´ì „ ë¬¸ì„œ ë‚´ìš© ì €ì¥
                if current_doc and current_content:
                    clean_content = '\n'.join(current_content).strip()
                    if clean_content:
                        formatted_content.append(f"**{current_doc}**\n{clean_content}")
                
                # ìƒˆ ë¬¸ì„œ ì‹œì‘
                current_doc = line.strip('[]')
                current_content = []
            elif line.strip():
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ë¬¸ì„œ ë‚´ìš© ì €ì¥
        if current_doc and current_content:
            clean_content = '\n'.join(current_content).strip()
            if clean_content:
                formatted_content.append(f"**{current_doc}**\n{clean_content}")
        
        # ë‹µë³€ êµ¬ì„±
        answer_parts = []
        
        if error_type == "API í• ë‹¹ëŸ‰ ì´ˆê³¼":
            answer_parts.append("í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í•˜ì—¬ AI ë¶„ì„ì€ ì œí•œë˜ì§€ë§Œ, ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì•„ë“œë ¸ìŠµë‹ˆë‹¤.")
        else:
            answer_parts.append("AI ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì§€ë§Œ, ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì•„ë“œë ¸ìŠµë‹ˆë‹¤.")
        
        answer_parts.append("")  # ë¹ˆ ì¤„
        
        if formatted_content:
            for content in formatted_content:
                answer_parts.append(content)
                answer_parts.append("")  # ë¬¸ì„œ ê°„ êµ¬ë¶„
        else:
            answer_parts.append("ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:")
            answer_parts.append(context[:1500] + ("..." if len(context) > 1500 else ""))
        
        return "\n".join(answer_parts)
    
    def _calculate_confidence(self, search_results: List[SearchResult]) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not search_results:
            return 0.0
        
        # ìµœê³  ì ìˆ˜ì™€ í‰ê·  ì ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ ì‹ ë¢°ë„ ê³„ì‚°
        scores = [result.score for result in search_results]
        max_score = max(scores)
        avg_score = sum(scores) / len(scores)
        
        # ê²°ê³¼ ê°œìˆ˜ë„ ê³ ë ¤ (ë” ë§ì€ ê´€ë ¨ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ìƒìŠ¹)
        result_count_factor = min(len(search_results) / 5.0, 1.0)
        
        confidence = (max_score * 0.6 + avg_score * 0.3 + result_count_factor * 0.1)
        return min(confidence, 1.0)
    
    async def _handle_general_conversation(self, question: str, processing_time: float) -> QueryResponse:
        """ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ - RAGê°€ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì§ˆë¬¸ ì‘ë‹µ"""
        try:
            # ì¼ë°˜ì ì¸ ì¸ì‚¬, ì†Œê°œ ë“±ì˜ ì§ˆë¬¸ì„ Geminië¡œ ì²˜ë¦¬
            system_prompt = """
ë‹¹ì‹ ì€ RAG(Retrieval Augmented Generation) ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œê°€ ì—†ì§€ë§Œ, ì¼ë°˜ì ì¸ ëŒ€í™”ëŠ” ê°€ëŠ¥í•©ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì£¼ìš” ê¸°ëŠ¥:
1. ë¬¸ì„œ ì—…ë¡œë“œ ë° ë¶„ì„ (PDF, Word, í…ìŠ¤íŠ¸ íŒŒì¼ ë“±)
2. ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ì •ë³´ ê²€ìƒ‰ ë° ì§ˆì˜ì‘ë‹µ
3. ë‹¤êµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ (í•œêµ­ì–´, ì˜ì–´ ë“±)
4. OCRì„ í†µí•œ ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
5. ë²¡í„° ê²€ìƒ‰ì„ í†µí•œ ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ë§¤ì¹­
6. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
1. ì¸ì‚¬ ì§ˆë¬¸ (ì•ˆë…•, ì•ˆë…•í•˜ì„¸ìš” ë“±): ì¹œê·¼í•˜ê²Œ ì¸ì‚¬í•˜ê³  ìì‹ ì„ RAG ì‹œìŠ¤í…œìœ¼ë¡œ ì†Œê°œ
2. ì •ì²´ì„± ì§ˆë¬¸ (ë„ˆëŠ” ëˆ„êµ¬ì•¼, ë­˜ í•˜ëŠ” AIì•¼ ë“±): RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ AIë¼ê³  êµ¬ì²´ì ìœ¼ë¡œ ì†Œê°œ
3. ê¸°ëŠ¥ ì§ˆë¬¸ (ë­˜ í•  ìˆ˜ ìˆì–´, ì–´ë–¤ ê¸°ëŠ¥ì´ ìˆì–´ ë“±): ìœ„ì˜ ì£¼ìš” ê¸°ëŠ¥ë“¤ì„ ìì„¸íˆ ì„¤ëª…
4. ì‚¬ìš©ë²• ì§ˆë¬¸ (ì–´ë–»ê²Œ ì‚¬ìš©í•´, ë¬¸ì„œëŠ” ì–´ë–»ê²Œ ì˜¬ë ¤ ë“±): ë¬¸ì„œ ì—…ë¡œë“œ ë°©ë²•ê³¼ ì§ˆë¬¸ ë°©ë²• ì•ˆë‚´
5. ì§€ì› íŒŒì¼ ì§ˆë¬¸: PDF, Word, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ íŒŒì¼ ë“± ì§€ì› í˜•ì‹ ì„¤ëª…
6. ê¸°íƒ€ ì¼ë°˜ ì§ˆë¬¸: ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ë˜, ë¬¸ì„œ ì—…ë¡œë“œë¥¼ í†µí•œ ë” ì •í™•í•œ ë‹µë³€ ê°€ëŠ¥ì„± ì–¸ê¸‰

í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ë©° ë„ì›€ì´ ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œê°€ ì—†ëŠ” ìƒí™©ì—ì„œë„ ìµœëŒ€í•œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
"""
            
            answer = await self.gemini_service.generate_with_system_prompt(
                system_prompt=system_prompt,
                user_message=question,
                max_tokens=500,
                temperature=0.7  # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•´ ë†’ì€ ê°’
            )
            
            return QueryResponse(
                answer=answer,
                sources=[],
                confidence=0.8,  # ì¼ë°˜ ëŒ€í™”ëŠ” ë†’ì€ ì‹ ë¢°ë„
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_no_result_response(question, processing_time)
    
    def _create_no_result_response(self, question: str, processing_time: float) -> QueryResponse:
        """ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ì‘ë‹µ (fallback)"""
        return QueryResponse(
            answer="ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ë³´ì‹œê±°ë‚˜, ê´€ë ¨ ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
            sources=[],
            confidence=0.0,
            processing_time=processing_time
        )
    
    async def health_check(self) -> Dict[str, Any]:
        """RAG ì—”ì§„ ìƒíƒœ í™•ì¸"""
        try:
            status = {
                "rag_engine": "healthy",
                "initialized": self._initialized,
                "components": {}
            }
            
            # ê° ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸
            if self._initialized:
                status["components"]["embedding_manager"] = self.embedding_manager.get_model_info()
                status["components"]["gemini_service"] = self.gemini_service.get_service_info()
                
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
                test_embedding = await self.embedding_manager.embed_text("í…ŒìŠ¤íŠ¸")
                status["components"]["embedding_test"] = {
                    "success": len(test_embedding) > 0,
                    "vector_size": len(test_embedding)
                }
            
            return status
            
        except Exception as e:
            logger.error(f"RAG ì—”ì§„ í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
            return {
                "rag_engine": "unhealthy",
                "error": str(e),
                "initialized": self._initialized
            }
    
    async def _analyze_question_context(self, question: str, conversation_key: str) -> tuple[str, bool]:
        """ëŒ€í™” ë§¥ë½ì„ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì„ ë³´ì™„"""
        try:
            history = self._get_recent_history(conversation_key, limit=3)
            if not history:
                return question, False
            
            # LLMìœ¼ë¡œ ë§¥ë½ ë¶„ì„ ë° ì§ˆë¬¸ ë³´ì™„
            prompt = f"""ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ì™€ í˜„ì¬ ì§ˆë¬¸ì„ ë³´ê³  ì§ˆë¬¸ì„ ë³´ì™„í•´ì£¼ì„¸ìš”.

ì´ì „ ëŒ€í™”:
{self._format_history_for_prompt(history)}

í˜„ì¬ ì§ˆë¬¸: "{question}"

ë§Œì•½ í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì™€ ì—°ê´€ëœ ë¶€ê°€ ì§ˆë¬¸ì´ë¼ë©´, ë§¥ë½ì„ í¬í•¨í•˜ì—¬ ì™„ì „í•œ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”.
ì˜ˆ: "ë³„í‘œ1ì´ ë­”ì§€ ëª¨ë¥´ê² ì–´" â†’ "ì—¬ë¹„ ê·œì •ì—ì„œ ì–¸ê¸‰ëœ ë³„í‘œ1ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì•Œë ¤ì£¼ì„¸ìš”"

ë§Œì•½ ë…ë¦½ì ì¸ ì§ˆë¬¸ì´ë¼ë©´ ì›ë˜ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.

ë³´ì™„ëœ ì§ˆë¬¸ë§Œ ë‹µë³€í•˜ì„¸ìš”:"""

            if hasattr(self.gemini_service, 'model') and self.gemini_service.model:
                def analyze():
                    response = self.gemini_service.model.generate_content(prompt)
                    enhanced_question = response.text.strip()
                    # ì›ë³¸ê³¼ ë‹¤ë¥´ë©´ ë§¥ë½ì  ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨
                    is_contextual = enhanced_question != question and len(enhanced_question) > len(question)
                    return enhanced_question, is_contextual
                
                return await asyncio.to_thread(analyze)
        
        except Exception as e:
            logger.warning(f"ì§ˆë¬¸ ë§¥ë½ ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return question, False
    
    def _add_to_history(self, conversation_key: str, question: str, answer: str, sources: List[SearchResult]):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— Q&A ì¶”ê°€"""
        if conversation_key not in self.conversation_history:
            self.conversation_history[conversation_key] = []
        
        entry = {
            "timestamp": time.time(),
            "question": question,
            "answer": answer,
            "sources_count": len(sources),
            "confidence": self._calculate_confidence(sources) if sources else 0.0
        }
        
        self.conversation_history[conversation_key].append(entry)
        
        # íˆìŠ¤í† ë¦¬ í¬ê¸° ì œí•œ (ìµœê·¼ 10ê°œë§Œ ìœ ì§€)
        if len(self.conversation_history[conversation_key]) > 10:
            self.conversation_history[conversation_key] = self.conversation_history[conversation_key][-10:]
    
    def _get_recent_history(self, conversation_key: str, limit: int = 5) -> List[Dict[str, Any]]:
        """ìµœê·¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        if conversation_key not in self.conversation_history:
            return []
        
        history = self.conversation_history[conversation_key]
        return history[-limit:] if len(history) > limit else history
    
    def _format_history_for_prompt(self, history: List[Dict[str, Any]]) -> str:
        """í”„ë¡¬í”„íŠ¸ìš© íˆìŠ¤í† ë¦¬ í¬ë§·íŒ…"""
        formatted = []
        for i, entry in enumerate(history, 1):
            formatted.append(f"Q{i}: {entry['question']}")
            formatted.append(f"A{i}: {entry['answer'][:200]}...")
        
        return "\n".join(formatted)
    
    async def cleanup(self):
        """RAG ì—”ì§„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("RAG ì—”ì§„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
        
        await asyncio.gather(
            self.embedding_manager.cleanup(),
            self.vector_store.cleanup(),
            self.gemini_service.cleanup(),
            return_exceptions=True
        )
        
        self._initialized = False
        logger.info("RAG ì—”ì§„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

    def _remove_duplicate_content(self, answer: str) -> str:
        """ë‹µë³€ì—ì„œ ì¤‘ë³µëœ ë‚´ìš©ì„ ì œê±°í•˜ê³  í‘œë¥¼ ì¬í¬ë§·íŒ…"""
        import re
        
        # ë¨¼ì € í‘œ ì¬í¬ë§·íŒ…
        answer = self._reformat_markdown_table(answer)
        
        # ì„¹ì…˜ë³„ë¡œ ë‚˜ëˆ„ê¸° (ì˜ˆ: **ê°„ë‹¨í•œ ë‹µë³€:**, **ìì„¸í•œ ì„¤ëª…:** ë“±)
        sections = re.split(r'(\*\*.*?\*\*:)', answer)
        
        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì§‘í•©
        seen_lines = set()
        cleaned_sections = []
        
        for section in sections:
            if section.startswith('**') and section.endswith('**:'):
                # ì„¹ì…˜ í—¤ë”ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
                cleaned_sections.append(section)
                seen_lines.clear()  # ì„¹ì…˜ ë°”ë€” ë•Œë§ˆë‹¤ ì´ˆê¸°í™”
            else:
                # ì„¹ì…˜ ë‚´ìš©ì—ì„œ ì¤‘ë³µ ë¼ì¸ ì œê±°
                lines = section.split('\n')
                unique_lines = []
                for line in lines:
                    line_stripped = line.strip()
                    if line_stripped and line_stripped not in seen_lines:
                        unique_lines.append(line)
                        seen_lines.add(line_stripped)
                    elif not line_stripped:
                        unique_lines.append(line)  # ë¹ˆ ì¤„ì€ ìœ ì§€
                cleaned_sections.append('\n'.join(unique_lines))
        
        return ''.join(cleaned_sections).strip()

    def _reformat_markdown_table(self, text: str) -> str:
        """Markdown í‘œë¥¼ ì°¾ì•„ì„œ ì •ë ¬ëœ í‘œë¡œ ì¬í¬ë§·íŒ…"""
        import re
        
        # í‘œ íŒ¨í„´ ì°¾ê¸°: |ë¡œ ì‹œì‘í•˜ëŠ” ë¼ì¸ë“¤
        lines = text.split('\n')
        table_start = -1
        table_end = -1
        
        for i, line in enumerate(lines):
            if line.strip().startswith('|') and '|' in line:
                if table_start == -1:
                    table_start = i
                table_end = i
            elif table_start != -1 and not line.strip().startswith('|'):
                break
        
        if table_start == -1 or table_end - table_start < 2:
            return text  # í‘œê°€ ì—†ê±°ë‚˜ ë„ˆë¬´ ì‘ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        
        # í‘œ ë¼ì¸ë“¤ ì¶”ì¶œ
        table_lines = lines[table_start:table_end + 1]
        
        # ê° ì—´ì˜ ìµœëŒ€ ë„ˆë¹„ ê³„ì‚°
        columns = []
        for line in table_lines:
            cells = [cell.strip() for cell in line.split('|')[1:-1]]  # | ì œê±°í•˜ê³  ì–‘ìª½ ê³µë°± ì œê±°
            columns.append(cells)
        
        if not columns:
            return text
        
        # ê° ì—´ì˜ ìµœëŒ€ ë„ˆë¹„ ê³„ì‚° (í—¤ë” í¬í•¨)
        max_widths = []
        for col_idx in range(len(columns[0])):
            max_width = 0
            for row in columns:
                if col_idx < len(row):
                    max_width = max(max_width, len(row[col_idx]))
            max_widths.append(max_width)
        
        # í‘œ ì¬êµ¬ì„±
        formatted_lines = []
        for row_idx, row in enumerate(columns):
            formatted_cells = []
            for col_idx, cell in enumerate(row):
                if col_idx < len(max_widths):
                    # ì¤‘ì•™ ì •ë ¬ë¡œ í¬ë§·íŒ…
                    formatted_cells.append(cell.center(max_widths[col_idx]))
                else:
                    formatted_cells.append(cell)
            
            # |ë¡œ ë¬¶ì–´ì„œ ë¼ì¸ ìƒì„±
            formatted_line = '| ' + ' | '.join(formatted_cells) + ' |'
            formatted_lines.append(formatted_line)
            
            # í—¤ë” ë‹¤ìŒì— êµ¬ë¶„ì„  ì¶”ê°€ (---|--- í˜•íƒœ)
            if row_idx == 0:
                separator_cells = ['-' * max_widths[col_idx] for col_idx in range(len(max_widths))]
                separator_line = '| ' + ' | '.join(separator_cells) + ' |'
                formatted_lines.append(separator_line)
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ì— ì¬ì‚½ì…
        new_lines = lines[:table_start] + formatted_lines + lines[table_end + 1:]
        return '\n'.join(new_lines)


# ì „ì—­ RAG ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
rag_engine = RAGEngine()