import logging
from typing import List, Dict, Any, Optional
import asyncio
import time

from app.core.embedding_manager import embedding_manager
from app.core.vector_store import vector_store
from app.core.gemini_service import gemini_service
from app.models.schemas import QueryRequest, QueryResponse, SearchResult
from app.models.enums import EmbeddingModelType
from app.core.config import settings

logger = logging.getLogger(__name__)


class RAGEngine:
    """RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ ì—”ì§„ - ê²€ìƒ‰ê³¼ ìƒì„±ì„ í†µí•© ê´€ë¦¬"""
    
    def __init__(self):
        self.embedding_manager = embedding_manager
        self.vector_store = vector_store
        self.gemini_service = gemini_service
        self._initialized = False
    
    async def initialize(self):
        """RAG ì—”ì§„ ì´ˆê¸°í™”"""
        if self._initialized:
            return
        
        logger.info("RAG ì—”ì§„ ì´ˆê¸°í™” ì‹œì‘...")
        
        try:
            # ëª¨ë“  ì„œë¹„ìŠ¤ ë³‘ë ¬ ì´ˆê¸°í™” (GeminiëŠ” ì—°ê²° í…ŒìŠ¤íŠ¸ ë¹„í™œì„±í™”)
            await asyncio.gather(
                self.embedding_manager.initialize(),
                self.vector_store.initialize(),
                self.gemini_service.initialize(test_connection=False)
            )
            
            self._initialized = True
            logger.info("RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!")
            
        except Exception as e:
            logger.error(f"RAG ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def query(self, request: QueryRequest) -> QueryResponse:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        try:
            if not self._initialized:
                await self.initialize()
            
            logger.info(f"RAG ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: '{request.question[:50]}...' (ì‚¬ìš©ì: {request.user_id})")
            
            # 0ë‹¨ê³„: ë©”íƒ€ ì§ˆë¬¸ ê°ì§€ ë° ì²˜ë¦¬ (ë¬¸ì„œ ëª©ë¡, ì‹œìŠ¤í…œ ìƒíƒœ ë“±)
            meta_response = await self._handle_meta_questions(request.question, request.user_id)
            if meta_response:
                return QueryResponse(
                    answer=meta_response,
                    sources=[],
                    confidence=0.9,
                    processing_time=time.time() - start_time
                )
            
            # 1ë‹¨ê³„: ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
            question_embedding = await self._embed_question(request.question)
            
            # 2ë‹¨ê³„: ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
            search_results = await self._vector_search(
                question_embedding=question_embedding,
                user_id=request.user_id,
                limit=request.max_results,
                score_threshold=request.score_threshold
            )
            
            # 3ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¡œ ì „í™˜
            if not search_results:
                return await self._handle_general_conversation(request.question, time.time() - start_time)
            
            # 4ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(search_results)
            
            # 5ë‹¨ê³„: Geminië¡œ ìµœì¢… ë‹µë³€ ìƒì„±
            answer = await self._generate_answer(request.question, context)
            
            # 6ë‹¨ê³„: ì‘ë‹µ êµ¬ì„±
            processing_time = time.time() - start_time
            confidence = self._calculate_confidence(search_results)
            
            response = QueryResponse(
                answer=answer,
                sources=search_results,
                confidence=confidence,
                processing_time=processing_time
            )
            
            logger.info(f"RAG ì¿¼ë¦¬ ì™„ë£Œ: {processing_time:.2f}ì´ˆ, ì‹ ë¢°ë„: {confidence:.2f}")
            return response
            
        except Exception as e:
            logger.error(f"RAG ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return QueryResponse(
                answer="ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                sources=[],
                confidence=0.0,
                processing_time=time.time() - start_time
            )
    
    async def _handle_meta_questions(self, question: str, user_id: str) -> Optional[str]:
        """ì‹œìŠ¤í…œ ë©”íƒ€ ì§ˆë¬¸ ì²˜ë¦¬ (ë¬¸ì„œ ëª©ë¡, ìƒíƒœ ë“±)"""
        question_lower = question.lower().strip()
        
        # ë¬¸ì„œ ëª©ë¡ ê´€ë ¨ ì§ˆë¬¸ë“¤
        document_keywords = [
            "ë¬¸ì„œ", "íŒŒì¼", "ê°€ì§„", "ì—…ë¡œë“œ", "ì €ì¥", "ëª©ë¡", "ë¦¬ìŠ¤íŠ¸", 
            "ë­ê°€ ìˆ", "ë­ ìˆ", "ë¬´ì—‡", "ì–´ë–¤", "ì–¼ë§ˆë‚˜", "ëª‡ ê°œ"
        ]
        
        if any(keyword in question_lower for keyword in document_keywords):
            try:
                # ì‚¬ìš©ìì˜ ì»¬ë ‰ì…˜ì—ì„œ ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                collection_name = f"documents_{user_id}"
                await self.vector_store.ensure_collection(collection_name)
                
                # ê°„ë‹¨í•œ ê²€ìƒ‰ìœ¼ë¡œ ì €ì¥ëœ ë¬¸ì„œë“¤ í™•ì¸
                test_embedding = await self.embedding_manager.embed_text("test")
                all_docs = await self.vector_store.search_similar(
                    collection_name=collection_name,
                    query_vector=test_embedding,
                    limit=100,
                    score_threshold=0.0
                )
                
                if not all_docs:
                    return "í˜„ì¬ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. '/upload <íŒŒì¼ëª…>' ëª…ë ¹ì–´ë¡œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ë³´ì„¸ìš”."
                
                # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
                doc_groups = {}
                for doc in all_docs:
                    filename = doc.metadata.get("original_filename", "Unknown")
                    if filename not in doc_groups:
                        doc_groups[filename] = []
                    doc_groups[filename].append(doc)
                
                # ì‘ë‹µ ìƒì„±
                response_parts = [f"ğŸ“š í˜„ì¬ ì €ì¥ëœ ë¬¸ì„œ ({len(doc_groups)}ê°œ):"]
                
                for i, (filename, docs) in enumerate(doc_groups.items(), 1):
                    file_type = docs[0].metadata.get("file_type", "unknown")
                    chunk_count = len(docs)
                    upload_time = docs[0].metadata.get("created_at", "Unknown")[:10] if docs[0].metadata.get("created_at") else "Unknown"
                    
                    response_parts.append(f"{i}. ğŸ“„ {filename} ({file_type.upper()})")
                    response_parts.append(f"   - ì²­í¬ ìˆ˜: {chunk_count}ê°œ")
                    response_parts.append(f"   - ì—…ë¡œë“œ: {upload_time}")
                
                response_parts.append("\nğŸ’¡ ì´ ë¬¸ì„œë“¤ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì‹œë©´ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•„ë“œë¦½ë‹ˆë‹¤!")
                
                return "\n".join(response_parts)
                
            except Exception as e:
                logger.error(f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                return "ë¬¸ì„œ ëª©ë¡ì„ í™•ì¸í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # ì‹œìŠ¤í…œ ìƒíƒœ/ì—°ê²° ê´€ë ¨ ì§ˆë¬¸ë“¤ (ë” êµ¬ì²´ì ìœ¼ë¡œ ë¶„ë¦¬)
        connection_keywords = ["ì—°ê²°", "ì ‘ì†", "ì»¤ë„¥ì…˜"]
        system_keywords = ["ìƒíƒœ", "ì‹œìŠ¤í…œ", "ì–´ë–»ê²Œ", "ì‘ë™"]
        
        if any(keyword in question_lower for keyword in connection_keywords):
            return """ğŸ”— ì—°ê²° ìƒíƒœ:
âœ… Qdrant ë²¡í„° DB: ì—°ê²°ë¨
âœ… ì„ë² ë”© ì„œë¹„ìŠ¤: ì •ìƒ
âš ï¸ Gemini API: í• ë‹¹ëŸ‰ ì œí•œ"""
            
        elif any(keyword in question_lower for keyword in system_keywords):
            return """ğŸ¤– RAG ì‹œìŠ¤í…œ ìƒíƒœ:
âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤: ì—°ê²°ë¨ (Qdrant)
âœ… ì„ë² ë”© ì—”ì§„: í™œì„±í™”ë¨
âš ï¸ AI ë‹µë³€ ì—”ì§„: í• ë‹¹ëŸ‰ ì œí•œ ì¤‘ (Gemini)

ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥:
â€¢ ë¬¸ì„œ ì—…ë¡œë“œ ë° ì €ì¥ (/upload)
â€¢ ë¬¸ì„œ ë‚´ìš© ê²€ìƒ‰ ë° ì§ˆì˜ì‘ë‹µ
â€¢ íŒŒì¼ ëª©ë¡ í™•ì¸ (/list)
â€¢ ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (/search)

í˜„ì¬ AI ë‹µë³€ í• ë‹¹ëŸ‰ì´ ì œí•œë˜ì–´ ìˆì§€ë§Œ, ë¬¸ì„œ ê²€ìƒ‰ê³¼ ì»¨í…ìŠ¤íŠ¸ ì œê³µì€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤."""
        
        return None

    async def _embed_question(self, question: str) -> List[float]:
        """ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        try:
            return await self.embedding_manager.embed_text(
                question, 
                EmbeddingModelType.KOREAN
            )
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _vector_search(
        self,
        question_embedding: List[float],
        user_id: str,
        limit: int,
        score_threshold: float
    ) -> List[SearchResult]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            collection_name = f"documents_{user_id}"
            
            return await self.vector_store.search_similar(
                collection_name=collection_name,
                query_vector=question_embedding,
                limit=limit,
                score_threshold=score_threshold
            )
            
        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _build_context(self, search_results: List[SearchResult]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        if not search_results:
            return ""
        
        context_parts = []
        for i, result in enumerate(search_results, 1):
            metadata = result.metadata
            
            # ì»¨í…ìŠ¤íŠ¸ í—¤ë” êµ¬ì„±
            context_header = f"[ë¬¸ì„œ {i}"
            if metadata.get("page"):
                context_header += f", í˜ì´ì§€ {metadata['page']}"
            if metadata.get("type"):
                context_header += f", {metadata['type']}"
            if metadata.get("file_path"):
                file_name = metadata["file_path"].split("/")[-1]
                context_header += f", ì¶œì²˜: {file_name}"
            context_header += "]"
            
            # ë‚´ìš© ì¶”ê°€
            content = result.content.strip()
            if len(content) > 1000:  # ê¸´ ë‚´ìš©ì€ ìš”ì•½
                content = content[:1000] + "..."
            
            context_part = f"{context_header}\n{content}\n"
            context_parts.append(context_part)
        
        return "\n".join(context_parts)
    
    async def _generate_answer(self, question: str, context: str) -> str:
        """Geminië¥¼ ì‚¬ìš©í•´ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        try:
            return await self.gemini_service.generate_answer(
                question=question,
                context=context,
                max_tokens=1000,
                temperature=0.1  # ì¼ê´€ì„± ìˆëŠ” ë‹µë³€ì„ ìœ„í•´ ë‚®ì€ ê°’
            )
            
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # Gemini API ì‹¤íŒ¨ ì‹œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ì ‘ ë‹µë³€ ìƒì„±
            if "í• ë‹¹ëŸ‰" in str(e) or "quota" in str(e).lower():
                return f"""í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. 

ì°¾ì•„ì§„ ê´€ë ¨ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤:

{context}

ìœ„ ë‚´ìš©ì´ '{question}' ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œì—ì„œ ì°¾ì€ ì •ë³´ì…ë‹ˆë‹¤. ë” ì •í™•í•œ AI ë‹µë³€ì„ ì›í•˜ì‹œë©´ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."""
            else:
                return f"""ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©:
{context}

ìœ„ ë‚´ìš©ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."""
    
    def _calculate_confidence(self, search_results: List[SearchResult]) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        if not search_results:
            return 0.0
        
        # ìµœê³  ì ìˆ˜ì™€ í‰ê·  ì ìˆ˜ë¥¼ ì¡°í•©í•˜ì—¬ ì‹ ë¢°ë„ ê³„ì‚°
        scores = [result.score for result in search_results]
        max_score = max(scores)
        avg_score = sum(scores) / len(scores)
        
        # ê²°ê³¼ ê°œìˆ˜ë„ ê³ ë ¤ (ë” ë§ì€ ê´€ë ¨ ë¬¸ì„œê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ ìƒìŠ¹)
        result_count_factor = min(len(search_results) / 5.0, 1.0)
        
        confidence = (max_score * 0.6 + avg_score * 0.3 + result_count_factor * 0.1)
        return min(confidence, 1.0)
    
    async def _handle_general_conversation(self, question: str, processing_time: float) -> QueryResponse:
        """ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ - RAGê°€ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì§ˆë¬¸ ì‘ë‹µ"""
        try:
            # ì¼ë°˜ì ì¸ ì¸ì‚¬, ì†Œê°œ ë“±ì˜ ì§ˆë¬¸ì„ Geminië¡œ ì²˜ë¦¬
            system_prompt = """
ë‹¹ì‹ ì€ RAG(Retrieval Augmented Generation) ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
í˜„ì¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œê°€ ì—†ì§€ë§Œ, ì¼ë°˜ì ì¸ ëŒ€í™”ëŠ” ê°€ëŠ¥í•©ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ì£¼ìš” ê¸°ëŠ¥:
1. ë¬¸ì„œ ì—…ë¡œë“œ ë° ë¶„ì„ (PDF, Word, í…ìŠ¤íŠ¸ íŒŒì¼ ë“±)
2. ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ì •ë³´ ê²€ìƒ‰ ë° ì§ˆì˜ì‘ë‹µ
3. ë‹¤êµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ (í•œêµ­ì–´, ì˜ì–´ ë“±)
4. OCRì„ í†µí•œ ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
5. ë²¡í„° ê²€ìƒ‰ì„ í†µí•œ ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ë§¤ì¹­
6. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
1. ì¸ì‚¬ ì§ˆë¬¸ (ì•ˆë…•, ì•ˆë…•í•˜ì„¸ìš” ë“±): ì¹œê·¼í•˜ê²Œ ì¸ì‚¬í•˜ê³  ìì‹ ì„ RAG ì‹œìŠ¤í…œìœ¼ë¡œ ì†Œê°œ
2. ì •ì²´ì„± ì§ˆë¬¸ (ë„ˆëŠ” ëˆ„êµ¬ì•¼, ë­˜ í•˜ëŠ” AIì•¼ ë“±): RAG ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ AIë¼ê³  êµ¬ì²´ì ìœ¼ë¡œ ì†Œê°œ
3. ê¸°ëŠ¥ ì§ˆë¬¸ (ë­˜ í•  ìˆ˜ ìˆì–´, ì–´ë–¤ ê¸°ëŠ¥ì´ ìˆì–´ ë“±): ìœ„ì˜ ì£¼ìš” ê¸°ëŠ¥ë“¤ì„ ìì„¸íˆ ì„¤ëª…
4. ì‚¬ìš©ë²• ì§ˆë¬¸ (ì–´ë–»ê²Œ ì‚¬ìš©í•´, ë¬¸ì„œëŠ” ì–´ë–»ê²Œ ì˜¬ë ¤ ë“±): ë¬¸ì„œ ì—…ë¡œë“œ ë°©ë²•ê³¼ ì§ˆë¬¸ ë°©ë²• ì•ˆë‚´
5. ì§€ì› íŒŒì¼ ì§ˆë¬¸: PDF, Word, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ íŒŒì¼ ë“± ì§€ì› í˜•ì‹ ì„¤ëª…
6. ê¸°íƒ€ ì¼ë°˜ ì§ˆë¬¸: ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ë˜, ë¬¸ì„œ ì—…ë¡œë“œë¥¼ í†µí•œ ë” ì •í™•í•œ ë‹µë³€ ê°€ëŠ¥ì„± ì–¸ê¸‰

í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ë©° ë„ì›€ì´ ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
ë¬¸ì„œê°€ ì—†ëŠ” ìƒí™©ì—ì„œë„ ìµœëŒ€í•œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
"""
            
            answer = await self.gemini_service.generate_with_system_prompt(
                system_prompt=system_prompt,
                user_message=question,
                max_tokens=500,
                temperature=0.7  # ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ë¥¼ ìœ„í•´ ë†’ì€ ê°’
            )
            
            return QueryResponse(
                answer=answer,
                sources=[],
                confidence=0.8,  # ì¼ë°˜ ëŒ€í™”ëŠ” ë†’ì€ ì‹ ë¢°ë„
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"ì¼ë°˜ ëŒ€í™” ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return self._create_no_result_response(question, processing_time)
    
    def _create_no_result_response(self, question: str, processing_time: float) -> QueryResponse:
        """ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ ì‘ë‹µ (fallback)"""
        return QueryResponse(
            answer="ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ë³´ì‹œê±°ë‚˜, ê´€ë ¨ ë¬¸ì„œë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.",
            sources=[],
            confidence=0.0,
            processing_time=processing_time
        )
    
    async def health_check(self) -> Dict[str, Any]:
        """RAG ì—”ì§„ ìƒíƒœ í™•ì¸"""
        try:
            status = {
                "rag_engine": "healthy",
                "initialized": self._initialized,
                "components": {}
            }
            
            # ê° ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸
            if self._initialized:
                status["components"]["embedding_manager"] = self.embedding_manager.get_model_info()
                status["components"]["gemini_service"] = self.gemini_service.get_service_info()
                
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
                test_embedding = await self.embedding_manager.embed_text("í…ŒìŠ¤íŠ¸")
                status["components"]["embedding_test"] = {
                    "success": len(test_embedding) > 0,
                    "vector_size": len(test_embedding)
                }
            
            return status
            
        except Exception as e:
            logger.error(f"RAG ì—”ì§„ í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨: {e}")
            return {
                "rag_engine": "unhealthy",
                "error": str(e),
                "initialized": self._initialized
            }
    
    async def cleanup(self):
        """RAG ì—”ì§„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        logger.info("RAG ì—”ì§„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œì‘...")
        
        await asyncio.gather(
            self.embedding_manager.cleanup(),
            self.vector_store.cleanup(),
            self.gemini_service.cleanup(),
            return_exceptions=True
        )
        
        self._initialized = False
        logger.info("RAG ì—”ì§„ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# ì „ì—­ RAG ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
rag_engine = RAGEngine()