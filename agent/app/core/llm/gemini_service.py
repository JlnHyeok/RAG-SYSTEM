import logging
import hashlib
import asyncio
from typing import Optional, Dict, Any, List, Tuple
from functools import lru_cache
from datetime import datetime, timedelta

from google import genai
from google.genai import types

from app.core.config import settings
from app.models.exceptions import (
    GeminiAPIError,
    QuotaExceededError,
    ModelNotInitializedError
)

logger = logging.getLogger(__name__)


class GeminiService:
    """
    Google Gemini LLM ì„œë¹„ìŠ¤ í´ë˜ìŠ¤ (Updated for google-genai SDK 1.0+)
    
    Gemini APIì™€ì˜ ëª¨ë“  ìƒí˜¸ì‘ìš©ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤.
    ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±, í”„ë¡¬í”„íŠ¸ ìºì‹±, ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ëŠ¥ ì œê³µ.
    
    Attributes:
        client: google.genai.Client ì¸ìŠ¤í„´ìŠ¤
        model_name: ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì´ë¦„
        _initialized: ì´ˆê¸°í™” ì™„ë£Œ ì—¬ë¶€
        _prompt_cache: í”„ë¡¬í”„íŠ¸ ê²°ê³¼ ìºì‹œ
    """
    
    def __init__(self) -> None:
        self.client: Optional[genai.Client] = None
        self.model_name: str = ""
        self._initialized: bool = False
        self._prompt_cache: Dict[str, Tuple[str, datetime]] = {}  # hash -> (result, timestamp)
        self._cache_ttl_minutes: int = 30
        
    async def initialize(self, test_connection: bool = False) -> None:
        """
        Gemini API ì´ˆê¸°í™”
        
        Args:
            test_connection: ì—°ê²° í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ì—¬ë¶€
            
        Raises:
            GeminiAPIError: API ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
        """
        try:
            if not settings.GEMINI_API_KEY:
                logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ. Gemini ì„œë¹„ìŠ¤ ë¹„í™œì„±í™”")
                self._initialized = False
                return
            
            # Gemini Client ì´ˆê¸°í™”
            self.client = genai.Client(api_key=settings.GEMINI_API_KEY)
            self.model_name = settings.GEMINI_MODEL or "gemini-2.0-flash-lite"
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸ (ì„ íƒì )
            if test_connection:
                try:
                    await self._test_connection()
                except QuotaExceededError:
                    logger.warning("Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼, ì—°ê²° í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°")
                except GeminiAPIError as e:
                    logger.error(f"Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                    raise
            
            self._initialized = True
            logger.info(f"Gemini API ì´ˆê¸°í™” ì™„ë£Œ (Model: {self.model_name})")
            
        except Exception as e:
            if self._is_quota_exceeded(e):
                logger.warning(f"Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼, ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì´ˆê¸°í™”: {e}")
                self.client = genai.Client(api_key=settings.GEMINI_API_KEY)
                self.model_name = settings.GEMINI_MODEL or "gemini-2.0-flash-lite"
                self._initialized = True
                logger.info("Gemini API ê¸°ë³¸ ì´ˆê¸°í™” ì™„ë£Œ (ì—°ê²° í…ŒìŠ¤íŠ¸ ë¯¸ì‹¤í–‰)")
            else:
                logger.error(f"Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise GeminiAPIError(f"Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    
    async def _test_connection(self) -> None:
        """
        Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸
        
        Raises:
            QuotaExceededError: API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ
            GeminiAPIError: ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œ
        """
        try:
            test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. ì—°ê²° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
            # SDK 1.0+: client.models.generate_content
            response = await asyncio.to_thread(
                self.client.models.generate_content,
                model=self.model_name,
                contents=test_prompt
            )
            
            if not response.text:
                raise GeminiAPIError("Gemini API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                
            logger.info("Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
        except Exception as e:
            if self._is_quota_exceeded(e):
                raise QuotaExceededError()
            logger.error(f"Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise GeminiAPIError(f"Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    def _is_quota_exceeded(self, error: Exception) -> bool:
        """í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì—¬ë¶€ í™•ì¸"""
        error_str = str(error).lower()
        return "quota" in error_str or "429" in error_str
    
    def _create_generation_config(self, max_tokens: int, temperature: float) -> types.GenerateContentConfig:
        """
        ì¼ê´€ëœ GenerationConfig ìƒì„±
        SDK 1.0+ì—ì„œëŠ” types.GenerateContentConfig ì‚¬ìš©
        """
        return types.GenerateContentConfig(
            max_output_tokens=max_tokens,
            temperature=temperature,
            top_p=0.95,
            top_k=40,
            candidate_count=1,
            stop_sequences=[]
        )
    
    def _extract_text_from_response(self, response) -> str:
        """
        Gemini ì‘ë‹µ ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        SDK updated: response.text property is preferred
        """
        try:
            if not response:
                return ""
            
            # 1. response.text (Standard access)
            if hasattr(response, 'text') and response.text:
                return response.text
                
            # í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ìƒì„¸ ë¶„ì„
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'content') and candidate.content:
                    if hasattr(candidate.content, 'parts') and candidate.content.parts:
                        for part in candidate.content.parts:
                            if hasattr(part, 'function_call') and part.function_call:
                                logger.warning(f"ëª¨ë¸ì´ í…ìŠ¤íŠ¸ ëŒ€ì‹  í•¨ìˆ˜ í˜¸ì¶œì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {part.function_call.name}")
                            if hasattr(part, 'executable_code') and part.executable_code:
                                logger.warning("ëª¨ë¸ì´ ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤.")
                    else:
                        logger.warning(f"Candidate content partsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. Content: {candidate.content}")
                else:
                    logger.warning("Candidate contentê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.warning("Gemini ì‘ë‹µ í…ìŠ¤íŠ¸ ì—†ìŒ (êµ¬ì¡° í™•ì¸ í•„ìš”)")
            return ""
            
        except Exception as e:
            logger.error(f"Gemini í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    @lru_cache(maxsize=200)
    def _cached_generate_lru(self, prompt_hash: str, prompt: str) -> str:
        """LRU ìºì‹œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if not self._initialized:
            raise ModelNotInitializedError()
            
        response = self.client.models.generate_content(
            model=self.model_name,
            contents=prompt
        )
        
        return self._extract_text_from_response(response)
    
    def _get_cached_result(self, prompt: str) -> Optional[str]:
        """TTL ê¸°ë°˜ ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ"""
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        
        if prompt_hash in self._prompt_cache:
            result, timestamp = self._prompt_cache[prompt_hash]
            if datetime.now() - timestamp < timedelta(minutes=self._cache_ttl_minutes):
                logger.debug(f"í”„ë¡¬í”„íŠ¸ ìºì‹œ íˆíŠ¸: {prompt_hash[:8]}...")
                return result
            else:
                del self._prompt_cache[prompt_hash]
        
        return None
    
    def _set_cached_result(self, prompt: str, result: str) -> None:
        """TTL ê¸°ë°˜ ìºì‹œì— ê²°ê³¼ ì €ì¥"""
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        self._prompt_cache[prompt_hash] = (result, datetime.now())
        
        if len(self._prompt_cache) > 500:
            oldest_key = min(self._prompt_cache.keys(), 
                           key=lambda k: self._prompt_cache[k][1])
            del self._prompt_cache[oldest_key]
    
    async def generate_answer(
        self,
        question: str,
        context: str,
        max_tokens: int = 2000,
        temperature: float = 0.1
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        try:
            if not self._initialized:
                await self.initialize()
            
            prompt = self._build_rag_prompt(question, context)
            
            # ìºì‹œ ë¡œì§
            if len(prompt) < 2000:
                cached_result = self._get_cached_result(prompt)
                if cached_result:
                    return cached_result
            
            if len(prompt) < 500:
                prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
                try:
                    result = self._cached_generate_lru(prompt_hash, prompt)
                    self._set_cached_result(prompt, result)
                    return result
                except Exception as e:
                    if not self._is_quota_exceeded(e):
                        logger.warning(f"LRU ìƒì„± ì‹¤íŒ¨, ì¼ë°˜ ìƒì„± ì‹œë„: {e}")
            
            # ë¹„ë™ê¸° ìƒì„± (Sync wrapper in Thread)
            def generate() -> str:
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=self._create_generation_config(max_tokens, temperature)
                )
                return self._extract_text_from_response(response)
            
            result = await asyncio.to_thread(generate)
            self._set_cached_result(prompt, result)
            return result
            
        except QuotaExceededError:
            return self._get_quota_exceeded_response(question, context)
        except Exception as e:
            logger.error(f"Gemini ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            if self._is_quota_exceeded(e):
                return self._get_quota_exceeded_response(question, context)
            return self._get_fallback_response(e)
    
    async def generate_with_system_prompt(
        self,
        system_prompt: str,
        user_message: str,
        max_tokens: int = 4096,
        temperature: float = 0.7
    ) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ ë‹µë³€ ìƒì„±"""
        try:
            if not self._initialized:
                await self.initialize()
            
            # SDK 1.0 supports system instructions in config usually, but content concatenation is safer cross-version
            # Or use config(system_instruction=...) if supported. 
            # For robustness, we'll mimic the prompt structure unless we confirm system_instruction param
            
            # NOTE: google-genai supports `config=types.GenerateContentConfig(system_instruction=...)`
            # Let's use that for "proper" usage.
            
            def generate():
                config = self._create_generation_config(max_tokens, temperature)
                config.system_instruction = system_prompt
                
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=user_message,
                    config=config
                )
                return self._extract_text_from_response(response)
            
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"Gemini ì¼ë°˜ ëŒ€í™” ìƒì„± ì‹¤íŒ¨: {e}")
            if self._is_quota_exceeded(e):
                return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            return self._get_basic_fallback_response(user_message)
    
    def _build_rag_prompt(self, question: str, context: str) -> str:
        """RAGìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„± (ê¸°ì¡´ ìœ ì§€)"""
        return f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
                ì¤‘ìš”í•œ ê·œì¹™:
                1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”
                2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥´ê² ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”
                3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”
                4. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”
                5. ì¶œì²˜ ì •ë³´ê°€ ìˆë‹¤ë©´ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
                6. ë‹µë³€ì„ ì™„ì „íˆ ëê¹Œì§€ ì‘ì„±í•˜ì„¸ìš” - ì¤‘ê°„ì— ëŠì§€ ë§ˆì„¸ìš”
                
                7. [ğŸš¨ CRITICAL - ì ˆëŒ€ ê·œì¹™] ì»¨í…ìŠ¤íŠ¸ì— ë¹„ìŠ·í•œ í˜•ì‹ì˜ ë°ì´í„°ê°€ 3ê°œ ì´ìƒ ë°˜ë³µëœë‹¤ë©´:
                   âœ… í•„ìˆ˜ ì¤€ìˆ˜: ë°˜ë“œì‹œ Markdown í‘œ(Table) í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
                   
                   í‘œ ì‘ì„± í•„ìˆ˜ ìš”êµ¬ì‚¬í•­:
                   - ìƒíƒœ ì»¬ëŸ¼ì—ëŠ” ë°˜ë“œì‹œ ì•„ì´ì½˜ê³¼ í…ìŠ¤íŠ¸ ì‚¬ìš©: âœ… (ì •ìƒ), âš ï¸ (ê²½ê³ /ì´ìƒ), âŒ (ë¶ˆëŸ‰/ì—ëŸ¬)
                   - ì»¨í…ìŠ¤íŠ¸ì— ì´ë¯¸ ì•„ì´ì½˜ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”
                   
                   ğŸ”‘ ì¤‘ìš” ì •ë³´ ëˆ„ë½ ê¸ˆì§€:
                   - ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” í•µì‹¬ ì •ë³´ë¥¼ ìƒëµí•˜ì§€ ë§ˆì„¸ìš”
                   - ì»¨í…ìŠ¤íŠ¸ì— ì—¬ëŸ¬ ì„¹ì…˜ì˜ ë°ì´í„°ê°€ ìˆë‹¤ë©´ (ì˜ˆ: "ìƒì‚°í’ˆë³„ ì´ìƒ íŒì •", "ìµœê·¼ ì´ìƒê°ì§€ ìƒì„¸ ì´ë ¥" ë“±) ëª¨ë“  ì„¹ì…˜ì„ ë¹ ì§ì—†ì´ í‘œë¡œ ì‘ì„±í•˜ì„¸ìš”
                   
                   ğŸ“ í‘œ ì •ë ¬ ë° ê°€ë…ì„±:
                   - ë„ˆë¹„ê°€ ë¶ˆê·œì¹™í•œ ê²½ìš°, í‘œì˜ ë„ˆë¹„ë¥¼ ë„“ê²Œ ì¡°ì •í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”
                   - ë„ˆë¬´ ê¸´ ê°’ì€ ì ì ˆíˆ ì¤„ì—¬ì„œ í‘œì‹œí•˜ë˜ ì¤‘ìš” ì •ë³´ëŠ” ìœ ì§€í•˜ì„¸ìš”
                   
                8. ìƒì„¸í•˜ê³  ì™„ì„±ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
                ì»¨í…ìŠ¤íŠ¸:
                {context}
                ì§ˆë¬¸: {question}
                ë‹µë³€ (í‘œ í˜•ì‹ í•„ìˆ˜, ì™„ì „í•˜ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±):"""
    
    async def generate_summary(self, text: str, max_length: int = 200) -> str:
        try:
            if not self._initialized:
                await self.initialize()
            
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {max_length}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ ë‚´ìš©ì„ ë†“ì¹˜ì§€ ë§ê³  ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.
                        í…ìŠ¤íŠ¸:
                        {text}
                        ìš”ì•½:"""
            
            def generate():
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=self._create_generation_config(max_length * 2, 0.3)
                )
                return self._extract_text_from_response(response)
                
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return text[:max_length] + "..."
    
    async def generate_keywords(self, text: str, max_keywords: int = 5) -> list:
        try:
            if not self._initialized:
                await self.initialize()
            
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ {max_keywords}ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ê° í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”.
                        í…ìŠ¤íŠ¸:
                        {text}
                        í‚¤ì›Œë“œ:"""
            
            def generate():
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt
                )
                text = self._extract_text_from_response(response)
                keywords_text = text.strip()
                return [kw.strip() for kw in keywords_text.split(',')]
                
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_fallback_response(self, error: Exception) -> str:
        error_str = str(error).lower()
        if "quota" in error_str or "429" in error_str:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "rate limit" in error_str:
            return "í˜„ì¬ ì„œë¹„ìŠ¤ ì‚¬ìš©ëŸ‰ì´ ë§ì•„ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "api key" in error_str:
            return "API ì¸ì¦ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
        else:
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _get_quota_exceeded_response(self, question: str, context: str) -> str:
        """í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ë‹µë³€ (ê¸°ì¡´ ìœ ì§€)"""
        # (ìƒëµ: ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        # ë¡œì§ì´ ê¸¸ì–´ì„œ ì—¬ê¸°ì„œëŠ” ë³µì› í•„ìš”. ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©.
        if not context or len(context.strip()) == 0:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        context_lines = context.strip().split('\n')
        formatted_parts = []
        current_doc = ""
        current_content = []
        
        for line in context_lines:
            if line.startswith('[ë¬¸ì„œ'):
                if current_doc and current_content:
                    content_text = '\n'.join(current_content).strip()
                    if content_text:
                        formatted_parts.append(f"**{current_doc}**\n{content_text}")
                current_doc = line.strip('[]')
                current_content = []
            elif line.strip():
                current_content.append(line)
        
        if current_doc and current_content:
            content_text = '\n'.join(current_content).strip()
            if content_text:
                formatted_parts.append(f"**{current_doc}**\n{content_text}")
        
        result = "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\n\n"
        result += f"'{question}' ì§ˆë¬¸ê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
        
        if formatted_parts:
            result += "\n\n".join(formatted_parts)
        else:
            result += context[:800] + ("..." if len(context) > 800 else "")
        return result
    
    async def _get_intelligent_fallback_response(self, user_message: str, quota_exceeded: bool = False) -> str:
        """ì§€ëŠ¥ì ì¸ fallback ì‘ë‹µ ìƒì„±"""
        if quota_exceeded:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."
        
        try:
            if self._initialized:
                system_prompt = """ë‹¹ì‹ ì€ RAG ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”."""
                return await self.generate_with_system_prompt(
                    system_prompt=system_prompt,
                    user_message=user_message,
                    max_tokens=300,
                    temperature=0.7
                )
        except Exception as e:
            logger.warning(f"LLM ê¸°ë°˜ fallback ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return self._get_basic_fallback_response(user_message)
    
    def _get_basic_fallback_response(self, user_message: str) -> str:
        return "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” RAG ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"

    def get_service_info(self) -> dict:
        """ì„œë¹„ìŠ¤ ì •ë³´ ë°˜í™˜"""
        # _cached_generate_lruëŠ” lru_cacheë¡œ ë˜í•‘ë˜ì–´ ìˆìŒ
        return {
            "service": "Google Gemini (Updated Client)",
            "model": settings.GEMINI_MODEL or "default",
            "initialized": self._initialized,
            "cache_info": self._cached_generate_lru.cache_info()._asdict() if hasattr(self._cached_generate_lru, 'cache_info') else {}
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self._cached_generate_lru, 'cache_clear'):
            self._cached_generate_lru.cache_clear()
        
        self._initialized = False
        self.client = None
        logger.info("Gemini ì„œë¹„ìŠ¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

# ì „ì—­ Gemini ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiService()