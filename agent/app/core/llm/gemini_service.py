import logging
import hashlib
from typing import Optional, Dict, Any, List, Tuple
import google.generativeai as genai
import asyncio
from functools import lru_cache
from datetime import datetime, timedelta

from app.core.config import settings
from app.models.exceptions import (
    GeminiAPIError,
    QuotaExceededError,
    ModelNotInitializedError
)

logger = logging.getLogger(__name__)


class GeminiService:
    """
    Google Gemini LLM ì„œë¹„ìŠ¤ í´ë˜ìŠ¤
    
    Gemini APIì™€ì˜ ëª¨ë“  ìƒí˜¸ì‘ìš©ì„ ì²˜ë¦¬í•˜ëŠ” ì„œë¹„ìŠ¤ í´ë˜ìŠ¤.
    ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±, í”„ë¡¬í”„íŠ¸ ìºì‹±, ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ëŠ¥ ì œê³µ.
    
    Attributes:
        model: Gemini GenerativeModel ì¸ìŠ¤í„´ìŠ¤
        model_name: ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ ì´ë¦„
        _initialized: ì´ˆê¸°í™” ì™„ë£Œ ì—¬ë¶€
        _prompt_cache: í”„ë¡¬í”„íŠ¸ ê²°ê³¼ ìºì‹œ
    """
    
    def __init__(self) -> None:
        self.model: Optional[genai.GenerativeModel] = None
        self.model_name: str = ""
        self._initialized: bool = False
        self._prompt_cache: Dict[str, Tuple[str, datetime]] = {}  # hash -> (result, timestamp)
        self._cache_ttl_minutes: int = 30
        
    async def initialize(self, test_connection: bool = False) -> None:
        """
        Gemini API ì´ˆê¸°í™”
        
        Args:
            test_connection: ì—°ê²° í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ì—¬ë¶€
            
        Raises:
            GeminiAPIError: API ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ
        """
        try:
            if not settings.GEMINI_API_KEY:
                logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ. Gemini ì„œë¹„ìŠ¤ ë¹„í™œì„±í™”")
                self._initialized = False
                return
            
            # Gemini API ì„¤ì •
            genai.configure(api_key=settings.GEMINI_API_KEY)
            
            # ëª¨ë¸ ì´ˆê¸°í™”
            self.model_name = settings.GEMINI_MODEL or "gemini-2.0-flash-lite"
            self.model = genai.GenerativeModel(self.model_name)
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸ (ì„ íƒì )
            if test_connection:
                try:
                    await self._test_connection()
                except QuotaExceededError:
                    logger.warning("Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼, ì—°ê²° í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°")
                except GeminiAPIError as e:
                    logger.error(f"Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                    raise
            
            self._initialized = True
            logger.info(f"Gemini API ì´ˆê¸°í™” ì™„ë£Œ (Model: {self.model_name})")
            
        except Exception as e:
            if self._is_quota_exceeded(e):
                logger.warning(f"Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼, ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì´ˆê¸°í™”: {e}")
                genai.configure(api_key=settings.GEMINI_API_KEY)
                self.model_name = settings.GEMINI_MODEL or "gemini-2.0-flash-lite"
                self.model = genai.GenerativeModel(self.model_name)
                self._initialized = True
                logger.info("Gemini API ê¸°ë³¸ ì´ˆê¸°í™” ì™„ë£Œ (ì—°ê²° í…ŒìŠ¤íŠ¸ ë¯¸ì‹¤í–‰)")
            else:
                logger.error(f"Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise GeminiAPIError(f"Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
    
    async def _test_connection(self) -> None:
        """
        Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸
        
        Raises:
            QuotaExceededError: API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ
            GeminiAPIError: ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì‹œ
        """
        try:
            test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. ì—°ê²° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
            response = await asyncio.to_thread(
                self.model.generate_content,
                test_prompt
            )
            
            if not response.text:
                raise GeminiAPIError("Gemini API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                
            logger.info("Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
        except Exception as e:
            if self._is_quota_exceeded(e):
                raise QuotaExceededError()
            logger.error(f"Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise GeminiAPIError(f"Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    def _is_quota_exceeded(self, error: Exception) -> bool:
        """í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì—¬ë¶€ í™•ì¸"""
        error_str = str(error).lower()
        return "quota" in error_str or "429" in error_str
    
    def _create_generation_config(self, max_tokens: int, temperature: float) -> genai.types.GenerationConfig:
        """ì¼ê´€ëœ GenerationConfig ìƒì„± - ë” ì™„ì „í•œ ë‹µë³€ì„ ìœ„í•œ ì„¤ì •"""
        return genai.types.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=temperature,
            top_p=0.95,  # ë” ë‹¤ì–‘í•œ ì‘ë‹µì„ ìœ„í•´
            top_k=40,   # í† í° ì„ íƒ ë²”ìœ„ í™•ì¥
            candidate_count=1,  # í•˜ë‚˜ì˜ ì™„ì „í•œ ë‹µë³€ ìƒì„±
            stop_sequences=[]   # ì¤‘ë‹¨ ì‹œí€€ìŠ¤ ì—†ìŒìœ¼ë¡œ ì™„ì „í•œ ë‹µë³€ ë³´ì¥
        )
    
    @lru_cache(maxsize=200)
    def _cached_generate_lru(self, prompt_hash: str, prompt: str) -> str:
        """
        LRU ìºì‹œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„± (ì§§ì€ í”„ë¡¬í”„íŠ¸ìš©)
        
        Args:
            prompt_hash: í”„ë¡¬í”„íŠ¸ í•´ì‹œê°’
            prompt: ì‹¤ì œ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            
        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
            
        Raises:
            ModelNotInitializedError: ëª¨ë¸ ë¯¸ì´ˆê¸°í™” ì‹œ
        """
        if not self._initialized:
            raise ModelNotInitializedError()
            
        response = self.model.generate_content(prompt)
        
        # ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if not response.candidates:
            logger.warning("Gemini ì‘ë‹µì— candidatesê°€ ì—†ìŠµë‹ˆë‹¤. (Safety Block ê°€ëŠ¥ì„± - LRU Cache)")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        try:
            return response.text
        except Exception:
            if response.candidates and response.candidates[0].content.parts:
                return response.candidates[0].content.parts[0].text
            return ""
    
    def _get_cached_result(self, prompt: str) -> Optional[str]:
        """
        TTL ê¸°ë°˜ ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ
        
        Args:
            prompt: í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            
        Returns:
            ìºì‹œëœ ê²°ê³¼ ë˜ëŠ” None
        """
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        
        if prompt_hash in self._prompt_cache:
            result, timestamp = self._prompt_cache[prompt_hash]
            # TTL í™•ì¸
            if datetime.now() - timestamp < timedelta(minutes=self._cache_ttl_minutes):
                logger.debug(f"í”„ë¡¬í”„íŠ¸ ìºì‹œ íˆíŠ¸: {prompt_hash[:8]}...")
                return result
            else:
                # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                del self._prompt_cache[prompt_hash]
        
        return None
    
    def _set_cached_result(self, prompt: str, result: str) -> None:
        """
        TTL ê¸°ë°˜ ìºì‹œì— ê²°ê³¼ ì €ì¥
        
        Args:
            prompt: í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸
            result: ìƒì„±ëœ ê²°ê³¼
        """
        prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
        self._prompt_cache[prompt_hash] = (result, datetime.now())
        
        # ìºì‹œ í¬ê¸° ì œí•œ (500ê°œ ì´ˆê³¼ ì‹œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°)
        if len(self._prompt_cache) > 500:
            oldest_key = min(self._prompt_cache.keys(), 
                           key=lambda k: self._prompt_cache[k][1])
            del self._prompt_cache[oldest_key]
    
    async def generate_answer(
        self,
        question: str,
        context: str,
        max_tokens: int = 2000,
        temperature: float = 0.1
    ) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±
        
        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
            max_tokens: ìµœëŒ€ ì‘ë‹µ í† í° ìˆ˜
            temperature: ìƒì„± ë‹¤ì–‘ì„± (0.0~1.0)
            
        Returns:
            ìƒì„±ëœ ë‹µë³€ í…ìŠ¤íŠ¸
            
        Raises:
            GeminiAPIError: API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
        """
        try:
            if not self._initialized:
                await self.initialize()
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_rag_prompt(question, context)
            
            # TTL ìºì‹œ í™•ì¸ (ì¤‘ê°„ ê¸¸ì´ í”„ë¡¬í”„íŠ¸)
            if len(prompt) < 2000:
                cached_result = self._get_cached_result(prompt)
                if cached_result:
                    return cached_result
            
            # ì§§ì€ í”„ë¡¬í”„íŠ¸ëŠ” LRU ìºì‹œë„ ì‚¬ìš©
            if len(prompt) < 500:
                prompt_hash = hashlib.md5(prompt.encode()).hexdigest()
                try:
                    result = self._cached_generate_lru(prompt_hash, prompt)
                    self._set_cached_result(prompt, result)  # TTL ìºì‹œì—ë„ ì €ì¥
                    return result
                except QuotaExceededError:
                    return self._get_quota_exceeded_response(question, context)
                except Exception as e:
                    if self._is_quota_exceeded(e):
                        return self._get_quota_exceeded_response(question, context)
                    raise
            
            # ê¸´ í”„ë¡¬í”„íŠ¸ëŠ” ë¹„ë™ê¸° ì²˜ë¦¬
            def generate() -> str:
                response = self.model.generate_content(
                    prompt,
                    generation_config=self._create_generation_config(max_tokens, temperature)
                )
                
                # ì•ˆì „í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                if not response.candidates:
                    logger.warning("Gemini ì‘ë‹µì— candidatesê°€ ì—†ìŠµë‹ˆë‹¤. (Safety Block ê°€ëŠ¥ì„±)")
                    if response.prompt_feedback:
                        logger.warning(f"Prompt Feedback: {response.prompt_feedback}")
                    return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ì•ˆì „ ì •ì±… ë˜ëŠ” ì˜¤ë¥˜)."
                
                try:
                    return response.text
                except Exception as e:
                    logger.error(f"response.text ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    # ëŒ€ì²´ ì ‘ê·¼ ì‹œë„
                    if response.candidates and response.candidates[0].content.parts:
                        return response.candidates[0].content.parts[0].text
                    return ""
            
            result = await asyncio.to_thread(generate)
            
            # ê²°ê³¼ë¥¼ TTL ìºì‹œì— ì €ì¥
            self._set_cached_result(prompt, result)
            return result
            
        except QuotaExceededError:
            return self._get_quota_exceeded_response(question, context)
        except Exception as e:
            logger.error(f"Gemini ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            if self._is_quota_exceeded(e):
                return self._get_quota_exceeded_response(question, context)
            return self._get_fallback_response(e)
    
    async def generate_with_system_prompt(
        self,
        system_prompt: str,
        user_message: str,
        max_tokens: int = 4096,
        temperature: float = 0.7
    ) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ ë‹µë³€ ìƒì„± (ì¼ë°˜ ëŒ€í™”ìš©)"""
        try:
            if not self._initialized:
                await self.initialize()
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì‚¬ìš©ì ë©”ì‹œì§€ ì¡°í•©
            full_prompt = f"""
                            {system_prompt}
                            ì‚¬ìš©ì ì§ˆë¬¸: {user_message}
                            ë‹µë³€:"""
            
            def generate():
                response = self.model.generate_content(
                    full_prompt,
                    generation_config=self._create_generation_config(max_tokens, temperature)
                )
                return response.text
            
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"Gemini ì¼ë°˜ ëŒ€í™” ìƒì„± ì‹¤íŒ¨: {e}")
            
            # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            if self._is_quota_exceeded(e):
                return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì€ ì—¬ì „íˆ ê°€ëŠ¥í•˜ë‹ˆ, ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ê´€ë ¨ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?"
            
            # ê¸°íƒ€ ì—ëŸ¬ ì‹œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ (ì¬ê·€ í˜¸ì¶œ ë°©ì§€)
            return self._get_basic_fallback_response(user_message)
    
    def _build_rag_prompt(self, question: str, context: str) -> str:
        """RAGìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±"""
        return f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
                ì¤‘ìš”í•œ ê·œì¹™:
                1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”
                2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥´ê² ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”
                3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”
                4. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”
                5. ì¶œì²˜ ì •ë³´ê°€ ìˆë‹¤ë©´ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
                6. ë‹µë³€ì„ ì™„ì „íˆ ëê¹Œì§€ ì‘ì„±í•˜ì„¸ìš” - ì¤‘ê°„ì— ëŠì§€ ë§ˆì„¸ìš”
                7. í‘œë‚˜ ëª©ë¡ì´ ìˆë‹¤ë©´ ëª¨ë“  í•­ëª©ì„ í¬í•¨í•˜ì„¸ìš”
                8. ìƒì„¸í•˜ê³  ì™„ì„±ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
                ì»¨í…ìŠ¤íŠ¸:
                {context}
                ì§ˆë¬¸: {question}
                ë‹µë³€ (ì™„ì „í•˜ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±):"""
    
    async def generate_summary(self, text: str, max_length: int = 200) -> str:
        """í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        try:
            if not self._initialized:
                await self.initialize()
            
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {max_length}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ ë‚´ìš©ì„ ë†“ì¹˜ì§€ ë§ê³  ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.
                        í…ìŠ¤íŠ¸:
                        {text}
                        ìš”ì•½:"""
            
            def generate():
                response = self.model.generate_content(
                    prompt,
                    generation_config=self._create_generation_config(max_length * 2, 0.3)  # í•œêµ­ì–´ íŠ¹ì„±ìƒ ì—¬ìœ ë¶„, ë‚®ì€ temperature
                )
                return response.text
                
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return text[:max_length] + "..."
    
    async def generate_keywords(self, text: str, max_keywords: int = 5) -> list:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            if not self._initialized:
                await self.initialize()
            
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ {max_keywords}ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ê° í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”.
                        í…ìŠ¤íŠ¸:
                        {text}
                        í‚¤ì›Œë“œ:"""
            
            def generate():
                response = self.model.generate_content(prompt)
                keywords_text = response.text.strip()
                return [kw.strip() for kw in keywords_text.split(',')]
                
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_fallback_response(self, error: Exception) -> str:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ"""
        error_str = str(error).lower()
        if "quota" in error_str or "429" in error_str:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "rate limit" in error_str:
            return "í˜„ì¬ ì„œë¹„ìŠ¤ ì‚¬ìš©ëŸ‰ì´ ë§ì•„ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "api key" in error_str:
            return "API ì¸ì¦ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
        else:
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _get_quota_exceeded_response(self, question: str, context: str) -> str:
        """í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ë‹µë³€"""
        if not context or len(context.strip()) == 0:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        # ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ í‘œì‹œ
        context_lines = context.strip().split('\n')
        formatted_parts = []
        
        current_doc = ""
        current_content = []
        
        for line in context_lines:
            if line.startswith('[ë¬¸ì„œ'):
                # ì´ì „ ë¬¸ì„œ ì €ì¥
                if current_doc and current_content:
                    content_text = '\n'.join(current_content).strip()
                    if content_text:
                        formatted_parts.append(f"**{current_doc}**\n{content_text}")
                
                # ìƒˆ ë¬¸ì„œ ì‹œì‘
                current_doc = line.strip('[]')
                current_content = []
            elif line.strip():
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ë¬¸ì„œ ì €ì¥
        if current_doc and current_content:
            content_text = '\n'.join(current_content).strip()
            if content_text:
                formatted_parts.append(f"**{current_doc}**\n{content_text}")
        
        result = "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\n\n"
        result += f"'{question}' ì§ˆë¬¸ê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
        
        if formatted_parts:
            result += "\n\n".join(formatted_parts)
        else:
            result += context[:800] + ("..." if len(context) > 800 else "")
        
        return result
    
    async def _get_intelligent_fallback_response(self, user_message: str, quota_exceeded: bool = False) -> str:
        """ì§€ëŠ¥ì ì¸ fallback ì‘ë‹µ ìƒì„± - LLMì„ í™œìš©í•œ ìì—°ì–´ ì´í•´"""
        
        # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œì—ëŠ” ê°„ë‹¨í•œ ê¸°ë³¸ ì‘ë‹µ
        if quota_exceeded:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì€ ì—¬ì „íˆ ê°€ëŠ¥í•˜ë‹ˆ, ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ê´€ë ¨ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?"
        
        # LLMì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°, ìì—°ì–´ë¡œ ì˜ë„ íŒŒì•… í›„ ì ì ˆí•œ ì‘ë‹µ ìƒì„±
        try:
            if self._initialized:
                system_prompt = """ë‹¹ì‹ ì€ RAG ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”.
                                    ë‹¹ì‹ ì˜ ì •ë³´:
                                    - ì´ë¦„: RAG ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸
                                    - ì£¼ìš” ê¸°ëŠ¥: ë¬¸ì„œ ì—…ë¡œë“œ ë° ë¶„ì„, ì§ˆì˜ì‘ë‹µ, ë‹¤êµ­ì–´ ì§€ì›, OCR, ë²¡í„° ê²€ìƒ‰
                                    - ì§€ì› íŒŒì¼: PDF, Word, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ (ìµœëŒ€ 50MB)
                                    - íŠ¹ì§•: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°, ì •í™•í•œ ì •ë³´ ê²€ìƒ‰

                                    ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
                                    1. ì¸ì‚¬/ì²« ë§Œë‚¨: ê°„ë‹¨í•œ ì†Œê°œì™€ ë¬¸ì„œ ì—…ë¡œë“œ ì•ˆë‚´
                                    2. ì •ì²´ì„±/ì†Œê°œ ì§ˆë¬¸: RAG ì‹œìŠ¤í…œê³¼ ì£¼ìš” ê¸°ëŠ¥ ì„¤ëª…
                                    3. ê¸°ëŠ¥/ì‚¬ìš©ë²• ì§ˆë¬¸: êµ¬ì²´ì ì¸ ì‚¬ìš© ë°©ë²•ê³¼ ê¸°ëŠ¥ ëª©ë¡ ì œê³µ
                                    4. íŒŒì¼/ë¬¸ì„œ ê´€ë ¨: ì§€ì› í˜•ì‹ê³¼ ì—…ë¡œë“œ ë°©ë²• ì•ˆë‚´
                                    5. ê¸°íƒ€: ë„ì›€ì´ ë˜ëŠ” ì¼ë°˜ì ì¸ ì•ˆë‚´

                                    í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""

                response = await self.generate_with_system_prompt(
                    system_prompt=system_prompt,
                    user_message=user_message,
                    max_tokens=300,
                    temperature=0.7
                )
                return response
                
        except Exception as e:
            logger.warning(f"LLM ê¸°ë°˜ fallback ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        return self._get_basic_fallback_response(user_message)
    
    def _get_basic_fallback_response(self, user_message: str) -> str:
        """ê¸°ë³¸ fallback ì‘ë‹µ (LLM ì‹¤íŒ¨ ì‹œ)"""
        return """ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” RAG ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                ğŸ“„ **ì£¼ìš” ê¸°ëŠ¥:**
                â€¢ ë¬¸ì„œ ì—…ë¡œë“œ ë° ë¶„ì„ (PDF, Word, í…ìŠ¤íŠ¸ ë“±)
                â€¢ ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
                â€¢ ë‹¤êµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ ì§€ì›
                â€¢ OCRì„ í†µí•œ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                â€¢ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

                ğŸ’¡ **ì‚¬ìš©ë²•:**
                1. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”
                2. ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”
                3. ì •í™•í•œ ë‹µë³€ì„ ë°›ì•„ë³´ì„¸ìš”!

                ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”! ğŸš€"""
                    
    def get_service_info(self) -> dict:
        """ì„œë¹„ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return {
            "service": "Google Gemini",
            "model": settings.GEMINI_MODEL or "gemini-2.0-flash-exp (default)",
            "initialized": self._initialized,
            "cache_info": self._cached_generate.cache_info()._asdict() if hasattr(self._cached_generate, 'cache_info') else {}
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self._cached_generate, 'cache_clear'):
            self._cached_generate.cache_clear()
        
        self._initialized = False
        self.model = None
        
        logger.info("Gemini ì„œë¹„ìŠ¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# ì „ì—­ Gemini ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiService()