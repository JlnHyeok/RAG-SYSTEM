import logging
from typing import Optional
import google.generativeai as genai
import asyncio
from functools import lru_cache

from app.core.config import settings

logger = logging.getLogger(__name__)


class GeminiService:
    """Google Gemini LLM ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model = None
        self._initialized = False
        
    async def initialize(self, test_connection: bool = False):
        """Gemini API ì´ˆê¸°í™”"""
        try:
            if not settings.GEMINI_API_KEY:
                logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ. Gemini ì„œë¹„ìŠ¤ ë¹„í™œì„±í™”")
                self._initialized = False
                return
            
            # Gemini API ì„¤ì •
            genai.configure(api_key=settings.GEMINI_API_KEY)
            
            # ëª¨ë¸ ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            model_name = settings.GEMINI_MODEL or "gemini-2.0-flash-exp"
            self.model = genai.GenerativeModel(model_name)
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸ (ì„ íƒì )
            if test_connection:
                try:
                    await self._test_connection()
                except Exception as e:
                    if "quota" in str(e).lower() or "429" in str(e):
                        logger.warning(f"Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼, ì—°ê²° í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°: {e}")
                    else:
                        logger.error(f"Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                        raise
            
            self._initialized = True
            logger.info("Gemini API ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            if "quota" in str(e).lower() or "429" in str(e):
                logger.warning(f"Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼, ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì´ˆê¸°í™”: {e}")
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œë¼ë„ ì´ˆê¸°í™”
                genai.configure(api_key=settings.GEMINI_API_KEY)
                model_name = settings.GEMINI_MODEL or "gemini-2.0-flash-exp"
                self.model = genai.GenerativeModel(model_name)
                self._initialized = True
                logger.info("Gemini API ê¸°ë³¸ ì´ˆê¸°í™” ì™„ë£Œ (ì—°ê²° í…ŒìŠ¤íŠ¸ ë¯¸ì‹¤í–‰)")
            else:
                logger.error(f"Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise
    
    async def _test_connection(self):
        """Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. ì—°ê²° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
            response = await asyncio.to_thread(
                self.model.generate_content,
                test_prompt
            )
            
            if not response.text:
                raise Exception("Gemini API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                
            logger.info("Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
        except Exception as e:
            logger.error(f"Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise
    
    def _is_quota_exceeded(self, error: Exception) -> bool:
        """í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì—¬ë¶€ í™•ì¸"""
        error_str = str(error).lower()
        return "quota" in error_str or "429" in error_str
    
    def _create_generation_config(self, max_tokens: int, temperature: float) -> genai.types.GenerationConfig:
        """ì¼ê´€ëœ GenerationConfig ìƒì„± - ë” ì™„ì „í•œ ë‹µë³€ì„ ìœ„í•œ ì„¤ì •"""
        return genai.types.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=temperature,
            top_p=0.95,  # ë” ë‹¤ì–‘í•œ ì‘ë‹µì„ ìœ„í•´
            top_k=40,   # í† í° ì„ íƒ ë²”ìœ„ í™•ì¥
            candidate_count=1,  # í•˜ë‚˜ì˜ ì™„ì „í•œ ë‹µë³€ ìƒì„±
            stop_sequences=[]   # ì¤‘ë‹¨ ì‹œí€€ìŠ¤ ì—†ìŒìœ¼ë¡œ ì™„ì „í•œ ë‹µë³€ ë³´ì¥
        )
    
    @lru_cache(maxsize=100)
    def _cached_generate(self, prompt_hash: str, prompt: str) -> str:
        """ìì£¼ ì‚¬ìš©ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ìºì‹±"""
        if not self._initialized:
            raise RuntimeError("Gemini ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
        response = self.model.generate_content(prompt)
        return response.text
    
    async def generate_answer(
        self,
        question: str,
        context: str,
        max_tokens: int = 2000,  # ê¸°ë³¸ê°’ì„ 1000ì—ì„œ 2000ìœ¼ë¡œ ì¦ê°€
        temperature: float = 0.1
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        try:
            if not self._initialized:
                await self.initialize()
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_rag_prompt(question, context)
            
            # ì§§ì€ í”„ë¡¬í”„íŠ¸ëŠ” ìºì‹œ ì‚¬ìš©
            if len(prompt) < 500:
                prompt_hash = str(hash(prompt))
                try:
                    return self._cached_generate(prompt_hash, prompt)
                except Exception as e:
                    if self._is_quota_exceeded(e):
                        return self._get_quota_exceeded_response(question, context)
                    raise
            
            # ê¸´ í”„ë¡¬í”„íŠ¸ëŠ” ë¹„ë™ê¸° ì²˜ë¦¬
            def generate():
                response = self.model.generate_content(
                    prompt,
                    generation_config=self._create_generation_config(max_tokens, temperature)
                )
                return response.text
            
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"Gemini ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            if self._is_quota_exceeded(e):
                return self._get_quota_exceeded_response(question, context)
            return self._get_fallback_response(e)
    
    async def generate_with_system_prompt(
        self,
        system_prompt: str,
        user_message: str,
        max_tokens: int = 500,
        temperature: float = 0.7
    ) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ ë‹µë³€ ìƒì„± (ì¼ë°˜ ëŒ€í™”ìš©)"""
        try:
            if not self._initialized:
                await self.initialize()
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì‚¬ìš©ì ë©”ì‹œì§€ ì¡°í•©
            full_prompt = f"""
{system_prompt}

ì‚¬ìš©ì ì§ˆë¬¸: {user_message}

ë‹µë³€:"""
            
            def generate():
                response = self.model.generate_content(
                    full_prompt,
                    generation_config=self._create_generation_config(max_tokens, temperature)
                )
                return response.text
            
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"Gemini ì¼ë°˜ ëŒ€í™” ìƒì„± ì‹¤íŒ¨: {e}")
            
            # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            if self._is_quota_exceeded(e):
                return self._get_intelligent_fallback_response(user_message, quota_exceeded=True)
            
            # ê¸°íƒ€ ì—ëŸ¬ ì‹œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ (í• ë‹¹ëŸ‰ ì´ˆê³¼ê°€ ì•„ë‹Œ ê²½ìš°)
            return self._get_intelligent_fallback_response(user_message, quota_exceeded=False)
    
    def _build_rag_prompt(self, question: str, context: str) -> str:
        """RAGìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±"""
        return f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì¤‘ìš”í•œ ê·œì¹™:
1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥´ê² ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”
4. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”
5. ì¶œì²˜ ì •ë³´ê°€ ìˆë‹¤ë©´ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
6. ë‹µë³€ì„ ì™„ì „íˆ ëê¹Œì§€ ì‘ì„±í•˜ì„¸ìš” - ì¤‘ê°„ì— ëŠì§€ ë§ˆì„¸ìš”
7. í‘œë‚˜ ëª©ë¡ì´ ìˆë‹¤ë©´ ëª¨ë“  í•­ëª©ì„ í¬í•¨í•˜ì„¸ìš”
8. ìƒì„¸í•˜ê³  ì™„ì„±ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ (ì™„ì „í•˜ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±):"""
    
    async def generate_summary(self, text: str, max_length: int = 200) -> str:
        """í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        try:
            if not self._initialized:
                await self.initialize()
            
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {max_length}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ ë‚´ìš©ì„ ë†“ì¹˜ì§€ ë§ê³  ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

ìš”ì•½:"""
            
            def generate():
                response = self.model.generate_content(
                    prompt,
                    generation_config=self._create_generation_config(max_length * 2, 0.3)  # í•œêµ­ì–´ íŠ¹ì„±ìƒ ì—¬ìœ ë¶„, ë‚®ì€ temperature
                )
                return response.text
                
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return text[:max_length] + "..."
    
    async def generate_keywords(self, text: str, max_keywords: int = 5) -> list:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            if not self._initialized:
                await self.initialize()
            
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ {max_keywords}ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ê° í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

í‚¤ì›Œë“œ:"""
            
            def generate():
                response = self.model.generate_content(prompt)
                keywords_text = response.text.strip()
                return [kw.strip() for kw in keywords_text.split(',')]
                
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_fallback_response(self, error: Exception) -> str:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ"""
        error_str = str(error).lower()
        if "quota" in error_str or "429" in error_str:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "rate limit" in error_str:
            return "í˜„ì¬ ì„œë¹„ìŠ¤ ì‚¬ìš©ëŸ‰ì´ ë§ì•„ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "api key" in error_str:
            return "API ì¸ì¦ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
        else:
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _get_quota_exceeded_response(self, question: str, context: str) -> str:
        """í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ë‹µë³€"""
        if not context or len(context.strip()) == 0:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        # ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ í‘œì‹œ
        context_lines = context.strip().split('\n')
        formatted_parts = []
        
        current_doc = ""
        current_content = []
        
        for line in context_lines:
            if line.startswith('[ë¬¸ì„œ'):
                # ì´ì „ ë¬¸ì„œ ì €ì¥
                if current_doc and current_content:
                    content_text = '\n'.join(current_content).strip()
                    if content_text:
                        formatted_parts.append(f"**{current_doc}**\n{content_text}")
                
                # ìƒˆ ë¬¸ì„œ ì‹œì‘
                current_doc = line.strip('[]')
                current_content = []
            elif line.strip():
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ë¬¸ì„œ ì €ì¥
        if current_doc and current_content:
            content_text = '\n'.join(current_content).strip()
            if content_text:
                formatted_parts.append(f"**{current_doc}**\n{content_text}")
        
        result = "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\n\n"
        result += f"'{question}' ì§ˆë¬¸ê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
        
        if formatted_parts:
            result += "\n\n".join(formatted_parts)
        else:
            result += context[:800] + ("..." if len(context) > 800 else "")
        
        return result
    
    def _get_intelligent_fallback_response(self, user_message: str, quota_exceeded: bool = False) -> str:
        """ì§€ëŠ¥ì ì¸ fallback ì‘ë‹µ ìƒì„±"""
        import re
        
        message_lower = user_message.lower()
        quota_msg = " í˜„ì¬ API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ ì œí•œì ì´ì§€ë§Œ," if quota_exceeded else ""
        
        # ì¸ì‚¬ ê´€ë ¨ íŒ¨í„´
        greeting_patterns = ['ì•ˆë…•', 'í•˜ì´', 'í—¬ë¡œ', 'ë°˜ê°€', 'ì²˜ìŒ', 'ì‹œì‘']
        if any(pattern in message_lower for pattern in greeting_patterns):
            return f"ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” RAG ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.{quota_msg} ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì‹œë©´ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•´ë“œë¦´ ìˆ˜ ìˆì–´ìš”."
        
        # ì •ì²´ì„±/ì†Œê°œ ê´€ë ¨ íŒ¨í„´  
        identity_patterns = ['ëˆ„êµ¬', 'ë­ì•¼', 'ë­í•˜ëŠ”', 'ì–´ë–¤', 'ì†Œê°œ', 'ìê¸°ì†Œê°œ', 'ì •ì²´', 'ì´ë¦„']
        if any(pattern in message_lower for pattern in identity_patterns):
            return f"ì €ëŠ” RAG(Retrieval Augmented Generation) ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.{quota_msg} ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ê²ƒì´ ì£¼ ê¸°ëŠ¥ì´ì—ìš”!"
        
        # ê¸°ëŠ¥/ëŠ¥ë ¥ ê´€ë ¨ íŒ¨í„´ (ë” í¬ê´„ì ìœ¼ë¡œ)
        function_patterns = ['ê¸°ëŠ¥', 'í•  ìˆ˜ ìˆ', 'ëŠ¥ë ¥', 'ë¬´ì—‡ì„', 'ì–´ë–»ê²Œ', 'ë°©ë²•', 'ë„ì›€', 'ì§€ì›', 'ì„œë¹„ìŠ¤', 
                           'í• ìˆ˜ìˆ', 'ê°€ëŠ¥í•œ', 'ì œê³µ', 'íŠ¹ì§•', 'ì¥ì ', 'ìš©ë„', 'ì—­í• ', 'ì¼', 'ì—…ë¬´']
        if any(pattern in message_lower for pattern in function_patterns):
            quota_note = "\n\ní˜„ì¬ API í• ë‹¹ëŸ‰ ì´ˆê³¼ì´ì§€ë§Œ ë¬¸ì„œ ì—…ë¡œë“œ í›„ ì§ˆë¬¸í•´ë³´ì„¸ìš”!" if quota_exceeded else "\n\në¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ê´€ë ¨ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!"
            return f"""ì €ì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ ë° ë¶„ì„ (PDF, Word, í…ìŠ¤íŠ¸ ë“±)
2. ğŸ” ì—…ë¡œë“œëœ ë¬¸ì„œì—ì„œ ì •ë³´ ê²€ìƒ‰ ë° ì§ˆì˜ì‘ë‹µ
3. ğŸŒ ë‹¤êµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ ì§€ì› (í•œêµ­ì–´, ì˜ì–´ ë“±)
4. ğŸ‘ï¸ OCRì„ í†µí•œ ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
5. ğŸ¯ ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ìœ ì‚¬ë„ ë§¤ì¹­
6. âš¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ{quota_note}"""
        
        # ì‚¬ìš©ë²•/ë°©ë²• ê´€ë ¨ íŒ¨í„´
        usage_patterns = ['ì‚¬ìš©', 'ì´ìš©', 'í™œìš©', 'ì‹œì‘', 'ì„¤ì •', 'ì„¤ì¹˜', 'ì‹¤í–‰', 'ì‘ë™', 'ìš´ì˜']
        if any(pattern in message_lower for pattern in usage_patterns):
            return f"""ì‚¬ìš© ë°©ë²•ì€ ê°„ë‹¨í•©ë‹ˆë‹¤:

1. ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ: PDF, Word, í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì‹œìŠ¤í…œì— ì—…ë¡œë“œ
2. â“ ì§ˆë¬¸í•˜ê¸°: ì—…ë¡œë“œí•œ ë¬¸ì„œì— ê´€ë ¨ëœ ì§ˆë¬¸ ì…ë ¥
3. ğŸ’¬ ë‹µë³€ ë°›ê¸°: AIê°€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ë‹µë³€ ì œê³µ
4. ğŸ”„ ì‹¤ì‹œê°„ ëŒ€í™”: ì¶”ê°€ ì§ˆë¬¸ìœ¼ë¡œ ë” ê¹Šì´ ìˆëŠ” ì •ë³´ íƒìƒ‰{quota_msg}"""
        
        # íŒŒì¼/ë¬¸ì„œ ê´€ë ¨ íŒ¨í„´
        file_patterns = ['íŒŒì¼', 'ë¬¸ì„œ', 'ì—…ë¡œë“œ', 'ì˜¬ë¦¬', 'ì§€ì›', 'í¬ë§·', 'í˜•ì‹', 'ì¢…ë¥˜']
        if any(pattern in message_lower for pattern in file_patterns):
            return f"""ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹:

ğŸ“„ ë¬¸ì„œ: PDF, Word (.docx), í…ìŠ¤íŠ¸ (.txt)
ğŸ–¼ï¸ ì´ë¯¸ì§€: JPG, PNG (OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
ğŸ“Š ê¸°íƒ€: ë§ˆí¬ë‹¤ìš´, CSV ë“±

ìµœëŒ€ 50MBê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.{quota_msg}"""
        
        # ê¸°ë³¸ ì‘ë‹µ
        if quota_exceeded:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì€ ì—¬ì „íˆ ê°€ëŠ¥í•˜ë‹ˆ, ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ê´€ë ¨ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?"
        else:
            return "ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”! ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì‹  í›„ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ë³´ì‹œê±°ë‚˜, ì €ì˜ ê¸°ëŠ¥ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ë¬¼ì–´ë³´ì„¸ìš”."
    
    def get_service_info(self) -> dict:
        """ì„œë¹„ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return {
            "service": "Google Gemini",
            "model": settings.GEMINI_MODEL or "gemini-2.0-flash-exp (default)",
            "initialized": self._initialized,
            "cache_info": self._cached_generate.cache_info()._asdict() if hasattr(self._cached_generate, 'cache_info') else {}
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self._cached_generate, 'cache_clear'):
            self._cached_generate.cache_clear()
        
        self._initialized = False
        self.model = None
        
        logger.info("Gemini ì„œë¹„ìŠ¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# ì „ì—­ Gemini ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiService()