import logging
from typing import Optional
import google.generativeai as genai
import asyncio
from functools import lru_cache

from app.core.config import settings

logger = logging.getLogger(__name__)


class GeminiService:
    """Google Gemini LLM ì„œë¹„ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model = None
        self._initialized = False
        
    async def initialize(self, test_connection: bool = False):
        """Gemini API ì´ˆê¸°í™”"""
        try:
            if not settings.GEMINI_API_KEY:
                logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ. Gemini ì„œë¹„ìŠ¤ ë¹„í™œì„±í™”")
                self._initialized = False
                return
            
            # Gemini API ì„¤ì •
            genai.configure(api_key=settings.GEMINI_API_KEY)
            
            # ëª¨ë¸ ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            model_name = settings.GEMINI_MODEL or "gemini-2.0-flash-lite"
            self.model = genai.GenerativeModel(model_name)
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸ (ì„ íƒì )
            if test_connection:
                try:
                    await self._test_connection()
                except Exception as e:
                    if "quota" in str(e).lower() or "429" in str(e):
                        logger.warning(f"Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼, ì—°ê²° í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°: {e}")
                    else:
                        logger.error(f"Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                        raise
            
            self._initialized = True
            logger.info("Gemini API ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            if "quota" in str(e).lower() or "429" in str(e):
                logger.warning(f"Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼, ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì´ˆê¸°í™”: {e}")
                # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œë¼ë„ ì´ˆê¸°í™”
                genai.configure(api_key=settings.GEMINI_API_KEY)
                model_name = settings.GEMINI_MODEL or "gemini-2.0-flash-lite"
                self.model = genai.GenerativeModel(model_name)
                self._initialized = True
                logger.info("Gemini API ê¸°ë³¸ ì´ˆê¸°í™” ì™„ë£Œ (ì—°ê²° í…ŒìŠ¤íŠ¸ ë¯¸ì‹¤í–‰)")
            else:
                logger.error(f"Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise
    
    async def _test_connection(self):
        """Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. ì—°ê²° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
            response = await asyncio.to_thread(
                self.model.generate_content,
                test_prompt
            )
            
            if not response.text:
                raise Exception("Gemini API ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                
            logger.info("Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
        except Exception as e:
            logger.error(f"Gemini API ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise
    
    def _is_quota_exceeded(self, error: Exception) -> bool:
        """í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ ì—¬ë¶€ í™•ì¸"""
        error_str = str(error).lower()
        return "quota" in error_str or "429" in error_str
    
    def _create_generation_config(self, max_tokens: int, temperature: float) -> genai.types.GenerationConfig:
        """ì¼ê´€ëœ GenerationConfig ìƒì„± - ë” ì™„ì „í•œ ë‹µë³€ì„ ìœ„í•œ ì„¤ì •"""
        return genai.types.GenerationConfig(
            max_output_tokens=max_tokens,
            temperature=temperature,
            top_p=0.95,  # ë” ë‹¤ì–‘í•œ ì‘ë‹µì„ ìœ„í•´
            top_k=40,   # í† í° ì„ íƒ ë²”ìœ„ í™•ì¥
            candidate_count=1,  # í•˜ë‚˜ì˜ ì™„ì „í•œ ë‹µë³€ ìƒì„±
            stop_sequences=[]   # ì¤‘ë‹¨ ì‹œí€€ìŠ¤ ì—†ìŒìœ¼ë¡œ ì™„ì „í•œ ë‹µë³€ ë³´ì¥
        )
    
    @lru_cache(maxsize=100)
    def _cached_generate(self, prompt_hash: str, prompt: str) -> str:
        """ìì£¼ ì‚¬ìš©ë˜ëŠ” í”„ë¡¬í”„íŠ¸ ìºì‹±"""
        if not self._initialized:
            raise RuntimeError("Gemini ì„œë¹„ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
        response = self.model.generate_content(prompt)
        return response.text
    
    async def generate_answer(
        self,
        question: str,
        context: str,
        max_tokens: int = 2000,  # ê¸°ë³¸ê°’ì„ 1000ì—ì„œ 2000ìœ¼ë¡œ ì¦ê°€
        temperature: float = 0.1
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        try:
            if not self._initialized:
                await self.initialize()
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_rag_prompt(question, context)
            
            # ì§§ì€ í”„ë¡¬í”„íŠ¸ëŠ” ìºì‹œ ì‚¬ìš©
            if len(prompt) < 500:
                prompt_hash = str(hash(prompt))
                try:
                    return self._cached_generate(prompt_hash, prompt)
                except Exception as e:
                    if self._is_quota_exceeded(e):
                        return self._get_quota_exceeded_response(question, context)
                    raise
            
            # ê¸´ í”„ë¡¬í”„íŠ¸ëŠ” ë¹„ë™ê¸° ì²˜ë¦¬
            def generate():
                response = self.model.generate_content(
                    prompt,
                    generation_config=self._create_generation_config(max_tokens, temperature)
                )
                return response.text
            
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"Gemini ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            if self._is_quota_exceeded(e):
                return self._get_quota_exceeded_response(question, context)
            return self._get_fallback_response(e)
    
    async def generate_with_system_prompt(
        self,
        system_prompt: str,
        user_message: str,
        max_tokens: int = 500,
        temperature: float = 0.7
    ) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ í•¨ê»˜ ë‹µë³€ ìƒì„± (ì¼ë°˜ ëŒ€í™”ìš©)"""
        try:
            if not self._initialized:
                await self.initialize()
            
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì‚¬ìš©ì ë©”ì‹œì§€ ì¡°í•©
            full_prompt = f"""
{system_prompt}

ì‚¬ìš©ì ì§ˆë¬¸: {user_message}

ë‹µë³€:"""
            
            def generate():
                response = self.model.generate_content(
                    full_prompt,
                    generation_config=self._create_generation_config(max_tokens, temperature)
                )
                return response.text
            
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"Gemini ì¼ë°˜ ëŒ€í™” ìƒì„± ì‹¤íŒ¨: {e}")
            
            # í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            if self._is_quota_exceeded(e):
                return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì€ ì—¬ì „íˆ ê°€ëŠ¥í•˜ë‹ˆ, ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ê´€ë ¨ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?"
            
            # ê¸°íƒ€ ì—ëŸ¬ ì‹œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜ (ì¬ê·€ í˜¸ì¶œ ë°©ì§€)
            return self._get_basic_fallback_response(user_message)
    
    def _build_rag_prompt(self, question: str, context: str) -> str:
        """RAGìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì„±"""
        return f"""ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì¤‘ìš”í•œ ê·œì¹™:
1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œì„ ì‚¬ìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥´ê² ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”
3. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”
4. ê°€ëŠ¥í•˜ë©´ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”
5. ì¶œì²˜ ì •ë³´ê°€ ìˆë‹¤ë©´ ì–¸ê¸‰í•´ì£¼ì„¸ìš”
6. ë‹µë³€ì„ ì™„ì „íˆ ëê¹Œì§€ ì‘ì„±í•˜ì„¸ìš” - ì¤‘ê°„ì— ëŠì§€ ë§ˆì„¸ìš”
7. í‘œë‚˜ ëª©ë¡ì´ ìˆë‹¤ë©´ ëª¨ë“  í•­ëª©ì„ í¬í•¨í•˜ì„¸ìš”
8. ìƒì„¸í•˜ê³  ì™„ì„±ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ (ì™„ì „í•˜ê³  ìƒì„¸í•˜ê²Œ ì‘ì„±):"""
    
    async def generate_summary(self, text: str, max_length: int = 200) -> str:
        """í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"""
        try:
            if not self._initialized:
                await self.initialize()
            
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {max_length}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ ë‚´ìš©ì„ ë†“ì¹˜ì§€ ë§ê³  ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

ìš”ì•½:"""
            
            def generate():
                response = self.model.generate_content(
                    prompt,
                    generation_config=self._create_generation_config(max_length * 2, 0.3)  # í•œêµ­ì–´ íŠ¹ì„±ìƒ ì—¬ìœ ë¶„, ë‚®ì€ temperature
                )
                return response.text
                
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
            return text[:max_length] + "..."
    
    async def generate_keywords(self, text: str, max_keywords: int = 5) -> list:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            if not self._initialized:
                await self.initialize()
            
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ {max_keywords}ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ê° í‚¤ì›Œë“œëŠ” ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ë‚˜ì—´í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}

í‚¤ì›Œë“œ:"""
            
            def generate():
                response = self.model.generate_content(prompt)
                keywords_text = response.text.strip()
                return [kw.strip() for kw in keywords_text.split(',')]
                
            return await asyncio.to_thread(generate)
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_fallback_response(self, error: Exception) -> str:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ"""
        error_str = str(error).lower()
        if "quota" in error_str or "429" in error_str:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "rate limit" in error_str:
            return "í˜„ì¬ ì„œë¹„ìŠ¤ ì‚¬ìš©ëŸ‰ì´ ë§ì•„ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        elif "api key" in error_str:
            return "API ì¸ì¦ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
        else:
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    def _get_quota_exceeded_response(self, question: str, context: str) -> str:
        """í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œ ì»¨í…ìŠ¤íŠ¸ í¬í•¨ ë‹µë³€"""
        if not context or len(context.strip()) == 0:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        
        # ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ í‘œì‹œ
        context_lines = context.strip().split('\n')
        formatted_parts = []
        
        current_doc = ""
        current_content = []
        
        for line in context_lines:
            if line.startswith('[ë¬¸ì„œ'):
                # ì´ì „ ë¬¸ì„œ ì €ì¥
                if current_doc and current_content:
                    content_text = '\n'.join(current_content).strip()
                    if content_text:
                        formatted_parts.append(f"**{current_doc}**\n{content_text}")
                
                # ìƒˆ ë¬¸ì„œ ì‹œì‘
                current_doc = line.strip('[]')
                current_content = []
            elif line.strip():
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ë¬¸ì„œ ì €ì¥
        if current_doc and current_content:
            content_text = '\n'.join(current_content).strip()
            if content_text:
                formatted_parts.append(f"**{current_doc}**\n{content_text}")
        
        result = "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.\n\n"
        result += f"'{question}' ì§ˆë¬¸ê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
        
        if formatted_parts:
            result += "\n\n".join(formatted_parts)
        else:
            result += context[:800] + ("..." if len(context) > 800 else "")
        
        return result
    
    async def _get_intelligent_fallback_response(self, user_message: str, quota_exceeded: bool = False) -> str:
        """ì§€ëŠ¥ì ì¸ fallback ì‘ë‹µ ìƒì„± - LLMì„ í™œìš©í•œ ìì—°ì–´ ì´í•´"""
        
        # API í• ë‹¹ëŸ‰ ì´ˆê³¼ ì‹œì—ëŠ” ê°„ë‹¨í•œ ê¸°ë³¸ ì‘ë‹µ
        if quota_exceeded:
            return "í˜„ì¬ Gemini API í• ë‹¹ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ì€ ì—¬ì „íˆ ê°€ëŠ¥í•˜ë‹ˆ, ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ê´€ë ¨ ì§ˆë¬¸ì„ í•´ë³´ì‹œê² ì–´ìš”?"
        
        # LLMì´ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°, ìì—°ì–´ë¡œ ì˜ë„ íŒŒì•… í›„ ì ì ˆí•œ ì‘ë‹µ ìƒì„±
        try:
            if self._initialized:
                system_prompt = """ë‹¹ì‹ ì€ RAG ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë©”ì‹œì§€ ì˜ë„ë¥¼ íŒŒì•…í•˜ê³  ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”.

ë‹¹ì‹ ì˜ ì •ë³´:
- ì´ë¦„: RAG ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸
- ì£¼ìš” ê¸°ëŠ¥: ë¬¸ì„œ ì—…ë¡œë“œ ë° ë¶„ì„, ì§ˆì˜ì‘ë‹µ, ë‹¤êµ­ì–´ ì§€ì›, OCR, ë²¡í„° ê²€ìƒ‰
- ì§€ì› íŒŒì¼: PDF, Word, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ (ìµœëŒ€ 50MB)
- íŠ¹ì§•: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°, ì •í™•í•œ ì •ë³´ ê²€ìƒ‰

ì‘ë‹µ ê°€ì´ë“œë¼ì¸:
1. ì¸ì‚¬/ì²« ë§Œë‚¨: ê°„ë‹¨í•œ ì†Œê°œì™€ ë¬¸ì„œ ì—…ë¡œë“œ ì•ˆë‚´
2. ì •ì²´ì„±/ì†Œê°œ ì§ˆë¬¸: RAG ì‹œìŠ¤í…œê³¼ ì£¼ìš” ê¸°ëŠ¥ ì„¤ëª…
3. ê¸°ëŠ¥/ì‚¬ìš©ë²• ì§ˆë¬¸: êµ¬ì²´ì ì¸ ì‚¬ìš© ë°©ë²•ê³¼ ê¸°ëŠ¥ ëª©ë¡ ì œê³µ
4. íŒŒì¼/ë¬¸ì„œ ê´€ë ¨: ì§€ì› í˜•ì‹ê³¼ ì—…ë¡œë“œ ë°©ë²• ì•ˆë‚´
5. ê¸°íƒ€: ë„ì›€ì´ ë˜ëŠ” ì¼ë°˜ì ì¸ ì•ˆë‚´

í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""

                response = await self.generate_with_system_prompt(
                    system_prompt=system_prompt,
                    user_message=user_message,
                    max_tokens=300,
                    temperature=0.7
                )
                return response
                
        except Exception as e:
            logger.warning(f"LLM ê¸°ë°˜ fallback ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}")
        
        # LLM ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
        return self._get_basic_fallback_response(user_message)
    
    def _get_basic_fallback_response(self, user_message: str) -> str:
        """ê¸°ë³¸ fallback ì‘ë‹µ (LLM ì‹¤íŒ¨ ì‹œ)"""
        return """ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” RAG ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ğŸ“„ **ì£¼ìš” ê¸°ëŠ¥:**
â€¢ ë¬¸ì„œ ì—…ë¡œë“œ ë° ë¶„ì„ (PDF, Word, í…ìŠ¤íŠ¸ ë“±)
â€¢ ì—…ë¡œë“œëœ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
â€¢ ë‹¤êµ­ì–´ ë¬¸ì„œ ì²˜ë¦¬ ì§€ì›
â€¢ OCRì„ í†µí•œ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
â€¢ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

ğŸ’¡ **ì‚¬ìš©ë²•:**
1. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”
2. ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ììœ ë¡­ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”
3. ì •í™•í•œ ë‹µë³€ì„ ë°›ì•„ë³´ì„¸ìš”!

ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ì£¼ì„¸ìš”! ğŸš€"""
    
    def get_service_info(self) -> dict:
        """ì„œë¹„ìŠ¤ ì •ë³´ ë°˜í™˜"""
        return {
            "service": "Google Gemini",
            "model": settings.GEMINI_MODEL or "gemini-2.0-flash-exp (default)",
            "initialized": self._initialized,
            "cache_info": self._cached_generate.cache_info()._asdict() if hasattr(self._cached_generate, 'cache_info') else {}
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if hasattr(self._cached_generate, 'cache_clear'):
            self._cached_generate.cache_clear()
        
        self._initialized = False
        self.model = None
        
        logger.info("Gemini ì„œë¹„ìŠ¤ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")


# ì „ì—­ Gemini ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiService()