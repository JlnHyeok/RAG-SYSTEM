"""
í•˜ì´ë¸Œë¦¬ë“œ RAG ì—”ì§„ (í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°)
ë¬¸ì„œ(Qdrant), êµ¬ì¡°í™” ë°ì´í„°(MongoDB), ì‹œê³„ì—´ ë°ì´í„°(InfluxDB)ë¥¼ 
í†µí•©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì‹œìŠ¤í…œì˜ í•µì‹¬ ì—”ì§„ì…ë‹ˆë‹¤.

ì œì¡° ë„ë©”ì¸ ë°ì´í„° ì†ŒìŠ¤:
- DOCUMENT: Qdrant ë¬¸ì„œ ê²€ìƒ‰ (ë§¤ë‰´ì–¼, ê°€ì´ë“œ)
- PRODUCTION: MongoDB ìƒì‚° ì´ë ¥
- ABNORMAL: MongoDB ì´ìƒê°ì§€ ì´ë ¥
- MACHINE: MongoDB ì„¤ë¹„ ì •ë³´
- TOOL: MongoDB ê³µêµ¬ ì •ë³´
- RAW_SENSOR: InfluxDB ì‹¤ì‹œê°„ ì„¼ì„œ ë°ì´í„°
"""

import logging
import asyncio
from typing import List, Dict, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum
import time
import re
from datetime import datetime, timedelta

# DB ë° ê²€ìƒ‰ ëª¨ë“ˆ
from app.core.db.mongodb_connector import get_mongodb_connector, MongoDBConnector, FilterCommon
from app.core.db.influxdb_connector import get_influxdb_connector, InfluxDBConnector
from app.core.retrieval.document_retriever import document_retriever, DocumentRetriever

# LLM ë° ì²˜ë¦¬ ëª¨ë“ˆ
from app.core.llm.gemini_service import gemini_service
from app.core.llm.question_classifier import QuestionClassifier, QuestionType, SensorQueryType
from app.core.llm.answer_generator import AnswerGenerator
from app.core.processing.text_processor import text_processor
from app.core.session.conversation_manager import conversation_manager

# ëª¨ë¸ ë° ì„¤ì •
from app.models.schemas import QueryRequest, QueryResponse, SearchResult
from app.core.config import settings

logger = logging.getLogger(__name__)


class DataSourceType(str, Enum):
    """ë°ì´í„° ì†ŒìŠ¤ ìœ í˜•"""
    DOCUMENT = "document"           # Qdrant ë¬¸ì„œ ê²€ìƒ‰
    PRODUCTION = "production"       # MongoDB ìƒì‚° ì´ë ¥
    ABNORMAL = "abnormal"           # MongoDB ì´ìƒê°ì§€
    MACHINE = "machine"             # MongoDB ì„¤ë¹„ ì •ë³´
    TOOL = "tool"                   # MongoDB ê³µêµ¬ ì •ë³´
    RAW_SENSOR = "raw_sensor"       # InfluxDB ì‹¤ì‹œê°„ ì„¼ì„œ
    USER = "user"                   # MongoDB ì‚¬ìš©ì ì •ë³´
    HYBRID = "hybrid"               # ë‹¤ì¤‘ ì†ŒìŠ¤ í†µí•©


# QuestionType -> DataSourceType ë§¤í•‘
QUESTION_TO_SOURCE_MAP = {
    QuestionType.DOCUMENT_QUERY: DataSourceType.DOCUMENT,
    QuestionType.PRODUCTION_QUERY: DataSourceType.PRODUCTION,
    QuestionType.ABNORMAL_QUERY: DataSourceType.ABNORMAL,
    QuestionType.MACHINE_QUERY: DataSourceType.MACHINE,
    QuestionType.TOOL_QUERY: DataSourceType.TOOL,
    QuestionType.RAW_SENSOR_QUERY: DataSourceType.RAW_SENSOR,
    QuestionType.USER_QUERY: DataSourceType.USER,
    QuestionType.HYBRID_QUERY: DataSourceType.HYBRID,
}


@dataclass
class QueryIntent:
    """ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ê²°ê³¼"""
    primary_source: DataSourceType
    secondary_sources: List[DataSourceType] = field(default_factory=list)
    entities: Dict[str, Any] = field(default_factory=dict)
    time_range: Optional[str] = None  # "1h", "24h", "7d" ë“±
    confidence: float = 0.0
    sensor_query_type: Optional[SensorQueryType] = None  # ì„¼ì„œ ì¿¼ë¦¬ ì„¸ë¶€ ìœ í˜•
    target_field: Optional[str] = None  # ì¡°íšŒ ëŒ€ìƒ í•„ë“œ (CT, Load ë“±)


@dataclass
class HybridContext:
    """í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°"""
    document_results: List[SearchResult] = field(default_factory=list)
    document_context: Optional[str] = None
    production_data: Optional[List[Dict]] = None
    abnormal_data: Optional[List[Dict]] = None
    machine_data: Optional[Dict] = None
    tool_data: Optional[List[Dict]] = None
    sensor_data: Optional[Dict] = None
    user_data: Optional[Dict] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class HybridRAGEngine:
    """
    í•˜ì´ë¸Œë¦¬ë“œ RAG ì—”ì§„
    
    ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ê³ , ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤(ë¬¸ì„œ, DB, ì„¼ì„œ)ì—ì„œ 
    ìµœì ì˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì§€ëŠ¥ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # ë°ì´í„° ì†ŒìŠ¤
        self.retriever: DocumentRetriever = document_retriever
        self.mongodb: MongoDBConnector = get_mongodb_connector()
        self.influxdb: InfluxDBConnector = get_influxdb_connector()
        
        # ì²˜ë¦¬ ì—”ì§„
        self.gemini = gemini_service
        self.text_processor = text_processor
        self.conversation_manager = conversation_manager
        
        # ì´ˆê¸°í™” ìƒíƒœ ë° ì˜ì¡´ì„± ì£¼ì… ê°ì²´
        self._initialized = False
        self.question_classifier: Optional[QuestionClassifier] = None
        self.answer_generator: Optional[AnswerGenerator] = None
    
    async def initialize(self):
        """í•˜ì´ë¸Œë¦¬ë“œ RAG ì—”ì§„ ë° ê´€ë ¨ ëª¨ë“ˆ ì´ˆê¸°í™”"""
        if self._initialized:
            return
        
        print("\n" + "="*50)
        print(f"ğŸš€ {settings.APP_NAME} ì´ˆê¸°í™” ì‹œì‘")
        print("="*50)
        
        try:
            # ëª¨ë“  í•˜ìœ„ ì„œë¹„ìŠ¤ ë³‘ë ¬ ì´ˆê¸°í™”
            results = await asyncio.gather(
                self.retriever.initialize(),
                self.gemini.initialize(test_connection=False),
                self.mongodb.initialize(),
                self.influxdb.initialize(),
                return_exceptions=True
            )
            
            # ê° ì„œë¹„ìŠ¤ë³„ ìƒíƒœ í™•ì¸ ë° ë¦¬í¬íŠ¸
            component_names = ["Qdrant", "Gemini", "MongoDB", "InfluxDB"]
            all_success = True
            
            print("\nğŸ“‹ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”:")
            for name, result in zip(component_names, results):
                if isinstance(result, Exception):
                    print(f"  âŒ {name.ljust(20)}: ì‹¤íŒ¨ ({str(result)})")
                    all_success = False
                else:
                    print(f"  âœ… {name.ljust(20)}: ì„±ê³µ")
            
            if not all_success:
                logger.warning("ì¼ë¶€ ì»´í¬ë„ŒíŠ¸ê°€ ì •ìƒì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # ì˜ì¡´ì„± ì£¼ì… ë° ê°ì²´ ìƒì„±
            self.question_classifier = QuestionClassifier(self.gemini, self.retriever.vector_store)
            self.answer_generator = AnswerGenerator(self.gemini)
            self.conversation_manager.set_gemini_service(self.gemini)
            
            self._initialized = True
            print("\n" + "="*50)
            print(f"âœ¨ {settings.APP_NAME} ì´ˆê¸°í™” ì™„ë£Œ!")
            print("="*50 + "\n")
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ RAG ì—”ì§„ ì´ˆê¸°í™” ì¹˜ëª…ì  ì‹¤íŒ¨: {e}")
            print(f"\nâŒ ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    async def query(self, request: QueryRequest, on_status: Optional[callable] = None) -> QueryResponse:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ í†µí•© í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        if not self._initialized:
            await self.initialize()
            
        # 1. ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬
        conversation_key = self.conversation_manager.get_conversation_key(
            request.user_id, 
            request.conversation_id
        )
        self.conversation_manager.ensure_history_exists(conversation_key)
        
        try:
            # 2. ëŒ€í™” ë§¥ë½ ë¶„ì„ ë° ì§ˆë¬¸ ë³´ì •
            if on_status: await on_status("ëŒ€í™” ë§¥ë½ íŒŒì•… ì¤‘...")
            context_aware_question, _ = await self.conversation_manager.analyze_question_context(
                request.question, conversation_key
            )
            
            # 3. ë©”íƒ€ ì§ˆë¬¸ ë° ì˜ë„ ë¶„ì„
            if on_status: await on_status("ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ì¤‘...")
            
            # 3.1 ì¼ë°˜ ë©”íƒ€ ì§ˆë¬¸ (ì¸ì‚¬, ë¬¸ì„œëª©ë¡ ë“±) ì²˜ë¦¬
            meta_response = await self.question_classifier.handle_meta_questions(
                context_aware_question, request.user_id
            )
            if meta_response:
                return self._create_simple_response(meta_response, request, conversation_key, start_time)
            
            # 3.2 ë°ì´í„° ì†ŒìŠ¤ ì˜ë„ ë¶„ì„
            intent = await self._analyze_intent(context_aware_question)
            logger.info(f"ë°ì´í„° ì†ŒìŠ¤ ì˜ë„: {intent.primary_source.value}, ì—”í‹°í‹°: {intent.entities}")
            
            # 4. ë°ì´í„° ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            context = await self._gather_context(intent, request, context_aware_question, on_status)
            
            # 5. ê²€ìƒ‰ ê²°ê³¼ ë° ë°ì´í„° ìœ ë¬´ì— ë”°ë¥¸ ë‹µë³€ ìƒì„± ì „ëµ
            if on_status: await on_status("ë‹µë³€ ìƒì„± ì¤‘...")
            
            # ë°ì´í„°ê°€ ì•„ë¬´ê²ƒë„ ì—†ëŠ” ê²½ìš° ì¼ë°˜ ëŒ€í™” ì‹œë„
            has_data = {
                "document": bool(context.document_results),
                "production": bool(context.production_data),
                "abnormal": bool(context.abnormal_data),
                "machine": bool(context.machine_data),
                "tool": bool(context.tool_data),
                "sensor": bool(context.sensor_data),
                "user": bool(context.user_data)
            }
            logger.info(f"ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸: {has_data}")
            
            if not any(has_data.values()):
                logger.info("ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ - ì¼ë°˜ ëŒ€í™”ë¡œ ì „í™˜")
                return await self._handle_general_conversation(request.question, context_aware_question, conversation_key, start_time)

            # ë‹µë³€ ìƒì„± (LLM)
            response = await self._generate_answer(request, context, actual_question=context_aware_question, history_key=conversation_key, start_time=start_time, intent=intent)
            
            proc_time = response.processing_time if response.processing_time is not None else 0.0
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ ì™„ë£Œ: {proc_time:.2f}ì´ˆ")
            return response
                
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return QueryResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                sources=[],
                confidence=0.0,
                processing_time=time.time() - start_time
            )

    async def _analyze_intent(self, question: str) -> QueryIntent:
        """ì§ˆë¬¸ ë¶„ì„ ë° ì˜ë„ íŒŒì•… (í†µí•© ë¶„ì„ ê²°ê³¼ ì‚¬ìš©)"""
        logger.info(f"ğŸ” ì˜ë„ ë¶„ì„ ì‹œì‘ - ì…ë ¥ ì§ˆë¬¸: '{question}'")
        
        # 1. í†µí•© ì§ˆë¬¸ ë¶„ì„ (ì˜ë„, ì—”í‹°í‹°, ì‹œê°„ ë²”ìœ„)
        # ì´ì œ classify_questionì´ QuestionAnalysisResultë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        analysis_result = await self.question_classifier.classify_question(question)
        
        question_type = analysis_result.primary_type
        primary_source = QUESTION_TO_SOURCE_MAP.get(question_type, DataSourceType.DOCUMENT)
        
        # 2. ë³´ì¡° ì†ŒìŠ¤ ê²°ì •
        secondary_sources = []
        for sec_type in analysis_result.secondary_types:
            source = QUESTION_TO_SOURCE_MAP.get(sec_type)
            if source and source not in secondary_sources and source != primary_source:
                secondary_sources.append(source)
                
        # 3. ì—”í‹°í‹° ë° ì‹œê°„ ë²”ìœ„ ì‚¬ìš©
        entities = analysis_result.entities
        time_range = analysis_result.time_range
        
        # [ì•ˆì „ì¥ì¹˜] machine_idê°€ ì¶”ì¶œë˜ì—ˆë‹¤ë©´ MACHINE ì†ŒìŠ¤ ìë™ ì¶”ê°€
        if entities.get("machine_id") and primary_source != DataSourceType.MACHINE:
             if DataSourceType.MACHINE not in secondary_sources:
                secondary_sources.append(DataSourceType.MACHINE)
                logger.info("ì—”í‹°í‹° ê¸°ë°˜ MACHINE ì†ŒìŠ¤ ìë™ ì¶”ê°€")

        # 4. ë³µí•© ì§ˆë¬¸(HYBRID)ì¸ ê²½ìš° ë¬¸ì„œëŠ” ê¸°ë³¸ í¬í•¨
        if primary_source == DataSourceType.HYBRID:
            if DataSourceType.DOCUMENT not in secondary_sources:
                secondary_sources.append(DataSourceType.DOCUMENT)

        logger.info(f"ğŸ” ë¶„ì„ ê²°ê³¼ - Primary: {primary_source}, Secondary: {secondary_sources}, Time: {time_range}, Entities: {entities}")
        
        return QueryIntent(
            primary_source=primary_source,
            secondary_sources=secondary_sources,
            entities=entities,
            time_range=time_range,
            confidence=0.9,
            sensor_query_type=analysis_result.sensor_query_type,
            target_field=analysis_result.target_field
        )
    


    async def _gather_context(
        self, 
        intent: QueryIntent, 
        request: QueryRequest, 
        question: str,
        on_status: Optional[callable]
    ) -> HybridContext:
        """ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ë³‘ë ¬ ìˆ˜ì§‘"""
        context = HybridContext()
        tasks = []
        source_keys = []
        
        # [NEW] machine_idê°€ ìˆë‹¤ë©´ ì„¤ë¹„ ì •ë³´ë¥¼ ë¨¼ì € ì¡°íšŒí•˜ì—¬ ì •í™•í•œ í•„í„° ì •ë³´ êµ¬ì„±
        # (ê¸°ë³¸ê°’ F01 ëŒ€ì‹  ì‹¤ì œ workshopCode ë“±ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•¨ - ë°±ì—”ë“œ ë¡œì§ ì¼ì¹˜í™”)
        if intent.entities.get("machine_id"):
            try:
                mid = intent.entities["machine_id"]
                machine_info = await self.mongodb.get_machine_by_code(mid)
                if machine_info:
                    intent.entities["workshop_id"] = machine_info.get("workshopCode")
                    intent.entities["line_id"] = machine_info.get("lineCode")
                    intent.entities["op_code"] = machine_info.get("opCode")
                    logger.info(f"ì„¤ë¹„ ì •ë³´ ê¸°ë°˜ í•„í„° ì—…ë°ì´íŠ¸: {intent.entities}")
            except Exception as e:
                logger.warning(f"ì„¤ë¹„ ì •ë³´ ì„ í–‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # ê¸°ë³¸ í•„í„° ìƒì„±
        filter_common = self._create_filter_common(intent.entities)
        
        sources = list(set([intent.primary_source] + intent.secondary_sources))
        
        for source in sources:
            if source == DataSourceType.DOCUMENT:
                if on_status: await on_status("ë¬¸ì„œ ì§€ì‹ ê²€ìƒ‰ ì¤‘...")
                tasks.append(self.retriever.search(question, request.user_id, request.max_results, request.score_threshold))
                source_keys.append("document")
                
            elif source == DataSourceType.PRODUCTION:
                if on_status: await on_status("ìƒì‚° ì´ë ¥ ì¡°íšŒ ì¤‘...")
                hours = self._time_range_to_hours(intent.time_range)
                tasks.append(self._get_production_data(filter_common, hours))
                source_keys.append("production")
                
            elif source == DataSourceType.ABNORMAL:
                if on_status: await on_status("ì´ìƒê°ì§€ ì´ë ¥ ì¡°íšŒ ì¤‘...")
                hours = self._time_range_to_hours(intent.time_range)
                tasks.append(self._get_abnormal_data(filter_common, hours, intent.entities.get("abnormal_code")))
                source_keys.append("abnormal")
                
            elif source == DataSourceType.MACHINE:
                if on_status: await on_status("ì„¤ë¹„ ì •ë³´ ì¡°íšŒ ì¤‘...")
                tasks.append(self._get_machine_data(intent.entities))
                source_keys.append("machine")
                
            elif source == DataSourceType.TOOL:
                if on_status: await on_status("ê³µêµ¬ ì •ë³´ ì¡°íšŒ ì¤‘...")
                # time_range ì „ë‹¬ì„ ìœ„í•´ entities ë³µì‚¬ ë° ì¶”ê°€
                tool_entities = intent.entities.copy()
                tool_entities["time_range"] = intent.time_range
                tasks.append(self._get_tool_data(tool_entities))
                source_keys.append("tool")
                
            elif source == DataSourceType.RAW_SENSOR:
                if on_status: await on_status("ì„¼ì„œ ë°ì´í„° ì¡°íšŒ ì¤‘...")
                hours = self._time_range_to_hours(intent.time_range)
                tasks.append(self._get_sensor_data(filter_common, hours, intent.sensor_query_type, intent.target_field))
                source_keys.append("sensor")
            
            elif source == DataSourceType.USER:
                if on_status: await on_status("ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ ì¤‘...")
                tasks.append(self._get_user_data(intent.entities))
                source_keys.append("user")

        if not tasks: 
            return context

        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for key, res in zip(source_keys, results):
            if isinstance(res, Exception): 
                logger.warning(f"{key} ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {res}")
                continue
            if key == "document": 
                context.document_results = res
                context.document_context = self.text_processor.build_context(res)
            elif key == "production": 
                context.production_data = res
            elif key == "abnormal": 
                context.abnormal_data = res
            elif key == "machine": 
                context.machine_data = res
            elif key == "tool": 
                context.tool_data = res
            elif key == "sensor": 
                context.sensor_data = res
            elif key == "user":
                context.user_data = res
            
        return context
    
    def _create_filter_common(self, entities: Dict) -> Optional[FilterCommon]:
        """ì—”í‹°í‹°ì—ì„œ FilterCommon ìƒì„±"""
        workshop_id = entities.get("workshop_id") or settings.DEFAULT_WORKSHOP_ID
        line_id = entities.get("line_id") or settings.DEFAULT_LINE_ID
        op_code = entities.get("op_code") or settings.DEFAULT_OP_CODE
        machine_id = entities.get("machine_id")
        
        # í•„ìˆ˜ í•„ë“œê°€ ì—†ìœ¼ë©´ None ë°˜í™˜
        if not workshop_id or not line_id or not op_code:
            return None
        
        return FilterCommon(
            workshop_id=workshop_id,
            line_id=line_id,
            op_code=op_code,
            machine_id=machine_id
        )
    
    def _time_range_to_hours(self, time_range: Optional[str]) -> int:
        """ì‹œê°„ ë²”ìœ„ ë¬¸ìì—´ì„ ì‹œê°„ ë‹¨ìœ„ë¡œ ë³€í™˜ (ì˜ˆ: 1ë…„, 3ê°œì›”, 2ì£¼, 1d, 24h)"""
        if not time_range:
            return 24
            
        try:
            # ì •ê·œì‹ìœ¼ë¡œ ìˆ«ìì™€ ë‹¨ìœ„ ì¶”ì¶œ
            # ì˜ˆ: "1ë…„", "1y", "3ê°œì›”", "30d"
            match = re.search(r'(\d+)\s*(ë…„|y|ê°œì›”|m|ë‹¬|ì£¼|w|ì¼|d|ì‹œê°„|h|ë¶„|min)', time_range.lower())
            
            if match:
                val = int(match.group(1))
                unit = match.group(2)
                
                if unit in ['ë…„', 'y', 'year', 'years']:
                    return val * 365 * 24
                if unit in ['ê°œì›”', 'm', 'mon', 'month', 'months', 'ë‹¬']:
                    return val * 30 * 24
                if unit in ['ì£¼', 'w', 'week', 'weeks']:
                    return val * 7 * 24
                if unit in ['ì¼', 'd', 'day', 'days']:
                    return val * 24
                if unit in ['ì‹œê°„', 'h', 'hour', 'hours']:
                    return val
                if unit in ['ë¶„', 'min', 'minute', 'minutes']:
                    return max(1, int(val / 60))
            
            # ê¸°ì¡´ ë¡œì§ (Fallback)
            if time_range.endswith("h"):
                return int(time_range[:-1])
            elif time_range.endswith("d"):
                return int(time_range[:-1]) * 24
                
        except Exception as e:
            logger.warning(f"ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ ({time_range}): {e}")
            
        return 24
    
    # ============ ë°ì´í„° ì¡°íšŒ ë©”ì„œë“œ ============
    
    async def _get_production_data(self, filter_common: Optional[FilterCommon], hours: int) -> List[Dict]:
        """ìƒì‚° ì´ë ¥ ì¡°íšŒ"""
        if not filter_common:
            return []
        
        try:
            products = await self.mongodb.get_recent_products(filter_common, hours, limit=50)
            stats = await self.mongodb.get_product_stats(filter_common, hours)
            
            return {
                "recent_products": products[:30],  # ìµœê·¼ 30ê±´
                "stats": stats,
                "total_count": len(products)
            }
        except Exception as e:
            logger.error(f"ìƒì‚° ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def _get_abnormal_data(
        self, 
        filter_common: Optional[FilterCommon], 
        hours: int,
        abnormal_code: Optional[str] = None,
        product_no: Optional[str] = None
    ) -> Dict:
        """
        ì´ìƒê°ì§€ ë°ì´í„° ì¡°íšŒ - abnormalsì™€ abnormalSummary ë³‘ë ¬ ì¡°íšŒ
        
        Args:
            filter_common: ê³µí†µ í•„í„°
            hours: ì¡°íšŒ ê¸°ê°„ (ì‹œê°„)
            abnormal_code: íŠ¹ì • ì´ìƒ ì½”ë“œ (CT, LOAD, AI)
            product_no: íŠ¹ì • ìƒì‚° ë²ˆí˜¸
        
        Returns:
            {
                "summary_records": abnormalSummary ë ˆì½”ë“œ (CT/LOAD/AI í†µí•© íŒì •),
                "recent_abnormals": abnormals ìƒì„¸ ì´ë ¥,
                "stats": ì§‘ê³„ í†µê³„ (ìœ í˜•ë³„ ê±´ìˆ˜)
            }
        """
        if not filter_common:
            return {}
        
        try:
            # ë‘ ì»¬ë ‰ì…˜ì„ ë³‘ë ¬ë¡œ ì¡°íšŒ
            summary_records_task = self.mongodb.get_abnormal_summary_records(
                filter_common, 
                product_no=product_no,
                hours=hours
            )
            
            if abnormal_code:
                details_task = self.mongodb.get_abnormals_by_code(filter_common, abnormal_code, hours)
            else:
                details_task = self.mongodb.get_recent_abnormals(filter_common, hours)
            
            stats_task = self.mongodb.get_abnormal_summary(filter_common, hours)
            
            # ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
            summary_records, abnormals, stats = await asyncio.gather(
                summary_records_task,
                details_task,
                stats_task
            )
            
            return {
                "summary_records": summary_records,  # abnormalSummary ì»¬ë ‰ì…˜ (ìƒì‚°í’ˆë³„ CT/LOAD/AI íŒì •)
                "recent_abnormals": abnormals[:30],  # abnormals ì»¬ë ‰ì…˜ (ìƒì„¸ ì´ë²¤íŠ¸)
                "stats": stats,                       # ì§‘ê³„ í†µê³„
                "total_count": len(abnormals)
            }
        except Exception as e:
            logger.error(f"ì´ìƒê°ì§€ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    
    async def _get_machine_data(self, entities: Dict) -> Dict:
        """ì„¤ë¹„ ì •ë³´ ì¡°íšŒ"""
        machine_id = entities.get("machine_id")
        
        try:
            if machine_id:
                logger.info(f"ì„¤ë¹„ ë‹¨ê±´ ì¡°íšŒ: {machine_id}")
                machine = await self.mongodb.get_machine_by_code(machine_id)
                if machine:
                    threshold = await self.mongodb.get_threshold_by_machine(machine_id)
                    tools = await self.mongodb.get_tools_by_machine(machine_id)
                    
                    # InfluxDB ê°€ë™ ì‹œê°„ ì¡°íšŒ ë° ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ
                    runtime = {}
                    tool_counts = []
                    
                    # machine ì •ë³´ì—ì„œ í•„í„° ê°’ ì§ì ‘ ì¶”ì¶œ (machineMaster í•„ë“œ ì‚¬ìš©) ë˜ëŠ” ê¸°ë³¸ê°’
                    workshop_code = machine.get("workshopCode") or settings.DEFAULT_WORKSHOP_ID
                    line_code = machine.get("lineCode") or settings.DEFAULT_LINE_ID
                    op_code = machine.get("opCode") or settings.DEFAULT_OP_CODE
                    
                    logger.info(f"Machine í•„í„° ì •ë³´: workshop={workshop_code}, line={line_code}, op={op_code}")
                    
                    # FilterCommon ì§ì ‘ ìƒì„± (machine ì •ë³´ ê¸°ë°˜)
                    if workshop_code and line_code and op_code:
                        filter_common = FilterCommon(
                            workshop_id=workshop_code,
                            line_id=line_code,
                            op_code=op_code,
                            machine_id=machine_id
                        )
                        logger.info(f"FilterCommon ìƒì„±: {filter_common}")
                        
                        # ê°€ë™ ì‹œê°„ ì¡°íšŒ (InfluxDB - ì‹¤íŒ¨í•´ë„ ì§„í–‰)
                        try:
                            if filter_common.did:
                                hour_range = self._time_range_to_hours(entities.get("time_range"))
                                runtime = await self.influxdb.get_machine_runtime(filter_common, hours=hour_range)
                        except Exception as e:
                            logger.error(f"ê°€ë™ ì‹œê°„ ì¡°íšŒ ì‹¤íŒ¨ (InfluxDB ë¬´ì‹œ): {e}")

                        # ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (MongoDB - í•„ìˆ˜)
                        try:
                            tool_counts = await self.mongodb.get_current_tool_counts(filter_common)
                            logger.info(f"ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ê²°ê³¼: {len(tool_counts)}ê°œ")
                        except Exception as e:
                            logger.error(f"ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    else:
                        logger.warning(f"FilterCommon ìƒì„± ì‹¤íŒ¨: í•„ìˆ˜ í•„ë“œ ëˆ„ë½")
                    
                    return {"machine": machine, "threshold": threshold, "tools": tools, "runtime": runtime, "tool_counts": tool_counts}
            else:
                logger.info("ì„¤ë¹„ ì „ì²´ ëª©ë¡ ì¡°íšŒ ì‹œì‘")
                machines = await self.mongodb.get_all_machines()
                logger.info(f"ì¡°íšŒëœ ì„¤ë¹„ ìˆ˜: {len(machines)}")
                if machines:
                    logger.info(f"ì„¤ë¹„ ì½”ë“œ ëª©ë¡: {[m.get('machineCode') for m in machines]}")
                
                # ê° ì„¤ë¹„ë³„ ê³µêµ¬ ì •ë³´, ì‚¬ìš©ëŸ‰, ì„ê³„ì¹˜ ì¡°íšŒ
                machines_with_tools = []
                for machine in machines[:20]:  # ìµœëŒ€ 20ê°œ ì„¤ë¹„
                    machine_code = machine.get("machineCode")
                    if machine_code:
                        # 1. ê³µêµ¬ ë§ˆìŠ¤í„° ì¡°íšŒ
                        tools = await self.mongodb.get_tools_by_machine(machine_code)
                        machine["tools"] = tools
                        
                        # 2. ì„ê³„ì¹˜ ì¡°íšŒ
                        try:
                            threshold = await self.mongodb.get_threshold_by_machine(machine_code)
                            if threshold:
                                machine["threshold"] = threshold
                        except Exception as e:
                            logger.error(f"ì„¤ë¹„ {machine_code} ì„ê³„ì¹˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                        
                        # 3. ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (FilterCommon ìƒì„± í•„ìš”)
                        try:
                            workshop_code = machine.get("workshopCode") or settings.DEFAULT_WORKSHOP_ID
                            line_code = machine.get("lineCode") or settings.DEFAULT_LINE_ID
                            op_code = machine.get("opCode") or settings.DEFAULT_OP_CODE
                            
                            if workshop_code and line_code and op_code:
                                filter_common = FilterCommon(
                                    workshop_id=workshop_code,
                                    line_id=line_code,
                                    op_code=op_code,
                                    machine_id=machine_code
                                )
                                tool_counts = await self.mongodb.get_current_tool_counts(filter_common)
                                machine["tool_counts"] = tool_counts
                        except Exception as e:
                            logger.error(f"ì„¤ë¹„ {machine_code} ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                            
                    machines_with_tools.append(machine)
                
                return {"machines": machines_with_tools, "total_count": len(machines)}
        except Exception as e:
            logger.error(f"ì„¤ë¹„ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _get_tool_data(self, entities: Dict) -> Dict:
        """ê³µêµ¬ ì •ë³´ ì¡°íšŒ (InfluxDB í†µê³„ + MongoDB ì´ë ¥/í˜„í™©)"""
        tool_code = entities.get("tool_code")
        filter_common = self._create_filter_common(entities)
        hours = self._time_range_to_hours(entities.get("time_range"))
        
        if not filter_common:
            return {}

        tasks = []
        task_keys = []
        
        # 1. InfluxDB ì‚¬ìš© í†µê³„ (ê¸°ì¡´ ë¡œì§)
        tasks.append(self.influxdb.get_tool_stats(filter_common, hours=hours))
        task_keys.append("usage_stats")
        
        # 2. MongoDB ê³µêµ¬ í˜„ì¬ ìˆ˜ëª… í˜„í™© (ê¸°ì¡´ ë¡œì§)
        # ì„¤ë¹„ IDê°€ ìˆì„ ë•Œë§Œ ì¡°íšŒ ê°€ëŠ¥
        if filter_common.machine_id:
            tasks.append(self.mongodb.get_current_tool_counts(filter_common))
            task_keys.append("tool_counts")
        
        # 3. [NEW] MongoDB ê³µêµ¬ ìƒì„¸ ì´ë ¥ (tool_codeê°€ ìˆì„ ë•Œ)
        if tool_code:
            tasks.append(self.mongodb.get_tool_history(filter_common, tool_code, hours=hours))
            task_keys.append("history")
            
            tasks.append(self.mongodb.get_tool_usage_stats(filter_common, tool_code, hours=hours))
            task_keys.append("history_stats")
            
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            data = {}
            for key, res in zip(task_keys, results):
                if isinstance(res, Exception):
                    logger.error(f" [ToolDebug] {key} ì¡°íšŒ ì‹¤íŒ¨: {res}")
                    data[key] = [] if key in ["usage_stats", "tool_counts", "history"] else {}
                else:
                    data[key] = res
                    
            return data
            
        except Exception as e:
            logger.error(f"ê³µêµ¬ ë°ì´í„° í†µí•© ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}



    async def _get_user_data(self, entities: Dict) -> Dict:
        """ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ (ë¹„ë°€ë²ˆí˜¸ ì œì™¸)"""
        user_id = entities.get("user_id")
        
        try:
            if user_id and user_id != "ALL":
                # íŠ¹ì • ì‚¬ìš©ì ì¡°íšŒ
                logger.info(f"ì‚¬ìš©ì ë‹¨ê±´ ì¡°íšŒ: {user_id}")
                user = await self.mongodb.get_user_by_id(user_id)
                if user:
                    return {"user": user}
                else:
                    logger.warning(f"ì‚¬ìš©ì {user_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return {}
            else:
                # ì „ì²´ ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ
                logger.info("ì „ì²´ ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ")
                users = await self.mongodb.get_all_users()
                return {"users": users, "total_count": len(users)}
        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    

    
    async def _get_sensor_data(
        self, 
        filter_common: Optional[FilterCommon], 
        hours: int = 1,
        sensor_query_type: Optional[SensorQueryType] = None,
        target_field: Optional[str] = None
    ) -> Dict:
        """ì„¼ì„œ ë°ì´í„° ì¡°íšŒ - target_field ê¸°ë°˜ ë™ì  ë¼ìš°íŒ…"""
        if not filter_common:
            return {}
        
        try:
            result = {}
            
            # ë™ì  í•„ë“œ íŒŒì‹± (ì½¤ë§ˆ êµ¬ë¶„ ì§€ì›)
            raw_target = target_field or "Load"
            target_fields = [t.strip() for t in raw_target.split(",")]
            
            # ê¸°ë³¸ í•„ë“œ ë° measurement ì„¤ì • (TREND, ê¸°ë³¸ ì¼€ì´ìŠ¤ì—ì„œ ì‚¬ìš©)
            field = target_fields[0] if target_fields else "Load"
            measurement = None  # ê¸°ë³¸ê°’
            
            measurement_map = {
                "CT": settings.INFLUXDB_MEASUREMENT_PRODUCT,
                # í•„ìš”ì‹œ ì¶”ê°€ ë§¤í•‘
            }
            
            # sensor_query_typeì— ë”°ë¥¸ ë¶„ê¸° ë¡œì§
            if sensor_query_type == SensorQueryType.CURRENT_STATUS:
                # í˜„ì¬ ìƒíƒœë§Œ ì¡°íšŒ
                result["current_status"] = await self.influxdb.get_current_status(filter_common)
                
            elif sensor_query_type == SensorQueryType.RUNNING_STATS:
                # ê°€ë™ ì¤‘ í†µê³„ (ì²« ë²ˆì§¸ í•„ë“œ ê¸°ì¤€)
                field = target_fields[0]
                result["running_stats"] = await self.influxdb.get_running_stats(filter_common, hours=hours, field=field)
                
            elif sensor_query_type == SensorQueryType.RAW_STATS or sensor_query_type == SensorQueryType.CT_STATS: 
                # ì „ì²´ í†µê³„ (í‰ê· /ìµœëŒ€/ìµœì†Œ) - ë‹¤ì¤‘ í•„ë“œ ì§€ì›
                days = max(1, hours // 24)
                
                for t_field in target_fields:
                    measurement = measurement_map.get(t_field)
                    stats_res = {}
                    key_type = ""
                    
                    if days > 84:  # 12ì£¼ ì´ˆê³¼
                        months = max(1, days // 30)
                        stats_res = await self.influxdb.get_monthly_stats(filter_common, months=months, field=t_field, measurement=measurement)
                        key_type = "monthly_stats"
                    elif days > 30:
                        weeks = max(1, days // 7)
                        stats_res = await self.influxdb.get_weekly_stats(filter_common, weeks=weeks, field=t_field, measurement=measurement)
                        key_type = "weekly_stats"
                    elif days > 1:
                        stats_res = await self.influxdb.get_daily_stats(filter_common, days=days, field=t_field, measurement=measurement)
                        key_type = "daily_stats"
                    else:
                        stats_res = await self.influxdb.get_raw_stats(filter_common, hours=hours, field=t_field, measurement=measurement)
                        key_type = "raw_stats"
                    
                    # ê²°ê³¼ ì €ì¥ (stats_{Field}) - í¬ë§·í„°ì—ì„œ ì‹ë³„ ìš©ì´í•˜ê²Œ
                    result[f"stats_{t_field}"] = {"type": key_type, "data": stats_res}
                
            elif sensor_query_type == SensorQueryType.TREND:
                # íŠ¸ë Œë“œ ì¡°íšŒ
                result["trend"] = await self.influxdb.get_raw_trend(filter_common, hours=hours, interval="1h", field=field, measurement=measurement)
                
            elif sensor_query_type == SensorQueryType.RUNTIME:
                # ê°€ë™ ì‹œê°„/ë¥  - ê¸°ê°„ë³„ ìë™ ì§‘ê³„ ë‹¨ìœ„ ì„ íƒ
                # â‰¤30ì¼: ì¼ë³„, 31-84ì¼(12ì£¼): ì£¼ë³„, >84ì¼: ì›”ë³„
                days = max(1, hours // 24)
                if days > 84:  # 12ì£¼ ì´ˆê³¼
                    months = max(1, days // 30)
                    result["monthly_runtime"] = await self.influxdb.get_monthly_runtime(filter_common, months=months)
                elif days > 30:
                    weeks = max(1, days // 7)
                    result["weekly_runtime"] = await self.influxdb.get_weekly_runtime(filter_common, weeks=weeks)
                else:
                    result["daily_runtime"] = await self.influxdb.get_daily_runtime(filter_common, days=days)
                
            else:
                # ê¸°ë³¸: í˜„ì¬ ìƒíƒœ + ê°€ë™ ì¤‘ í†µê³„ + ì˜¤ëŠ˜ ê°€ë™ ì‹œê°„
                result["current_status"] = await self.influxdb.get_current_status(filter_common)
                result["running_stats"] = await self.influxdb.get_running_stats(filter_common, hours=hours, field=field)
                # ê¸ˆì¼ ê°€ë™ ì‹œê°„ ì¶”ê°€ (10-1 ì„¤ë¹„ì˜ ì˜¤ëŠ˜ ê°€ë™ ì‹œê°„ ë“±)
                result["today_runtime"] = await self.influxdb.get_today_runtime(filter_common)
            
            return result
        except Exception as e:
            logger.error(f"ì„¼ì„œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    async def _generate_answer(
        self, 
        request: QueryRequest, 
        context: HybridContext, 
        actual_question: str,
        history_key: str,
        start_time: float,
        intent: Optional[QueryIntent] = None
    ) -> QueryResponse:
        """LLMì„ í†µí•œ í•˜ì´ë¸Œë¦¬ë“œ ë‹µë³€ ìƒì„±"""
        
        # ì§ˆë¬¸ ìœ í˜• ê²°ì • (intentì˜ primary_sourceë¥¼ QuestionType í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
        question_type = "DOCUMENT_QUERY"
        if intent:
            source_to_question_type = {
                DataSourceType.DOCUMENT: "DOCUMENT_QUERY",
                DataSourceType.PRODUCTION: "PRODUCTION_QUERY",
                DataSourceType.ABNORMAL: "ABNORMAL_QUERY",
                DataSourceType.MACHINE: "MACHINE_QUERY",
                DataSourceType.TOOL: "TOOL_QUERY",
                DataSourceType.RAW_SENSOR: "RAW_SENSOR_QUERY",
                DataSourceType.USER: "USER_QUERY",
                DataSourceType.HYBRID: "HYBRID_QUERY",
            }
            question_type = source_to_question_type.get(intent.primary_source, "DOCUMENT_QUERY")
        
        logger.info(f"ë‹µë³€ ìƒì„± - ì§ˆë¬¸ ìœ í˜•: {question_type}")
        
        # 1. í•˜ì´ë¸Œë¦¬ë“œ ì •ë³´ í¬ë§·íŒ…
        info_blocks = []
        if context.document_context:
            info_blocks.append(f"[ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©]\n{context.document_context}")
        if context.production_data:
            info_blocks.append(f"[ìƒì‚° ì´ë ¥ ë°ì´í„°]\n{self._format_production_data(context.production_data)}")
        if context.abnormal_data:
            info_blocks.append(f"[ì´ìƒê°ì§€ í˜„í™©]\n{self._format_abnormal_data(context.abnormal_data)}")
        if context.machine_data:
            info_blocks.append(f"[ì„¤ë¹„ ì •ë³´]\n{self._format_machine_data(context.machine_data)}")
        if context.tool_data:
            info_blocks.append(f"[ê³µêµ¬ ì •ë³´]\n{self._format_tool_data(context.tool_data)}")
        if context.sensor_data:
            info_blocks.append(f"[ì‹¤ì‹œê°„ ì„¼ì„œ ë°ì´í„°]\n{self._format_sensor_data(context.sensor_data)}")
        if context.user_data:
            info_blocks.append(f"[ì‚¬ìš©ì ì •ë³´]\n{self._format_user_data(context.user_data)}")
            
        full_context = "\n\n".join(info_blocks)
        
        # 2. ë‹µë³€ ìƒì„± í˜¸ì¶œ (ì§ˆë¬¸ ìœ í˜• í¬í•¨)
        answer = await self.answer_generator.generate_intelligent_answer(
            request.question,
            full_context,
            actual_question,
            self.conversation_manager.get_recent_history(history_key),
            self.conversation_manager.format_history_for_prompt,
            question_type=question_type
        )
        
        # 3. ê²°ê³¼ êµ¬ì„±
        confidence = self.retriever.calculate_confidence(context.document_results) if context.document_results else 0.8
        
        self.conversation_manager.add_to_history(
            history_key, request.question, answer, context.document_results, confidence
        )
        
        # ì‚¬ìš©ëœ ë°ì´í„° ì†ŒìŠ¤
        sources_used = []
        if context.document_context: sources_used.append("document")
        if context.production_data: sources_used.append("production")
        if context.abnormal_data: sources_used.append("abnormal")
        if context.machine_data: sources_used.append("machine")
        if context.tool_data: sources_used.append("tool")
        if context.sensor_data: sources_used.append("sensor")
        if context.user_data: sources_used.append("user")
        
        return QueryResponse(
            answer=answer,
            sources=context.document_results,
            confidence=confidence,
            processing_time=time.time() - start_time,
            metadata={"sources_used": sources_used}
        )
    
    # ============ ë°ì´í„° í¬ë§·íŒ… ë©”ì„œë“œ ============
    
    def _to_kst(self, date_val: Union[datetime, str, None]) -> str:
        """UTC ì‹œê°„ì„ KST(UTC+9) ë¬¸ìì—´ë¡œ ë³€í™˜"""
        if not date_val:
            return "-"
        
        try:
            # ë¬¸ìì—´ì¸ ê²½ìš° datetimeìœ¼ë¡œ íŒŒì‹±
            if isinstance(date_val, str):
                # ISO í¬ë§· ì²˜ë¦¬ (milliseconds, Z ë“±)
                date_val = date_val.replace('Z', '+00:00')
                try:
                    dt = datetime.fromisoformat(date_val)
                except ValueError:
                    # fromisoformatì´ ì‹¤íŒ¨í•˜ë©´ ë‚ ì§œ í˜•ì‹ì— ë§ì¶° íŒŒì‹± ì‹œë„ (ì˜ˆ: YYYY-MM-DD HH:MM:SS)
                    dt = datetime.strptime(date_val, "%Y-%m-%d %H:%M:%S")
            else:
                dt = date_val
                
            # datetime ê°ì²´ê°€ ë§ë‹¤ë©´ 9ì‹œê°„ ë”í•˜ê¸°
            if isinstance(dt, datetime):
                kst_time = dt + timedelta(hours=9)
                return kst_time.strftime('%Y-%m-%d %H:%M:%S')
                
        except Exception as e:
            logger.warning(f"KST ë³€í™˜ ì‹¤íŒ¨ ({date_val}): {e}")
            
        return str(date_val)

    def _normalize_value(self, val: Union[float, int, str], is_ct: bool = False) -> str:
        """ê°’ ì •ê·œí™” ë° í¬ë§·íŒ… (CTëŠ” ì´ˆ ë‹¨ìœ„ ë³€í™˜)"""
        if val is None or val == '-':
            return '-'
            
        try:
            v = float(val)
            # CT ê°’ì´ê³  1,000,000 ì´ìƒì´ë©´ ë‚˜ë…¸ì´ˆë¡œ ê°„ì£¼í•˜ì—¬ ì´ˆ ë‹¨ìœ„ ë³€í™˜
            if is_ct and v > 1000000:
                v = v / 1e9
                
            return f"{v:.2f}"
            
        except (ValueError, TypeError):
            return str(val)

    def _format_production_data(self, data: Dict) -> str:
        """ìƒì‚° ë°ì´í„° í¬ë§·íŒ… - ê²°ê³¼ íŒì • ë° ìƒì„¸ ì •ë³´ ì¶”ê°€"""
        if not data:
            return "ë°ì´í„° ì—†ìŒ"
        
        lines = ["### [MongoDB] ìƒì‚° ì´ë ¥ ì •ë³´"]
        
        # í†µê³„ ì •ë³´
        if "stats" in data:
            stats = data["stats"]
            lines.append(f"\nğŸ“Š **í†µê³„ ìš”ì•½**:")
            lines.append(f"  - ì¡°íšŒ ê¸°ê°„ ë‚´ ìƒì‚°: {stats.get('count', 0)}ê±´")
            ct = stats.get("ct", {})
            if ct.get("avg"):
                lines.append(f"  - í‰ê·  CT: {ct['avg']:.2f}ì´ˆ")
            loadsum = stats.get("loadSum", {})
            if loadsum.get("avg"):
                lines.append(f"  - í‰ê·  LoadSum: {loadsum['avg']:.2f}")


        
        # ìµœê·¼ ìƒì‚° ì´ë ¥
        if "recent_products" in data and data["recent_products"]:
            lines.append(f"\nğŸ“‹ **ìµœê·¼ ìƒì‚° ì´ë ¥** ({len(data['recent_products'])}ê±´):")
            for p in data["recent_products"][:30]:
                product_no = p.get('productNo', 'N/A')
                result = p.get('productResult', 'N/A')
                count = p.get('count', 0)
                
                # ìƒì„¸ ì •ë³´ êµ¬ì„±
                details = []
                
                # CT
                ct = p.get('ct', 0)
                ct_res = p.get('ctResult', 'N/A')
                ct_icon = 'âœ…' if ct_res == 'Y' else 'âŒ'
                ct_val = self._normalize_value(ct, is_ct=True)
                details.append(f"CT: {ct_val} ({ct_icon})")
                
                # Load
                load = p.get('loadSum', 0)
                load_res = p.get('loadSumResult', 'N/A')
                load_icon = 'âœ…' if load_res == 'Y' else 'âŒ'
                load_val = self._normalize_value(load, is_ct=False)
                details.append(f"Load: {load_val} ({load_icon})")
                
                # AI
                ai = p.get('ai', 0)
                ai_res = p.get('aiResult', 'N/A')
                ai_icon = 'âœ…' if ai_res == 'Y' else 'âŒ'
                details.append(f"AI: {ai} ({ai_icon})")
                
                # CNC Params
                cnc = []
                if p.get('mainProgramNo'): cnc.append(f"M:{p.get('mainProgramNo')}")
                if p.get('tCode'): cnc.append(f"T:{p.get('tCode')}")
                if p.get('fov'): cnc.append(f"F:{p.get('fov')}")
                
                param_str = f", Params=[{', '.join(cnc)}]" if cnc else ""
                
                lines.append(f"  - [{result}] {product_no} | ìˆ˜ëŸ‰: {count} | {', '.join(details)}{param_str}")
        
        return "\n".join(lines)

    
    def _format_abnormal_data(self, data: Dict) -> str:
        """ì´ìƒê°ì§€ ë°ì´í„° í¬ë§·íŒ… - Summaryì™€ Details êµ¬ë¶„"""
        if not data:
            return "ë°ì´í„° ì—†ìŒ"
        
        lines = ["### [MongoDB] ì´ìƒê°ì§€ ë°œìƒ í˜„í™© ì •ë³´"]
        
        # 1. ìƒì‚°í’ˆë³„ ìµœì¢… íŒì • (abnormalSummary ì»¬ë ‰ì…˜)
        if "summary_records" in data and data["summary_records"]:
            lines.append("\nğŸ“Š **ìƒì‚°í’ˆë³„ ì´ìƒ íŒì • (ìµœì¢… ìƒíƒœ)**:")
            for s in data["summary_records"][:30]:
                product_no = s.get('productNo', 'N/A')
                
                status_parts = []
                if s.get('abnormalCt') == 'N':
                    val = self._normalize_value(s.get('abnormalCtValue'), is_ct=True)
                    status_parts.append(f"âš ï¸CT: {val}")
                else:
                    status_parts.append("âœ…CT")
                
                if s.get('abnormalLoad') == 'N':
                    val = self._normalize_value(s.get('abnormalLoadValue'))
                    status_parts.append(f"âš ï¸Load: {val}")
                else:
                    status_parts.append("âœ…Load")
                
                if s.get('abnormalAi') == 'N':
                    val = s.get('abnormalAiValue', 0)
                    status_parts.append(f"âš ï¸AI: {val}ê±´")
                else:
                    status_parts.append("âœ…AI")
                
                status_str = " | ".join(status_parts)
                lines.append(f"  - {product_no}: {status_str}")
        
        # 2. ì§‘ê³„ í†µê³„ (ê¸°ì¡´ summary)
        if "stats" in data and data["stats"]:
            stats = data["stats"]
            lines.append(f"\nğŸ“ˆ **í†µê³„ ìš”ì•½**:")
            lines.append(f"  - ì´ ì´ìƒê°ì§€ ê±´ìˆ˜: {stats.get('total', 0)}ê±´")
            by_code = stats.get("by_code", {})
            if by_code:
                lines.append(f"  - ìœ í˜•ë³„ ë°œìƒ: {', '.join([f'{k}={v}ê±´' for k, v in by_code.items()])}")
        
        if "recent_abnormals" in data and data["recent_abnormals"]:
            lines.append(f"\nğŸ“‹ **ìµœê·¼ ì´ìƒê°ì§€ ìƒì„¸ ì´ë ¥** ({len(data['recent_abnormals'])}ê±´):")
            for a in data["recent_abnormals"][:20]: # LLMì—ê²Œ ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
                begin_date = self._to_kst(a.get('abnormalBeginDate'))
                code = a.get('abnormalCode', 'N/A')
                
                is_ct = 'CT' in code.upper()
                val = self._normalize_value(a.get('abnormalValue'), is_ct=is_ct)
                
                tool = a.get('abnormalTool', '-')
                
                lines.append(f"  - [{begin_date}] {code}: ê°’={val}, ê³µêµ¬={tool}")
        
        return "\n".join(lines)

    
    def _format_machine_data(self, data: Dict) -> str:
        """ì„¤ë¹„ ë°ì´í„° í¬ë§·íŒ…"""
        if not data:
            return "ë°ì´í„° ì—†ìŒ"
        
        if "machine" in data:
            m = data["machine"]
            lines = [
                "### [MongoDB] ì„¤ë¹„ ë§ˆìŠ¤í„° ë° ì„¤ì • ì •ë³´",
                f"* ì„¤ë¹„ ì½”ë“œ: {m.get('machineCode', 'N/A')}",
                f"* ì„¤ë¹„ëª…: {m.get('machineName', 'N/A')}",
                f"* ê³µì •: {m.get('opCode', 'N/A')}",
                f"* IP/Port: {m.get('machineIp', 'N/A')}:{m.get('machinePort', 'N/A')}"
            ]
            
            # ì„ê³„ì¹˜ ì •ë³´ (ìƒì„¸)
            if "threshold" in data and data["threshold"]:
                t = data["threshold"]
                lines.append("\n### [MongoDB] ì„ê³„ì¹˜ ì„¤ì • ìƒì„¸")
                lines.append(f"* CT ì„ê³„ì¹˜: {t.get('minThresholdCt', 0):,.0f} ~ {t.get('maxThresholdCt', 0):,.0f}")
                lines.append(f"* LoadSum ì„ê³„ì¹˜: {t.get('minThresholdLoad', 0):,.0f} ~ {t.get('maxThresholdLoad', 0):,.0f}")
                
                # ì˜¤ì°¨ìœ¨ ì„ê³„ì¹˜
                if t.get("thresholdLoss"):
                    lines.append(f"* ì˜¤ì°¨ìœ¨ ì„ê³„ì¹˜: {t.get('thresholdLoss')}")
                
                # AI ì˜ˆì¸¡ êµ¬ê°„
                if t.get("predictPeriod"):
                    lines.append(f"* AI ì˜ˆì¸¡ êµ¬ê°„: {t.get('predictPeriod')}")
                
                # ê³µêµ¬ë³„ ì„ê³„ì¹˜
                tool_thresholds = []
                for i in range(1, 5):
                    key = f"tool{i}Threshold"
                    if key in t and t[key]:
                        tool_thresholds.append(f"T{i}: {t[key]}")
                if tool_thresholds:
                    lines.append(f"* ê³µêµ¬ë³„ ì„ê³„ì¹˜: {', '.join(tool_thresholds)}")
                
                # ë¹„ê³  ë° ì„ íƒ ìƒíƒœ
                if t.get("remark"):
                    lines.append(f"* ë¹„ê³ : {t.get('remark')}")
                if t.get("selected"):
                    lines.append(f"* ì„ íƒ ìƒíƒœ: {t.get('selected')}")
            
            # ê³µêµ¬ ì •ë³´ (ìƒì„¸)
            if "tools" in data and data["tools"]:
                lines.append(f"\n[ë“±ë¡ëœ ê³µêµ¬ ({len(data['tools'])}ê°œ)]")
                for tool in data["tools"]:
                    tool_info = f"* {tool.get('toolCode', 'N/A')}: {tool.get('toolName', 'N/A')}"
                    tool_info += f" (ìµœëŒ€ {tool.get('maxCount', 0)}íšŒ"
                    if tool.get('warnRate'):
                        tool_info += f", ê²½ê³  {tool.get('warnRate')}%"
                    tool_info += ")"
                    if tool.get('subToolCode'):
                        tool_info += f" [ì„œë¸Œì½”ë“œ: {tool.get('subToolCode')}]"
                    lines.append(tool_info)
            
            # ê°€ë™ ì‹œê°„ ì •ë³´ (ìƒì„¸)
            if "runtime" in data and data["runtime"]:
                r = data["runtime"]
                hours = r.get('period_hours', 24)
                
                # ê°€ë™ë¥ ì— ë”°ë¼ ìƒíƒœ ì•„ì´ì½˜ í‘œì‹œ
                status_icon = "ğŸŸ¢" if r.get('operating_rate', 0) > 80 else "ğŸŸ¡" if r.get('operating_rate', 0) > 50 else "ğŸ”´"
                
                lines.append(f"\n### [InfluxDB] ìµœê·¼ {hours}ì‹œê°„ ê°€ë™ ì´ë ¥ í˜„í™©")
                lines.append(f"* ê°€ë™ ì‹œê°„: {r.get('runtime_hours', 0)}ì‹œê°„ ({r.get('runtime_minutes', 0)}ë¶„)")
                lines.append(f"* ê°€ë™ë¥ : {status_icon} {r.get('operating_rate', 0)}%")
            
            # ê³µêµ¬ ì‚¬ìš©ëŸ‰ (ê³„ì‚°ëœ ê°’)
            if "tool_counts" in data and data["tool_counts"]:
                lines.append(f"\n### [MongoDB] ì‹¤ì‹œê°„ ê³µêµ¬ ì‚¬ìš©ëŸ‰ í˜„í™©")
                for tc in data["tool_counts"]:
                    # ìƒíƒœì— ë”°ë¥¸ ì•„ì´ì½˜
                    status = tc.get('status', 'OK')
                    icon = "ğŸŸ¢" if status == "OK" else "ğŸŸ¡" if status == "WARN" else "ğŸ”´"
                    
                    lines.append(f"* {tc.get('toolCode')}: {tc.get('useCount', 0)}/{tc.get('maxCount', 0)}íšŒ ({tc.get('usageRate', 0)}%) {icon}")
            
            return "\n".join(lines)
        
        if "machines" in data:
            lines = ["### [MongoDB] ì„¤ë¹„ ëª©ë¡ ë° ì„¤ì • ì •ë³´", f"ì´ {data.get('total_count', 0)}ëŒ€ ì„¤ë¹„:"]
            for m in data["machines"]:
                lines.append(f"\n**ì„¤ë¹„ {m.get('machineCode', 'N/A')}** ({m.get('machineName', 'N/A')})")
                
                # ì„ê³„ì¹˜ ì •ë³´ ìƒì„¸ í‘œì‹œ
                if "threshold" in m and m["threshold"]:
                    t = m["threshold"]
                    lines.append("  [ì„ê³„ì¹˜ ì„¤ì •]")
                    lines.append(f"  * CT ì„ê³„ì¹˜: {t.get('minThresholdCt', 0):,.0f} ~ {t.get('maxThresholdCt', 0):,.0f}")
                    lines.append(f"  * LoadSum ì„ê³„ì¹˜: {t.get('minThresholdLoad', 0):,.0f} ~ {t.get('maxThresholdLoad', 0):,.0f}")
                    
                    if t.get("thresholdLoss"):
                        lines.append(f"  * ì˜¤ì°¨ìœ¨ ì„ê³„ì¹˜: {t.get('thresholdLoss')}")
                    if t.get("predictPeriod"):
                        lines.append(f"  * AI ì˜ˆì¸¡ êµ¬ê°„: {t.get('predictPeriod')}")
                    
                    # ê³µêµ¬ë³„ ì„ê³„ì¹˜
                    tool_thresholds = []
                    for i in range(1, 5):
                        key = f"tool{i}Threshold"
                        if key in t and t[key]:
                            tool_thresholds.append(f"T{i}: {t[key]}")
                    if tool_thresholds:
                        lines.append(f"  * ê³µêµ¬ë³„ ì„ê³„ì¹˜: {', '.join(tool_thresholds)}")
                    
                    if t.get("remark"):
                        lines.append(f"  * ë¹„ê³ : {t.get('remark')}")
                else:
                    lines.append("  [ì„ê³„ì¹˜] ì„¤ì • ì—†ìŒ")
                
                # í•´ë‹¹ ì„¤ë¹„ì˜ ê³µêµ¬ ì •ë³´ë„ í‘œì‹œ
                tools = m.get("tools", [])
                if tools:
                    tool_names = [t.get('toolCode', 'N/A') for t in tools[:5]]
                    tool_line = f"  [ê³µêµ¬] {', '.join(tool_names)}"
                    if len(tools) > 5:
                        tool_line += f" ì™¸ {len(tools)-5}ê°œ"
                    lines.append(tool_line)
                
                # ê³µêµ¬ ì‚¬ìš©ëŸ‰ ìš”ì•½ í‘œì‹œ
                if "tool_counts" in m and m["tool_counts"]:
                    status_counts = {"OK": 0, "WARN": 0, "ERROR": 0}
                    for tc in m["tool_counts"]:
                        status = tc.get("status", "OK")
                        if status in status_counts:
                            status_counts[status] += 1
                    
                    icons = []
                    if status_counts["ERROR"] > 0: icons.append(f"ğŸ”´{status_counts['ERROR']}")
                    if status_counts["WARN"] > 0: icons.append(f"ğŸŸ¡{status_counts['WARN']}")
                    if status_counts["OK"] > 0: icons.append(f"ğŸŸ¢{status_counts['OK']}")
                    
                    if icons:
                        lines.append(f"  [ìƒíƒœ] {' '.join(icons)}")
                
            return "\n".join(lines)
        
        return "ë°ì´í„° ì—†ìŒ"
    
    def _format_tool_data(self, data: Dict) -> str:
        """ê³µêµ¬ ë°ì´í„° í¬ë§·íŒ… - InfluxDB í†µê³„ + MongoDB ì´ë ¥/í˜„í™©"""
        if not data:
            return "ë°ì´í„° ì—†ìŒ"
        
        lines = []
        
        # 1. [MongoDB] ê³µêµ¬ ìƒì„¸ ì´ë ¥ ë° í†µê³„ (NEW)
        if "history" in data and data["history"]:
            lines.append("### [MongoDB] ê³µêµ¬ ìƒì„¸ ì´ë ¥ ë° í†µê³„")
            
            # í†µê³„ ì •ë³´
            if "history_stats" in data and data["history_stats"]:
                s = data["history_stats"]
                lines.append(f"\nğŸ“Š **ì‚¬ìš© í†µê³„ ìš”ì•½** (ìµœê·¼ 7ì¼):")
                lines.append(f"  - ì´ ì‚¬ìš© íšŸìˆ˜: {s.get('totalUseCount', 0)}íšŒ")
                if s.get('avgCt'):
                    lines.append(f"  - í‰ê·  CT: {s.get('avgCt'):.2f}ì´ˆ")
                if s.get('avgLoadSum'):
                    lines.append(f"  - í‰ê·  LoadSum: {s.get('avgLoadSum'):.2f}")
            
            # ìƒì„¸ ì´ë ¥
            lines.append(f"\nğŸ“‹ **ìƒì„¸ ì´ë ¥** ({len(data['history'])}ê±´):")
            for h in data["history"][:30]:
                tool_code = h.get('toolCode', 'N/A')
                use_count = h.get('toolUseCount', 0)
                ct = h.get('toolCt', 0)
                load_sum = h.get('toolLoadSum', 0)
                date_str = self._to_kst(h.get('toolUseStartDate'))
                
                lines.append(f"  - {date_str} | {tool_code}: ì‚¬ìš© {use_count}íšŒ, CT: {ct:.2f}, Load: {load_sum:.2f}")
            lines.append("")

        # 2. [InfluxDB] ì‚¬ìš© í†µê³„
        if "usage_stats" in data and data["usage_stats"]:
            stats = data["usage_stats"]
            period = stats[0].get('period_hours', 24) if stats else 24
            lines.append(f"### [InfluxDB] ìµœê·¼ {period}ì‹œê°„ ê³µêµ¬ ì‚¬ìš© ì´ë ¥ í†µê³„")
            lines.append("(ê³¼ê±° ì¡°ì—… ì´ë ¥ì—ì„œ ì§‘ê³„ëœ ê³µêµ¬ë³„ ëˆ„ì  ì‚¬ìš© íšŸìˆ˜)")
            for s in stats:
                 lines.append(f"* ê³µêµ¬ {s.get('tool_code')}: {s.get('total_use_count')}íšŒ ì‚¬ìš©")
            lines.append("")
            
        # 3. [MongoDB] ê³µêµ¬ ì‹¤ì‹œê°„ ì‚¬ìš©ëŸ‰
        if "tool_counts" in data and data["tool_counts"]:
            tc_list = data["tool_counts"]
            lines.append(f"### [MongoDB] ê³µêµ¬ ì‹¤ì‹œê°„ ì‚¬ìš©ëŸ‰ ë° ìˆ˜ëª… ì •ë³´")
            lines.append("(í˜„ì¬ ì„¤ë¹„ì— ì¥ì°©ëœ ê³µêµ¬ì˜ ì‹¤ì‹œê°„ ì‚¬ìš©ëŸ‰(useCount) ë° ë§ˆìŠ¤í„° ìˆ˜ëª…(maxCount) ì •ë³´)")
            
            # ì„¤ë¹„ë³„ ê·¸ë£¹í™”
            by_machine = {}
            for item in tc_list:
                m_code = item.get("machineCode", "Unknown")
                if m_code not in by_machine: by_machine[m_code] = []
                by_machine[m_code].append(item)
            
            for m_code, sorted_tools in by_machine.items():
                if m_code != "Unknown":
                     lines.append(f"\n> ì„¤ë¹„: {m_code}")
                
                for tc in sorted_tools:
                    status = tc.get('status', 'OK')

                    icon = "ğŸŸ¢" if status == "OK" else "ğŸŸ¡" if status == "WARN" else "ğŸ”´"
                    tool_info = f"* {tc.get('toolCode')}: {tc.get('useCount', 0)}/{tc.get('maxCount', 0)}íšŒ ({tc.get('usageRate', 0)}%) {icon}"
                    
                    details = []
                    if tc.get('subToolCode'):
                        details.append(f"ì˜ˆë¹„: {tc.get('subToolCode')}")
                    if tc.get('toolOrder'):
                        details.append(f"ìˆœì„œ: {tc.get('toolOrder')}")
                    if tc.get('warnRate'):
                        details.append(f"ì•ŒëŒ: {tc.get('warnRate')}%")
                        
                    if details:
                        tool_info += f"\n    - ìƒì„¸: {', '.join(details)}"
                    
                    lines.append(tool_info)
            lines.append("")
        
        if "tool" in data:
            t = data["tool"]
            lines.append(f"* ê³µêµ¬ ì½”ë“œ: {t.get('toolCode', 'N/A')}")
            lines.append(f"* ê³µêµ¬ëª…: {t.get('toolName', 'N/A')}")
            lines.append(f"* ìµœëŒ€ ìˆ˜ëª…: {t.get('maxCount', 0)}íšŒ")
            return "\n".join(lines)
        
        if "tools" in data:
            lines.append(f"ê³µêµ¬ ëª©ë¡ ({len(data['tools'])}ê°œ):")
            for t in data["tools"]:
                lines.append(f"  - {t.get('toolCode', 'N/A')}: {t.get('toolName', 'N/A')} (ìµœëŒ€ {t.get('maxCount', 0)}íšŒ)")
            return "\n".join(lines)
            
        if lines:
            return "\n".join(lines)
        
        return "ë°ì´í„° ì—†ìŒ"
    
    def _format_sensor_data(self, data: Dict) -> str:
        """ì„¼ì„œ ë°ì´í„° í¬ë§·íŒ…"""
        if not data:
            return "ë°ì´í„° ì—†ìŒ"
        
        lines = ["### [InfluxDB] ì„¼ì„œ ë°ì´í„° ì‹œê³„ì—´ ë¶„ì„"]
        
        # [NEW] ë‹¤ì¤‘ í•„ë“œ í†µê³„ ì²˜ë¦¬ (stats_{Field})
        stats_keys = sorted([k for k in data.keys() if k.startswith("stats_")])
        for k in stats_keys:
            field_name = k.replace("stats_", "")
            item = data[k]
            p_type = item.get("type")
            res_data = item.get("data")
            
            if not res_data: continue
            
            # Daily Stats
            if p_type == "daily_stats":
                daily_list = res_data.get("daily", [])
                total = res_data.get("total", {})
                if daily_list:
                    lines.append(f"\n[ì¼ë³„ {field_name} í†µê³„ (ìµœê·¼ {total.get('period_days', len(daily_list))}ì¼)]")
                    for d in daily_list:
                        day = d.get("day", "N/A")
                        vals = []
                        if d.get("mean") is not None: vals.append(f"í‰ê·  {d['mean']}")
                        if d.get("max") is not None: vals.append(f"ìµœëŒ€ {d['max']}")
                        if d.get("min") is not None: vals.append(f"ìµœì†Œ {d['min']}")
                        lines.append(f"* {day}: {', '.join(vals)}")
                if total:
                    lines.append(f"* ì „ì²´ ìš”ì•½: í‰ê·  {total.get('mean')}, ìµœëŒ€ {total.get('max')}, ìµœì†Œ {total.get('min')}")

            # Weekly Stats
            elif p_type == "weekly_stats":
                weekly_list = res_data.get("weekly", [])
                total = res_data.get("total", {})
                if weekly_list:
                    lines.append(f"\n[ì£¼ë³„ {field_name} í†µê³„ (ìµœê·¼ {total.get('period_weeks', len(weekly_list))}ì£¼)]")
                    for w in weekly_list:
                        week = w.get("week", "N/A")
                        vals = []
                        if w.get("mean") is not None: vals.append(f"í‰ê·  {w['mean']}")
                        if w.get("max") is not None: vals.append(f"ìµœëŒ€ {w['max']}")
                        if w.get("min") is not None: vals.append(f"ìµœì†Œ {w['min']}")
                        lines.append(f"* {week}: {', '.join(vals)}")
                if total:
                    lines.append(f"* ì „ì²´ ìš”ì•½: í‰ê·  {total.get('mean')}, ìµœëŒ€ {total.get('max')}, ìµœì†Œ {total.get('min')}")

            # Monthly Stats
            elif p_type == "monthly_stats":
                monthly_list = res_data.get("monthly", [])
                total = res_data.get("total", {})
                if monthly_list:
                    lines.append(f"\n[ì›”ë³„ {field_name} í†µê³„ (ìµœê·¼ {total.get('period_months', len(monthly_list))}ê°œì›”)]")
                    for m in monthly_list:
                        month = m.get("month", "N/A")
                        vals = []
                        if m.get("mean") is not None: vals.append(f"í‰ê·  {m['mean']}")
                        if m.get("max") is not None: vals.append(f"ìµœëŒ€ {m['max']}")
                        if m.get("min") is not None: vals.append(f"ìµœì†Œ {m['min']}")
                        lines.append(f"* {month}: {', '.join(vals)}")
                if total:
                    lines.append(f"* ì „ì²´ ìš”ì•½: í‰ê·  {total.get('mean')}, ìµœëŒ€ {total.get('max')}, ìµœì†Œ {total.get('min')}")

            # Raw Stats
            elif p_type == "raw_stats":
                stats = res_data
                if stats.get("mean") is not None:
                    lines.append(f"\n[ì „ì²´ {field_name} í†µê³„ ({stats.get('hours')}ì‹œê°„)]")
                    lines.append(f"* í‰ê· : {stats['mean']:.1f}, ìµœëŒ€: {stats['max']:.1f}, ìµœì†Œ: {stats['min']:.1f}")

        if "current_status" in data:
            status = data["current_status"]
            lines.append(f"\n### [InfluxDB] ì‹¤ì‹œê°„ ì„¼ì„œ ìƒíƒœ ìš”ì•½")
            lines.append(f"* ê°€ë™ ìƒíƒœ: {status.get('run_status', 'N/A')}")
            lines.append(f"* í˜„ì¬ ë¶€í•˜: {status.get('current_load', 'N/A')}")
            lines.append(f"* ì´ì†¡ ì†ë„: {status.get('current_feed', 'N/A')}")
            lines.append(f"* FOV: {status.get('fov', 'N/A')}%, SOV: {status.get('sov', 'N/A')}%")
        
        if "running_stats" in data:
            stats = data["running_stats"]
            if stats.get("mean") is not None:
                 lines.append(f"\n[ê°€ë™ ì¤‘ Load í†µê³„ ({stats.get('hours')}ì‹œê°„)]")
                 lines.append(f"* í‰ê· : {stats['mean']:.1f}")
                 lines.append(f"* ìµœëŒ€: {stats['max']:.1f}")
                 lines.append(f"* ìµœì†Œ: {stats['min']:.1f}")
        
        if "raw_stats" in data:
            stats = data["raw_stats"]
            if stats.get("mean") is not None:
                 lines.append(f"\n[ì „ì²´ Load í†µê³„ ({stats.get('hours')}ì‹œê°„)]")
                 lines.append(f"* í‰ê· : {stats['mean']:.1f}")
                 lines.append(f"* ìµœëŒ€: {stats['max']:.1f}")
                 lines.append(f"* ìµœì†Œ: {stats['min']:.1f}")
        
        if "trend" in data:
            trend = data["trend"]
            if trend:
                lines.append(f"\n[Load íŠ¸ë Œë“œ ({len(trend)}ê°œ í¬ì¸íŠ¸)]")
                for t in trend[:30]:  # ìµœëŒ€ 30ê°œë§Œ í‘œì‹œ
                    time_str = t.get('time', 'N/A')
                    if hasattr(time_str, 'strftime'):
                        time_str = time_str.strftime("%H:%M")
                    lines.append(f"* {time_str}: {t.get('value', 'N/A'):.1f}")
                if len(trend) > 10:
                    lines.append(f"  ... ì™¸ {len(trend) - 10}ê°œ")
        
        if "runtime" in data:
            rt = data["runtime"]
            lines.append(f"\n[ê°€ë™ ì‹œê°„/ë¥  ({rt.get('period_hours', 24)}ì‹œê°„)]")
            lines.append(f"* ê°€ë™ ì‹œê°„: {rt.get('runtime_hours', 0)}ì‹œê°„ ({rt.get('runtime_minutes', 0)}ë¶„)")
            lines.append(f"* ê°€ë™ë¥ : {rt.get('operating_rate', 0)}%")
        
        total = None

        if "daily_runtime" in data:
            daily_data = data["daily_runtime"]
            daily_list = daily_data.get("daily", [])
            total = daily_data.get("total", {})
            
            if daily_list:
                lines.append(f"\n[ì¼ë³„ ê°€ë™ë¥  (ìµœê·¼ {total.get('period_days', len(daily_list))}ì¼)]")
                for d in daily_list:
                    date_str = d.get("date", "N/A")
                    hours = d.get("runtime_hours", 0)
                    rate = d.get("operating_rate", 0)
                    # ê°€ë™ë¥ ì— ë”°ë¥¸ ìƒíƒœ ì•„ì´ì½˜
                    icon = "ğŸŸ¢" if rate > 80 else "ğŸŸ¡" if rate > 50 else "ğŸ”´"
                    lines.append(f"* {date_str}: {hours}ì‹œê°„ ({rate}%) {icon}")
        
        if "today_runtime" in data:
            tr = data["today_runtime"]
            lines.append(f"\n[ì˜¤ëŠ˜ ê°€ë™ ì‹œê°„/ë¥  (00:00~í˜„ì¬)]")
            lines.append(f"* ê°€ë™ ì‹œê°„: {tr.get('runtime_hours', 0)}ì‹œê°„ ({tr.get('runtime_minutes', 0)}ë¶„)")
            lines.append(f"* í˜„ì¬ ê°€ë™ë¥ : {tr.get('operating_rate', 0)}% (ê¸°ì¤€: {tr.get('total_elapsed_seconds', 0)/3600:.1f}ì‹œê°„ ê²½ê³¼)")
            
            if total:
                lines.append(f"\n[ì´ ê°€ë™ë¥ ]")
                lines.append(f"* ê¸°ê°„: {total.get('period_days', 0)}ì¼")
                lines.append(f"* ì´ ê°€ë™ ì‹œê°„: {total.get('runtime_hours', 0)}ì‹œê°„")
                lines.append(f"* í‰ê·  ê°€ë™ë¥ : {total.get('operating_rate', 0)}%")
        
        if "weekly_runtime" in data:
            weekly_data = data["weekly_runtime"]
            weekly_list = weekly_data.get("weekly", [])
            total = weekly_data.get("total", {})
            
            if weekly_list:
                lines.append(f"\n[ì£¼ë³„ ê°€ë™ë¥  (ìµœê·¼ {total.get('period_weeks', len(weekly_list))}ì£¼)]")
                for w in weekly_list:
                    week_start = w.get("week_start", "N/A")
                    hours = w.get("runtime_hours", 0)
                    rate = w.get("operating_rate", 0)
                    icon = "ğŸŸ¢" if rate > 80 else "ğŸŸ¡" if rate > 50 else "ğŸ”´"
                    lines.append(f"* {week_start} ì£¼: {hours}ì‹œê°„ ({rate}%) {icon}")
            
            if total:
                lines.append(f"\n[ì´ ê°€ë™ë¥ ]")
                lines.append(f"* ê¸°ê°„: {total.get('period_weeks', 0)}ì£¼")
                lines.append(f"* ì´ ê°€ë™ ì‹œê°„: {total.get('runtime_hours', 0)}ì‹œê°„")
                lines.append(f"* í‰ê·  ê°€ë™ë¥ : {total.get('operating_rate', 0)}%")
        
        if "monthly_runtime" in data:
            monthly_data = data["monthly_runtime"]
            monthly_list = monthly_data.get("monthly", [])
            total = monthly_data.get("total", {})
            
            if monthly_list:
                lines.append(f"\n[ì›”ë³„ ê°€ë™ë¥  (ìµœê·¼ {total.get('period_months', len(monthly_list))}ê°œì›”)]")
                for m in monthly_list:
                    month = m.get("month", "N/A")
                    hours = m.get("runtime_hours", 0)
                    rate = m.get("operating_rate", 0)
                    icon = "ğŸŸ¢" if rate > 80 else "ğŸŸ¡" if rate > 50 else "ğŸ”´"
                    lines.append(f"* {month}: {hours}ì‹œê°„ ({rate}%) {icon}")
            
            if total:
                lines.append(f"\n[ì´ ê°€ë™ë¥ ]")
                lines.append(f"* ê¸°ê°„: {total.get('period_months', 0)}ê°œì›”")
                lines.append(f"* ì´ ê°€ë™ ì‹œê°„: {total.get('runtime_hours', 0)}ì‹œê°„")
                lines.append(f"* í‰ê·  ê°€ë™ë¥ : {total.get('operating_rate', 0)}%")
        
        # ê¸°ê°„ë³„ Stats í¬ë§·íŒ…
        if "daily_stats" in data:
            daily_data = data["daily_stats"]
            daily_list = daily_data.get("daily", [])
            total = daily_data.get("total", {})
            
            if daily_list:
                lines.append(f"\n[ì¼ë³„ {total.get('field', 'Load')} í†µê³„ (ìµœê·¼ {total.get('period_days', len(daily_list))}ì¼)]")
                for d in daily_list:
                    day = d.get("day", "N/A")
                    mean = d.get("mean", 0) or 0
                    max_v = d.get("max", 0) or 0
                    min_v = d.get("min", 0) or 0
                    lines.append(f"* {day}: í‰ê·  {mean}, ìµœëŒ€ {max_v}, ìµœì†Œ {min_v}")
            
            if total:
                lines.append(f"\n[ì „ì²´ í†µê³„]")
                lines.append(f"* í‰ê· : {total.get('mean', 0)}, ìµœëŒ€: {total.get('max', 0)}, ìµœì†Œ: {total.get('min', 0)}")
        
        if "weekly_stats" in data:
            weekly_data = data["weekly_stats"]
            weekly_list = weekly_data.get("weekly", [])
            total = weekly_data.get("total", {})
            
            if weekly_list:
                lines.append(f"\n[ì£¼ë³„ {total.get('field', 'Load')} í†µê³„ (ìµœê·¼ {total.get('period_weeks', len(weekly_list))}ì£¼)]")
                for w in weekly_list:
                    week = w.get("week", "N/A")
                    mean = w.get("mean", 0) or 0
                    max_v = w.get("max", 0) or 0
                    min_v = w.get("min", 0) or 0
                    lines.append(f"* {week}: í‰ê·  {mean}, ìµœëŒ€ {max_v}, ìµœì†Œ {min_v}")
            
            if total:
                lines.append(f"\n[ì „ì²´ í†µê³„]")
                lines.append(f"* í‰ê· : {total.get('mean', 0)}, ìµœëŒ€: {total.get('max', 0)}, ìµœì†Œ: {total.get('min', 0)}")
        
        if "monthly_stats" in data:
            monthly_data = data["monthly_stats"]
            monthly_list = monthly_data.get("monthly", [])
            total = monthly_data.get("total", {})
            
            if monthly_list:
                lines.append(f"\n[ì›”ë³„ {total.get('field', 'Load')} í†µê³„ (ìµœê·¼ {total.get('period_months', len(monthly_list))}ê°œì›”)]")
                for m in monthly_list:
                    month = m.get("month", "N/A")
                    mean = m.get("mean", 0) or 0
                    max_v = m.get("max", 0) or 0
                    min_v = m.get("min", 0) or 0
                    lines.append(f"* {month}: í‰ê·  {mean}, ìµœëŒ€ {max_v}, ìµœì†Œ {min_v}")
            
            if total:
                lines.append(f"\n[ì „ì²´ í†µê³„]")
                lines.append(f"* í‰ê· : {total.get('mean', 0)}, ìµœëŒ€: {total.get('max', 0)}, ìµœì†Œ: {total.get('min', 0)}")

        # CT í†µê³„ í¬ë§·íŒ…
        if "daily_ct_stats" in data:
            ct_data = data["daily_ct_stats"]
            daily_list = ct_data.get("daily", [])
            total = ct_data.get("total", {})
            
            if daily_list:
                lines.append(f"\n[ì¼ë³„ CT(Cycle Time) í†µê³„ (ìµœê·¼ {total.get('period_days', len(daily_list))}ì¼)]")
                for d in daily_list:
                    lines.append(f"* {d.get('date')}: í‰ê·  {d.get('mean_ct', 0)}ì´ˆ")
            
            if total:
                lines.append(f"\n[ì „ì²´ CT í†µê³„]")
                lines.append(f"* í‰ê·  CT: {total.get('mean', 0)}ì´ˆ")
                lines.append(f"* ìµœëŒ€ CT: {total.get('max', 0)}ì´ˆ, ìµœì†Œ CT: {total.get('min', 0)}ì´ˆ")
                lines.append(f"* ì´ ìƒì‚° íšŸìˆ˜(ì§‘ê³„): {total.get('count', 0)}íšŒ")

        if "weekly_ct_stats" in data:
            ct_data = data["weekly_ct_stats"]
            weekly_list = ct_data.get("weekly", [])
            total = ct_data.get("total", {})
            
            if weekly_list:
                lines.append(f"\n[ì£¼ë³„ CT(Cycle Time) í†µê³„ (ìµœê·¼ {total.get('period_weeks', len(weekly_list))}ì£¼)]")
                for w in weekly_list:
                    lines.append(f"* {w.get('week_start')} ì£¼: í‰ê·  {w.get('mean_ct', 0)}ì´ˆ")
            
            if total:
                lines.append(f"\n[ì „ì²´ CT í†µê³„]")
                lines.append(f"* í‰ê·  CT: {total.get('mean', 0)}ì´ˆ")
                lines.append(f"* ìµœëŒ€ CT: {total.get('max', 0)}ì´ˆ, ìµœì†Œ CT: {total.get('min', 0)}ì´ˆ")
                lines.append(f"* ì´ ìƒì‚° íšŸìˆ˜(ì§‘ê³„): {total.get('count', 0)}íšŒ")

        if "monthly_ct_stats" in data:
            ct_data = data["monthly_ct_stats"]
            monthly_list = ct_data.get("monthly", [])
            total = ct_data.get("total", {})
            
            if monthly_list:
                lines.append(f"\n[ì›”ë³„ CT(Cycle Time) í†µê³„ (ìµœê·¼ {total.get('period_months', len(monthly_list))}ê°œì›”)]")
                for m in monthly_list:
                    lines.append(f"* {m.get('month')}: í‰ê·  {m.get('mean_ct', 0)}ì´ˆ")
            
            if total:
                lines.append(f"\n[ì „ì²´ CT í†µê³„]")
                lines.append(f"* í‰ê·  CT: {total.get('mean', 0)}ì´ˆ")
                lines.append(f"* ìµœëŒ€ CT: {total.get('max', 0)}ì´ˆ, ìµœì†Œ CT: {total.get('min', 0)}ì´ˆ")
                lines.append(f"* ì´ ìƒì‚° íšŸìˆ˜(ì§‘ê³„): {total.get('count', 0)}íšŒ")
        
        return "\n".join(lines)
    
    def _format_user_data(self, data: Dict) -> str:
        """ì‚¬ìš©ì ë°ì´í„° í¬ë§·íŒ… (ë¹„ë°€ë²ˆí˜¸ ì œì™¸ ë³´ì¥)"""
        if not data:
            return "ë°ì´í„° ì—†ìŒ"
        
        lines = ["### [MongoDB] ì‚¬ìš©ì ì •ë³´"]
        
        # ë‹¨ì¼ ì‚¬ìš©ì ì¡°íšŒ
        if "user" in data:
            u = data["user"]
            lines.append(f"* ì‚¬ìš©ì ID: {u.get('userId', 'N/A')}")
            lines.append(f"* ì´ë¦„: {u.get('userName', 'N/A')}")
            
            # ê¶Œí•œ ë ˆë²¨ í‘œì‹œ
            role = u.get('userRole', 0)
            role_text = {0: "ì¼ë°˜ ì‚¬ìš©ì", 1: "ê´€ë¦¬ì", 2: "ìŠˆí¼ ê´€ë¦¬ì"}.get(role, f"ê¶Œí•œ ë ˆë²¨ {role}")
            lines.append(f"* ê¶Œí•œ: {role_text}")
            
            # ê³„ì • ìƒíƒœ
            if u.get('resetFlag') == 'Y':
                lines.append("* ìƒíƒœ: ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì • í•„ìš”")
            else:
                lines.append("* ìƒíƒœ: ì •ìƒ")
            
            # ìƒì„±ì¼
            if u.get('createAt'):
                lines.append(f"* ê³„ì • ìƒì„±ì¼: {u.get('createAt')}")
            
            return "\n".join(lines)
        
        # ì „ì²´ ì‚¬ìš©ì ëª©ë¡
        if "users" in data:
            lines.append(f"ì´ {data.get('total_count', 0)}ëª…ì˜ ë“±ë¡ëœ ì‚¬ìš©ì:")
            
            for u in data["users"]:
                role = u.get('userRole', 0)
                role_text = {0: "ì¼ë°˜", 1: "ê´€ë¦¬ì", 2: "ìŠˆí¼ê´€ë¦¬ì"}.get(role, f"ê¶Œí•œ{role}")
                
                user_info = f"  - {u.get('userId', 'N/A')}: {u.get('userName', 'N/A')} ({role_text})"
                
                # ì¬ì„¤ì • í•„ìš”í•œ ê³„ì • í‘œì‹œ
                if u.get('resetFlag') == 'Y':
                    user_info += " [ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì • í•„ìš”]"
                
                lines.append(user_info)
            
            return "\n".join(lines)
        
        return "ë°ì´í„° ì—†ìŒ"

    async def _handle_general_conversation(self, original_q: str, aware_q: str, hist_key: str, start_time: float) -> QueryResponse:
        """ë°ì´í„° ì†ŒìŠ¤ê°€ ì—†ì„ ì‹œ ì¼ë°˜ ëŒ€í™”ë¡œ ì‘ë‹µ"""
        answer = await self.answer_generator.generate_general_conversation_response(aware_q)
        self.conversation_manager.add_to_history(hist_key, original_q, answer, [], 0.8)
        return QueryResponse(answer=answer, sources=[], confidence=0.8, processing_time=time.time() - start_time)

    def _create_simple_response(self, text: str, req: QueryRequest, key: str, start: float) -> QueryResponse:
        """ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„± ë° íˆìŠ¤í† ë¦¬ ê¸°ë¡"""
        self.conversation_manager.add_to_history(key, req.question, text, [], 1.0)
        return QueryResponse(answer=text, sources=[], confidence=1.0, processing_time=time.time() - start)

    async def health_check(self) -> Dict[str, Any]:
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸"""
        return {
            "status": "healthy" if self._initialized else "initializing",
            "mongodb": await self.mongodb.health_check(),
            "influxdb": await self.influxdb.health_check(),
            "retriever": await self.retriever.vector_store.health_check()
        }

    async def cleanup(self):
        """ìì› ì •ë¦¬"""
        await asyncio.gather(
            self.mongodb.close(),
            self.retriever.vector_store.cleanup(),
            self.gemini.cleanup(),
            return_exceptions=True
        )
        self._initialized = False


# ì „ì—­ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
hybrid_rag_engine = HybridRAGEngine()
