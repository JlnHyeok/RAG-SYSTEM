"""
í•˜ì´ë¸Œë¦¬ë“œ RAG ì—”ì§„ (í†µí•© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°)
ë¬¸ì„œ(Qdrant), êµ¬ì¡°í™” ë°ì´í„°(MongoDB), ì‹œê³„ì—´ ë°ì´í„°(InfluxDB)ë¥¼ 
í†µí•©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì‹œìŠ¤í…œì˜ í•µì‹¬ ì—”ì§„ì…ë‹ˆë‹¤.

ì œì¡° ë„ë©”ì¸ ë°ì´í„° ì†ŒìŠ¤:
- DOCUMENT: Qdrant ë¬¸ì„œ ê²€ìƒ‰ (ë§¤ë‰´ì–¼, ê°€ì´ë“œ)
- PRODUCTION: MongoDB ìƒì‚° ì´ë ¥
- ABNORMAL: MongoDB ì´ìƒê°ì§€ ì´ë ¥
- MACHINE: MongoDB ì„¤ë¹„ ì •ë³´
- TOOL: MongoDB ê³µêµ¬ ì •ë³´
- RAW_SENSOR: InfluxDB ì‹¤ì‹œê°„ ì„¼ì„œ ë°ì´í„°
"""

import logging
import asyncio
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import time
import re

# DB ë° ê²€ìƒ‰ ëª¨ë“ˆ
from app.core.db.mongodb_connector import get_mongodb_connector, MongoDBConnector, FilterCommon
from app.core.db.influxdb_connector import get_influxdb_connector, InfluxDBConnector
from app.core.retrieval.document_retriever import document_retriever, DocumentRetriever

# LLM ë° ì²˜ë¦¬ ëª¨ë“ˆ
from app.core.llm.gemini_service import gemini_service
from app.core.llm.question_classifier import QuestionClassifier, QuestionType, SensorQueryType
from app.core.llm.answer_generator import AnswerGenerator
from app.core.processing.text_processor import text_processor
from app.core.session.conversation_manager import conversation_manager

# ëª¨ë¸ ë° ì„¤ì •
from app.models.schemas import QueryRequest, QueryResponse, SearchResult
from app.core.config import settings

logger = logging.getLogger(__name__)


class DataSourceType(str, Enum):
    """ë°ì´í„° ì†ŒìŠ¤ ìœ í˜•"""
    DOCUMENT = "document"           # Qdrant ë¬¸ì„œ ê²€ìƒ‰
    PRODUCTION = "production"       # MongoDB ìƒì‚° ì´ë ¥
    ABNORMAL = "abnormal"           # MongoDB ì´ìƒê°ì§€
    MACHINE = "machine"             # MongoDB ì„¤ë¹„ ì •ë³´
    TOOL = "tool"                   # MongoDB ê³µêµ¬ ì •ë³´
    RAW_SENSOR = "raw_sensor"       # InfluxDB ì‹¤ì‹œê°„ ì„¼ì„œ
    HYBRID = "hybrid"               # ë‹¤ì¤‘ ì†ŒìŠ¤ í†µí•©


# QuestionType -> DataSourceType ë§¤í•‘
QUESTION_TO_SOURCE_MAP = {
    QuestionType.DOCUMENT_QUERY: DataSourceType.DOCUMENT,
    QuestionType.PRODUCTION_QUERY: DataSourceType.PRODUCTION,
    QuestionType.ABNORMAL_QUERY: DataSourceType.ABNORMAL,
    QuestionType.MACHINE_QUERY: DataSourceType.MACHINE,
    QuestionType.TOOL_QUERY: DataSourceType.TOOL,
    QuestionType.RAW_SENSOR_QUERY: DataSourceType.RAW_SENSOR,
    QuestionType.HYBRID_QUERY: DataSourceType.HYBRID,
}


@dataclass
class QueryIntent:
    """ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ê²°ê³¼"""
    primary_source: DataSourceType
    secondary_sources: List[DataSourceType] = field(default_factory=list)
    entities: Dict[str, Any] = field(default_factory=dict)
    time_range: Optional[str] = None  # "1h", "24h", "7d" ë“±
    confidence: float = 0.0
    sensor_query_type: Optional[SensorQueryType] = None  # ì„¼ì„œ ì¿¼ë¦¬ ì„¸ë¶€ ìœ í˜•
    target_field: Optional[str] = None  # ì¡°íšŒ ëŒ€ìƒ í•„ë“œ (CT, Load ë“±)


@dataclass
class HybridContext:
    """í•˜ì´ë¸Œë¦¬ë“œ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°"""
    document_results: List[SearchResult] = field(default_factory=list)
    document_context: Optional[str] = None
    production_data: Optional[List[Dict]] = None
    abnormal_data: Optional[List[Dict]] = None
    machine_data: Optional[Dict] = None
    tool_data: Optional[List[Dict]] = None
    sensor_data: Optional[Dict] = None
    metadata: Dict[str, Any] = field(default_factory=dict)


class HybridRAGEngine:
    """
    í•˜ì´ë¸Œë¦¬ë“œ RAG ì—”ì§„
    
    ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ë¶„ì„í•˜ê³ , ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤(ë¬¸ì„œ, DB, ì„¼ì„œ)ì—ì„œ 
    ìµœì ì˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ì§€ëŠ¥ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # ë°ì´í„° ì†ŒìŠ¤
        self.retriever: DocumentRetriever = document_retriever
        self.mongodb: MongoDBConnector = get_mongodb_connector()
        self.influxdb: InfluxDBConnector = get_influxdb_connector()
        
        # ì²˜ë¦¬ ì—”ì§„
        self.gemini = gemini_service
        self.text_processor = text_processor
        self.conversation_manager = conversation_manager
        
        # ì´ˆê¸°í™” ìƒíƒœ ë° ì˜ì¡´ì„± ì£¼ì… ê°ì²´
        self._initialized = False
        self.question_classifier: Optional[QuestionClassifier] = None
        self.answer_generator: Optional[AnswerGenerator] = None
    
    async def initialize(self):
        """í•˜ì´ë¸Œë¦¬ë“œ RAG ì—”ì§„ ë° ê´€ë ¨ ëª¨ë“ˆ ì´ˆê¸°í™”"""
        if self._initialized:
            return
        
        print("\n" + "="*50)
        print(f"ğŸš€ {settings.APP_NAME} ì´ˆê¸°í™” ì‹œì‘")
        print("="*50)
        
        try:
            # ëª¨ë“  í•˜ìœ„ ì„œë¹„ìŠ¤ ë³‘ë ¬ ì´ˆê¸°í™”
            results = await asyncio.gather(
                self.retriever.initialize(),
                self.gemini.initialize(test_connection=False),
                self.mongodb.initialize(),
                self.influxdb.initialize(),
                return_exceptions=True
            )
            
            # ê° ì„œë¹„ìŠ¤ë³„ ìƒíƒœ í™•ì¸ ë° ë¦¬í¬íŠ¸
            component_names = ["Qdrant", "Gemini", "MongoDB", "InfluxDB"]
            all_success = True
            
            print("\nğŸ“‹ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”:")
            for name, result in zip(component_names, results):
                if isinstance(result, Exception):
                    print(f"  âŒ {name.ljust(20)}: ì‹¤íŒ¨ ({str(result)})")
                    all_success = False
                else:
                    print(f"  âœ… {name.ljust(20)}: ì„±ê³µ")
            
            if not all_success:
                logger.warning("ì¼ë¶€ ì»´í¬ë„ŒíŠ¸ê°€ ì •ìƒì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # ì˜ì¡´ì„± ì£¼ì… ë° ê°ì²´ ìƒì„±
            self.question_classifier = QuestionClassifier(self.gemini, self.retriever.vector_store)
            self.answer_generator = AnswerGenerator(self.gemini)
            self.conversation_manager.set_gemini_service(self.gemini)
            
            self._initialized = True
            print("\n" + "="*50)
            print(f"âœ¨ {settings.APP_NAME} ì´ˆê¸°í™” ì™„ë£Œ!")
            print("="*50 + "\n")
            
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ RAG ì—”ì§„ ì´ˆê¸°í™” ì¹˜ëª…ì  ì‹¤íŒ¨: {e}")
            print(f"\nâŒ ì´ˆê¸°í™” ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise
    
    async def query(self, request: QueryRequest, on_status: Optional[callable] = None) -> QueryResponse:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ í†µí•© í•˜ì´ë¸Œë¦¬ë“œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        if not self._initialized:
            await self.initialize()
            
        # 1. ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬
        conversation_key = self.conversation_manager.get_conversation_key(
            request.user_id, 
            request.conversation_id
        )
        self.conversation_manager.ensure_history_exists(conversation_key)
        
        try:
            # 2. ëŒ€í™” ë§¥ë½ ë¶„ì„ ë° ì§ˆë¬¸ ë³´ì •
            if on_status: await on_status("ëŒ€í™” ë§¥ë½ íŒŒì•… ì¤‘...")
            context_aware_question, _ = await self.conversation_manager.analyze_question_context(
                request.question, conversation_key
            )
            
            # 3. ë©”íƒ€ ì§ˆë¬¸ ë° ì˜ë„ ë¶„ì„
            if on_status: await on_status("ì§ˆë¬¸ ì˜ë„ ë¶„ì„ ì¤‘...")
            
            # 3.1 ì¼ë°˜ ë©”íƒ€ ì§ˆë¬¸ (ì¸ì‚¬, ë¬¸ì„œëª©ë¡ ë“±) ì²˜ë¦¬
            meta_response = await self.question_classifier.handle_meta_questions(
                context_aware_question, request.user_id
            )
            if meta_response:
                return self._create_simple_response(meta_response, request, conversation_key, start_time)
            
            # 3.2 ë°ì´í„° ì†ŒìŠ¤ ì˜ë„ ë¶„ì„
            intent = await self._analyze_intent(context_aware_question)
            logger.info(f"ë°ì´í„° ì†ŒìŠ¤ ì˜ë„: {intent.primary_source.value}, ì—”í‹°í‹°: {intent.entities}")
            
            # 4. ë°ì´í„° ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            context = await self._gather_context(intent, request, context_aware_question, on_status)
            
            # 5. ê²€ìƒ‰ ê²°ê³¼ ë° ë°ì´í„° ìœ ë¬´ì— ë”°ë¥¸ ë‹µë³€ ìƒì„± ì „ëµ
            if on_status: await on_status("ë‹µë³€ ìƒì„± ì¤‘...")
            
            # ë°ì´í„°ê°€ ì•„ë¬´ê²ƒë„ ì—†ëŠ” ê²½ìš° ì¼ë°˜ ëŒ€í™” ì‹œë„
            has_data = {
                "document": bool(context.document_results),
                "production": bool(context.production_data),
                "abnormal": bool(context.abnormal_data),
                "machine": bool(context.machine_data),
                "tool": bool(context.tool_data),
                "sensor": bool(context.sensor_data)
            }
            logger.info(f"ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸: {has_data}")
            
            if not any(has_data.values()):
                logger.info("ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ì—†ìŒ - ì¼ë°˜ ëŒ€í™”ë¡œ ì „í™˜")
                return await self._handle_general_conversation(request.question, context_aware_question, conversation_key, start_time)

            # ë‹µë³€ ìƒì„± (LLM)
            response = await self._generate_answer(request, context, actual_question=context_aware_question, history_key=conversation_key, start_time=start_time, intent=intent)
            
            proc_time = response.processing_time if response.processing_time is not None else 0.0
            logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ ì™„ë£Œ: {proc_time:.2f}ì´ˆ")
            return response
                
        except Exception as e:
            logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return QueryResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                sources=[],
                confidence=0.0,
                processing_time=time.time() - start_time
            )

    async def _analyze_intent(self, question: str) -> QueryIntent:
        """ì§ˆë¬¸ ë¶„ì„ ë° ì˜ë„ íŒŒì•… (í†µí•© ë¶„ì„ ê²°ê³¼ ì‚¬ìš©)"""
        logger.info(f"ğŸ” ì˜ë„ ë¶„ì„ ì‹œì‘ - ì…ë ¥ ì§ˆë¬¸: '{question}'")
        
        # 1. í†µí•© ì§ˆë¬¸ ë¶„ì„ (ì˜ë„, ì—”í‹°í‹°, ì‹œê°„ ë²”ìœ„)
        # ì´ì œ classify_questionì´ QuestionAnalysisResultë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        analysis_result = await self.question_classifier.classify_question(question)
        
        question_type = analysis_result.primary_type
        primary_source = QUESTION_TO_SOURCE_MAP.get(question_type, DataSourceType.DOCUMENT)
        
        # 2. ë³´ì¡° ì†ŒìŠ¤ ê²°ì •
        secondary_sources = []
        for sec_type in analysis_result.secondary_types:
            source = QUESTION_TO_SOURCE_MAP.get(sec_type)
            if source and source not in secondary_sources and source != primary_source:
                secondary_sources.append(source)
                
        # 3. ì—”í‹°í‹° ë° ì‹œê°„ ë²”ìœ„ ì‚¬ìš©
        entities = analysis_result.entities
        time_range = analysis_result.time_range
        
        # [ì•ˆì „ì¥ì¹˜] machine_idê°€ ì¶”ì¶œë˜ì—ˆë‹¤ë©´ MACHINE ì†ŒìŠ¤ ìë™ ì¶”ê°€
        if entities.get("machine_id") and primary_source != DataSourceType.MACHINE:
             if DataSourceType.MACHINE not in secondary_sources:
                secondary_sources.append(DataSourceType.MACHINE)
                logger.info("ì—”í‹°í‹° ê¸°ë°˜ MACHINE ì†ŒìŠ¤ ìë™ ì¶”ê°€")

        # 4. ë³µí•© ì§ˆë¬¸(HYBRID)ì¸ ê²½ìš° ë¬¸ì„œëŠ” ê¸°ë³¸ í¬í•¨
        if primary_source == DataSourceType.HYBRID:
            if DataSourceType.DOCUMENT not in secondary_sources:
                secondary_sources.append(DataSourceType.DOCUMENT)

        logger.info(f"ğŸ” ë¶„ì„ ê²°ê³¼ - Primary: {primary_source}, Secondary: {secondary_sources}, Time: {time_range}, Entities: {entities}")
        
        return QueryIntent(
            primary_source=primary_source,
            secondary_sources=secondary_sources,
            entities=entities,
            time_range=time_range,
            confidence=0.9,
            sensor_query_type=analysis_result.sensor_query_type,
            target_field=analysis_result.target_field
        )
    


    async def _gather_context(
        self, 
        intent: QueryIntent, 
        request: QueryRequest, 
        question: str,
        on_status: Optional[callable]
    ) -> HybridContext:
        """ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ë³‘ë ¬ ìˆ˜ì§‘"""
        context = HybridContext()
        tasks = []
        source_keys = []
        
        # [NEW] machine_idê°€ ìˆë‹¤ë©´ ì„¤ë¹„ ì •ë³´ë¥¼ ë¨¼ì € ì¡°íšŒí•˜ì—¬ ì •í™•í•œ í•„í„° ì •ë³´ êµ¬ì„±
        # (ê¸°ë³¸ê°’ F01 ëŒ€ì‹  ì‹¤ì œ workshopCode ë“±ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•¨ - ë°±ì—”ë“œ ë¡œì§ ì¼ì¹˜í™”)
        if intent.entities.get("machine_id"):
            try:
                mid = intent.entities["machine_id"]
                machine_info = await self.mongodb.get_machine_by_code(mid)
                if machine_info:
                    intent.entities["workshop_id"] = machine_info.get("workshopCode")
                    intent.entities["line_id"] = machine_info.get("lineCode")
                    intent.entities["op_code"] = machine_info.get("opCode")
                    logger.info(f"ì„¤ë¹„ ì •ë³´ ê¸°ë°˜ í•„í„° ì—…ë°ì´íŠ¸: {intent.entities}")
            except Exception as e:
                logger.warning(f"ì„¤ë¹„ ì •ë³´ ì„ í–‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # ê¸°ë³¸ í•„í„° ìƒì„±
        filter_common = self._create_filter_common(intent.entities)
        
        sources = list(set([intent.primary_source] + intent.secondary_sources))
        
        for source in sources:
            if source == DataSourceType.DOCUMENT:
                if on_status: await on_status("ë¬¸ì„œ ì§€ì‹ ê²€ìƒ‰ ì¤‘...")
                tasks.append(self.retriever.search(question, request.user_id, request.max_results, request.score_threshold))
                source_keys.append("document")
                
            elif source == DataSourceType.PRODUCTION:
                if on_status: await on_status("ìƒì‚° ì´ë ¥ ì¡°íšŒ ì¤‘...")
                hours = self._time_range_to_hours(intent.time_range)
                tasks.append(self._get_production_data(filter_common, hours))
                source_keys.append("production")
                
            elif source == DataSourceType.ABNORMAL:
                if on_status: await on_status("ì´ìƒê°ì§€ ì´ë ¥ ì¡°íšŒ ì¤‘...")
                hours = self._time_range_to_hours(intent.time_range)
                tasks.append(self._get_abnormal_data(filter_common, hours, intent.entities.get("abnormal_code")))
                source_keys.append("abnormal")
                
            elif source == DataSourceType.MACHINE:
                if on_status: await on_status("ì„¤ë¹„ ì •ë³´ ì¡°íšŒ ì¤‘...")
                tasks.append(self._get_machine_data(intent.entities))
                source_keys.append("machine")
                
            elif source == DataSourceType.TOOL:
                if on_status: await on_status("ê³µêµ¬ ì •ë³´ ì¡°íšŒ ì¤‘...")
                tasks.append(self._get_tool_data(intent.entities))
                source_keys.append("tool")
                
            elif source == DataSourceType.RAW_SENSOR:
                if on_status: await on_status("ì„¼ì„œ ë°ì´í„° ì¡°íšŒ ì¤‘...")
                hours = self._time_range_to_hours(intent.time_range)
                tasks.append(self._get_sensor_data(filter_common, hours, intent.sensor_query_type, intent.target_field))
                source_keys.append("sensor")

        if not tasks: 
            return context

        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for key, res in zip(source_keys, results):
            if isinstance(res, Exception): 
                logger.warning(f"{key} ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {res}")
                continue
            if key == "document": 
                context.document_results = res
                context.document_context = self.text_processor.build_context(res)
            elif key == "production": 
                context.production_data = res
            elif key == "abnormal": 
                context.abnormal_data = res
            elif key == "machine": 
                context.machine_data = res
            elif key == "tool": 
                context.tool_data = res
            elif key == "sensor": 
                context.sensor_data = res
            
        return context
    
    def _create_filter_common(self, entities: Dict) -> Optional[FilterCommon]:
        """ì—”í‹°í‹°ì—ì„œ FilterCommon ìƒì„±"""
        workshop_id = entities.get("workshop_id") or settings.DEFAULT_WORKSHOP_ID
        line_id = entities.get("line_id") or settings.DEFAULT_LINE_ID
        op_code = entities.get("op_code") or settings.DEFAULT_OP_CODE
        machine_id = entities.get("machine_id")
        
        # í•„ìˆ˜ í•„ë“œê°€ ì—†ìœ¼ë©´ None ë°˜í™˜
        if not workshop_id or not line_id or not op_code:
            return None
        
        return FilterCommon(
            workshop_id=workshop_id,
            line_id=line_id,
            op_code=op_code,
            machine_id=machine_id
        )
    
    def _time_range_to_hours(self, time_range: Optional[str]) -> int:
        """ì‹œê°„ ë²”ìœ„ ë¬¸ìì—´ì„ ì‹œê°„ ë‹¨ìœ„ë¡œ ë³€í™˜ (ì˜ˆ: 1ë…„, 3ê°œì›”, 2ì£¼, 1d, 24h)"""
        if not time_range:
            return 24
            
        try:
            # ì •ê·œì‹ìœ¼ë¡œ ìˆ«ìì™€ ë‹¨ìœ„ ì¶”ì¶œ
            # ì˜ˆ: "1ë…„", "1y", "3ê°œì›”", "30d"
            match = re.search(r'(\d+)\s*(ë…„|y|ê°œì›”|m|ë‹¬|ì£¼|w|ì¼|d|ì‹œê°„|h|ë¶„|min)', time_range.lower())
            
            if match:
                val = int(match.group(1))
                unit = match.group(2)
                
                if unit in ['ë…„', 'y', 'year', 'years']:
                    return val * 365 * 24
                if unit in ['ê°œì›”', 'm', 'mon', 'month', 'months', 'ë‹¬']:
                    return val * 30 * 24
                if unit in ['ì£¼', 'w', 'week', 'weeks']:
                    return val * 7 * 24
                if unit in ['ì¼', 'd', 'day', 'days']:
                    return val * 24
                if unit in ['ì‹œê°„', 'h', 'hour', 'hours']:
                    return val
                if unit in ['ë¶„', 'min', 'minute', 'minutes']:
                    return max(1, int(val / 60))
            
            # ê¸°ì¡´ ë¡œì§ (Fallback)
            if time_range.endswith("h"):
                return int(time_range[:-1])
            elif time_range.endswith("d"):
                return int(time_range[:-1]) * 24
                
        except Exception as e:
            logger.warning(f"ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ ({time_range}): {e}")
            
        return 24
    
    # ============ ë°ì´í„° ì¡°íšŒ ë©”ì„œë“œ ============
    
    async def _get_production_data(self, filter_common: Optional[FilterCommon], hours: int) -> List[Dict]:
        """ìƒì‚° ì´ë ¥ ì¡°íšŒ"""
        if not filter_common:
            return []
        
        try:
            products = await self.mongodb.get_recent_products(filter_common, hours, limit=50)
            stats = await self.mongodb.get_product_stats(filter_common, hours)
            
            return {
                "recent_products": products[:10],  # ìµœê·¼ 10ê±´
                "stats": stats,
                "total_count": len(products)
            }
        except Exception as e:
            logger.error(f"ìƒì‚° ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def _get_abnormal_data(
        self, 
        filter_common: Optional[FilterCommon], 
        hours: int,
        abnormal_code: Optional[str] = None
    ) -> List[Dict]:
        """ì´ìƒê°ì§€ ì´ë ¥ ì¡°íšŒ"""
        if not filter_common:
            return []
        
        try:
            if abnormal_code:
                abnormals = await self.mongodb.get_abnormals_by_code(filter_common, abnormal_code, hours)
            else:
                abnormals = await self.mongodb.get_recent_abnormals(filter_common, hours)
            
            summary = await self.mongodb.get_abnormal_summary(filter_common, hours)
            
            return {
                "recent_abnormals": abnormals[:10],
                "summary": summary,
                "total_count": len(abnormals)
            }
        except Exception as e:
            logger.error(f"ì´ìƒê°ì§€ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def _get_machine_data(self, entities: Dict) -> Dict:
        """ì„¤ë¹„ ì •ë³´ ì¡°íšŒ"""
        machine_id = entities.get("machine_id")
        
        try:
            if machine_id:
                logger.info(f"ì„¤ë¹„ ë‹¨ê±´ ì¡°íšŒ: {machine_id}")
                machine = await self.mongodb.get_machine_by_code(machine_id)
                if machine:
                    threshold = await self.mongodb.get_threshold_by_machine(machine_id)
                    tools = await self.mongodb.get_tools_by_machine(machine_id)
                    
                    # InfluxDB ê°€ë™ ì‹œê°„ ì¡°íšŒ ë° ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ
                    runtime = {}
                    tool_counts = []
                    
                    # machine ì •ë³´ì—ì„œ í•„í„° ê°’ ì§ì ‘ ì¶”ì¶œ (machineMaster í•„ë“œ ì‚¬ìš©) ë˜ëŠ” ê¸°ë³¸ê°’
                    workshop_code = machine.get("workshopCode") or settings.DEFAULT_WORKSHOP_ID
                    line_code = machine.get("lineCode") or settings.DEFAULT_LINE_ID
                    op_code = machine.get("opCode") or settings.DEFAULT_OP_CODE
                    
                    logger.info(f"Machine í•„í„° ì •ë³´: workshop={workshop_code}, line={line_code}, op={op_code}")
                    
                    # FilterCommon ì§ì ‘ ìƒì„± (machine ì •ë³´ ê¸°ë°˜)
                    if workshop_code and line_code and op_code:
                        filter_common = FilterCommon(
                            workshop_id=workshop_code,
                            line_id=line_code,
                            op_code=op_code,
                            machine_id=machine_id
                        )
                        logger.info(f"FilterCommon ìƒì„±: {filter_common}")
                        
                        # ê°€ë™ ì‹œê°„ ì¡°íšŒ (InfluxDB - ì‹¤íŒ¨í•´ë„ ì§„í–‰)
                        try:
                            if filter_common.did:
                                hour_range = self._time_range_to_hours(entities.get("time_range"))
                                runtime = await self.influxdb.get_machine_runtime(filter_common, hours=hour_range)
                        except Exception as e:
                            logger.error(f"ê°€ë™ ì‹œê°„ ì¡°íšŒ ì‹¤íŒ¨ (InfluxDB ë¬´ì‹œ): {e}")

                        # ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (MongoDB - í•„ìˆ˜)
                        try:
                            tool_counts = await self.mongodb.get_current_tool_counts(filter_common)
                            logger.info(f"ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ê²°ê³¼: {len(tool_counts)}ê°œ")
                        except Exception as e:
                            logger.error(f"ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    else:
                        logger.warning(f"FilterCommon ìƒì„± ì‹¤íŒ¨: í•„ìˆ˜ í•„ë“œ ëˆ„ë½")
                    
                    return {"machine": machine, "threshold": threshold, "tools": tools, "runtime": runtime, "tool_counts": tool_counts}
            else:
                logger.info("ì„¤ë¹„ ì „ì²´ ëª©ë¡ ì¡°íšŒ ì‹œì‘")
                machines = await self.mongodb.get_all_machines()
                logger.info(f"ì¡°íšŒëœ ì„¤ë¹„ ìˆ˜: {len(machines)}")
                if machines:
                    logger.info(f"ì„¤ë¹„ ì½”ë“œ ëª©ë¡: {[m.get('machineCode') for m in machines]}")
                
                # ê° ì„¤ë¹„ë³„ ê³µêµ¬ ì •ë³´ ë° ì‚¬ìš©ëŸ‰ ì¡°íšŒ
                machines_with_tools = []
                for machine in machines[:20]:  # ìµœëŒ€ 20ê°œ ì„¤ë¹„
                    machine_code = machine.get("machineCode")
                    if machine_code:
                        # 1. ê³µêµ¬ ë§ˆìŠ¤í„° ì¡°íšŒ
                        tools = await self.mongodb.get_tools_by_machine(machine_code)
                        machine["tools"] = tools
                        
                        # 2. ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (FilterCommon ìƒì„± í•„ìš”)
                        try:
                            workshop_code = machine.get("workshopCode") or settings.DEFAULT_WORKSHOP_ID
                            line_code = machine.get("lineCode") or settings.DEFAULT_LINE_ID
                            op_code = machine.get("opCode") or settings.DEFAULT_OP_CODE
                            
                            if workshop_code and line_code and op_code:
                                filter_common = FilterCommon(
                                    workshop_id=workshop_code,
                                    line_id=line_code,
                                    op_code=op_code,
                                    machine_id=machine_code
                                )
                                tool_counts = await self.mongodb.get_current_tool_counts(filter_common)
                                machine["tool_counts"] = tool_counts
                        except Exception as e:
                            logger.error(f"ì„¤ë¹„ {machine_code} ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                            
                    machines_with_tools.append(machine)
                
                return {"machines": machines_with_tools, "total_count": len(machines)}
        except Exception as e:
            logger.error(f"ì„¤ë¹„ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _get_tool_data(self, entities: Dict) -> Dict:
        """ê³µêµ¬ ì •ë³´ ì¡°íšŒ (InfluxDB ì‚¬ìš© í†µê³„ í¬í•¨)"""
        machine_id = entities.get("machine_id")
        tool_code = entities.get("tool_code")
        
        try:
            # InfluxDB ì‚¬ìš© í†µê³„ ì¡°íšŒ
            usage_stats = []
            filter_common = self._create_filter_common(entities)
            
            tool_counts = []
            
            if filter_common:
                logger.info(f" [ToolDebug] ê³µêµ¬ ë°ì´í„° ì¡°íšŒ ì‹œì‘ - Filter: {filter_common.to_mongo_filter()}")
                
                # InfluxDB ì¡°íšŒ
                try:
                    hour_range = self._time_range_to_hours(entities.get("time_range"))
                    usage_stats = await self.influxdb.get_tool_stats(filter_common, hours=hour_range)
                    logger.info(f" [ToolDebug] InfluxDB Tool Stats: {len(usage_stats)}ê±´")
                except Exception as e:
                    logger.error(f" [ToolDebug] InfluxDB Tool Stats ì¡°íšŒ ì‹¤íŒ¨: {e}")

                # MongoDB ê³µêµ¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ (ì¶”ê°€ëœ ë¡œì§)
                try:
                    tool_counts = await self.mongodb.get_current_tool_counts(filter_common)
                    logger.info(f" [ToolDebug] MongoDB Tool Counts ì¡°íšŒ ê²°ê³¼: {len(tool_counts)}ê±´")
                    if tool_counts:
                         logger.info(f" [ToolDebug] ìƒì„¸: {[t.get('toolCode') for t in tool_counts]}")
                except Exception as e:
                    logger.error(f" [ToolDebug] MongoDB Tool Counts ì¡°íšŒ ì‹¤íŒ¨: {e}")
            else:
                logger.warning(" [ToolDebug] FilterCommon ìƒì„± ì‹¤íŒ¨ - í•„ìˆ˜ ì—”í‹°í‹° ëˆ„ë½")
            
            if machine_id:
                if tool_code:
                    tool = await self.mongodb.get_tool_by_code(machine_id, tool_code)
                    return {"tool": tool, "usage_stats": usage_stats, "tool_counts": tool_counts} if tool else {"usage_stats": usage_stats, "tool_counts": tool_counts}
                else:
                    tools = await self.mongodb.get_tools_by_machine(machine_id)
                    return {"tools": tools, "usage_stats": usage_stats, "tool_counts": tool_counts}
            return {"usage_stats": usage_stats, "tool_counts": tool_counts}
        except Exception as e:
            logger.error(f"ê³µêµ¬ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _get_sensor_data(
        self, 
        filter_common: Optional[FilterCommon], 
        hours: int = 1,
        sensor_query_type: Optional[SensorQueryType] = None,
        target_field: Optional[str] = None
    ) -> Dict:
        """ì„¼ì„œ ë°ì´í„° ì¡°íšŒ - target_field ê¸°ë°˜ ë™ì  ë¼ìš°íŒ…"""
        if not filter_common:
            return {}
        
        try:
            result = {}
            
            # ë™ì  í•„ë“œ íŒŒì‹± (ì½¤ë§ˆ êµ¬ë¶„ ì§€ì›)
            raw_target = target_field or "Load"
            target_fields = [t.strip() for t in raw_target.split(",")]
            
            measurement_map = {
                "CT": "cnc_product",
                # í•„ìš”ì‹œ ì¶”ê°€ ë§¤í•‘
            }
            
            # sensor_query_typeì— ë”°ë¥¸ ë¶„ê¸° ë¡œì§
            if sensor_query_type == SensorQueryType.CURRENT_STATUS:
                # í˜„ì¬ ìƒíƒœë§Œ ì¡°íšŒ
                result["current_status"] = await self.influxdb.get_current_status(filter_common)
                
            elif sensor_query_type == SensorQueryType.RUNNING_STATS:
                # ê°€ë™ ì¤‘ í†µê³„ (ì²« ë²ˆì§¸ í•„ë“œ ê¸°ì¤€)
                field = target_fields[0]
                result["running_stats"] = await self.influxdb.get_running_stats(filter_common, hours=hours, field=field)
                
            elif sensor_query_type == SensorQueryType.RAW_STATS or sensor_query_type == SensorQueryType.CT_STATS: 
                # ì „ì²´ í†µê³„ (í‰ê· /ìµœëŒ€/ìµœì†Œ) - ë‹¤ì¤‘ í•„ë“œ ì§€ì›
                days = max(1, hours // 24)
                
                for t_field in target_fields:
                    measurement = measurement_map.get(t_field)
                    stats_res = {}
                    key_type = ""
                    
                    if days > 84:  # 12ì£¼ ì´ˆê³¼
                        months = max(1, days // 30)
                        stats_res = await self.influxdb.get_monthly_stats(filter_common, months=months, field=t_field, measurement=measurement)
                        key_type = "monthly_stats"
                    elif days > 30:
                        weeks = max(1, days // 7)
                        stats_res = await self.influxdb.get_weekly_stats(filter_common, weeks=weeks, field=t_field, measurement=measurement)
                        key_type = "weekly_stats"
                    elif days > 1:
                        stats_res = await self.influxdb.get_daily_stats(filter_common, days=days, field=t_field, measurement=measurement)
                        key_type = "daily_stats"
                    else:
                        stats_res = await self.influxdb.get_raw_stats(filter_common, hours=hours, field=t_field, measurement=measurement)
                        key_type = "raw_stats"
                    
                    # ê²°ê³¼ ì €ì¥ (stats_{Field}) - í¬ë§·í„°ì—ì„œ ì‹ë³„ ìš©ì´í•˜ê²Œ
                    result[f"stats_{t_field}"] = {"type": key_type, "data": stats_res}
                
            elif sensor_query_type == SensorQueryType.TREND:
                # íŠ¸ë Œë“œ ì¡°íšŒ
                result["trend"] = await self.influxdb.get_raw_trend(filter_common, hours=hours, interval="1h", field=field, measurement=measurement)
                
            elif sensor_query_type == SensorQueryType.RUNTIME:
                # ê°€ë™ ì‹œê°„/ë¥  - ê¸°ê°„ë³„ ìë™ ì§‘ê³„ ë‹¨ìœ„ ì„ íƒ
                # â‰¤30ì¼: ì¼ë³„, 31-84ì¼(12ì£¼): ì£¼ë³„, >84ì¼: ì›”ë³„
                days = max(1, hours // 24)
                if days > 84:  # 12ì£¼ ì´ˆê³¼
                    months = max(1, days // 30)
                    result["monthly_runtime"] = await self.influxdb.get_monthly_runtime(filter_common, months=months)
                elif days > 30:
                    weeks = max(1, days // 7)
                    result["weekly_runtime"] = await self.influxdb.get_weekly_runtime(filter_common, weeks=weeks)
                else:
                    result["daily_runtime"] = await self.influxdb.get_daily_runtime(filter_common, days=days)
                
            else:
                # ê¸°ë³¸: í˜„ì¬ ìƒíƒœ + ê°€ë™ ì¤‘ í†µê³„
                result["current_status"] = await self.influxdb.get_current_status(filter_common)
                result["running_stats"] = await self.influxdb.get_running_stats(filter_common, hours=hours, field=field)
            
            return result
        except Exception as e:
            logger.error(f"ì„¼ì„œ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    async def _generate_answer(
        self, 
        request: QueryRequest, 
        context: HybridContext, 
        actual_question: str,
        history_key: str,
        start_time: float,
        intent: Optional[QueryIntent] = None
    ) -> QueryResponse:
        """LLMì„ í†µí•œ í•˜ì´ë¸Œë¦¬ë“œ ë‹µë³€ ìƒì„±"""
        
        # ì§ˆë¬¸ ìœ í˜• ê²°ì • (intentì˜ primary_sourceë¥¼ QuestionType í˜•ì‹ìœ¼ë¡œ ë³€í™˜)
        question_type = "DOCUMENT_QUERY"
        if intent:
            source_to_question_type = {
                DataSourceType.DOCUMENT: "DOCUMENT_QUERY",
                DataSourceType.PRODUCTION: "PRODUCTION_QUERY",
                DataSourceType.ABNORMAL: "ABNORMAL_QUERY",
                DataSourceType.MACHINE: "MACHINE_QUERY",
                DataSourceType.TOOL: "TOOL_QUERY",
                DataSourceType.RAW_SENSOR: "RAW_SENSOR_QUERY",
                DataSourceType.HYBRID: "HYBRID_QUERY",
            }
            question_type = source_to_question_type.get(intent.primary_source, "DOCUMENT_QUERY")
        
        logger.info(f"ë‹µë³€ ìƒì„± - ì§ˆë¬¸ ìœ í˜•: {question_type}")
        
        # 1. í•˜ì´ë¸Œë¦¬ë“œ ì •ë³´ í¬ë§·íŒ…
        info_blocks = []
        if context.document_context:
            info_blocks.append(f"[ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©]\n{context.document_context}")
        if context.production_data:
            info_blocks.append(f"[ìƒì‚° ì´ë ¥ ë°ì´í„°]\n{self._format_production_data(context.production_data)}")
        if context.abnormal_data:
            info_blocks.append(f"[ì´ìƒê°ì§€ í˜„í™©]\n{self._format_abnormal_data(context.abnormal_data)}")
        if context.machine_data:
            info_blocks.append(f"[ì„¤ë¹„ ì •ë³´]\n{self._format_machine_data(context.machine_data)}")
        if context.tool_data:
            info_blocks.append(f"[ê³µêµ¬ ì •ë³´]\n{self._format_tool_data(context.tool_data)}")
        if context.sensor_data:
            info_blocks.append(f"[ì‹¤ì‹œê°„ ì„¼ì„œ ë°ì´í„°]\n{self._format_sensor_data(context.sensor_data)}")
            
        full_context = "\n\n".join(info_blocks)
        
        # 2. ë‹µë³€ ìƒì„± í˜¸ì¶œ (ì§ˆë¬¸ ìœ í˜• í¬í•¨)
        answer = await self.answer_generator.generate_intelligent_answer(
            request.question,
            full_context,
            actual_question,
            self.conversation_manager.get_recent_history(history_key),
            self.conversation_manager.format_history_for_prompt,
            question_type=question_type
        )
        
        # 3. ê²°ê³¼ êµ¬ì„±
        confidence = self.retriever.calculate_confidence(context.document_results) if context.document_results else 0.8
        
        self.conversation_manager.add_to_history(
            history_key, request.question, answer, context.document_results, confidence
        )
        
        # ì‚¬ìš©ëœ ë°ì´í„° ì†ŒìŠ¤
        sources_used = []
        if context.document_context: sources_used.append("document")
        if context.production_data: sources_used.append("production")
        if context.abnormal_data: sources_used.append("abnormal")
        if context.machine_data: sources_used.append("machine")
        if context.tool_data: sources_used.append("tool")
        if context.sensor_data: sources_used.append("sensor")
        
        return QueryResponse(
            answer=answer,
            sources=context.document_results,
            confidence=confidence,
            processing_time=time.time() - start_time,
            metadata={"sources_used": sources_used}
        )
    
    # ============ ë°ì´í„° í¬ë§·íŒ… ë©”ì„œë“œ ============
    
    def _format_production_data(self, data: Dict) -> str:
        """ìƒì‚° ë°ì´í„° í¬ë§·íŒ…"""
        if not data:
            return "ë°ì´í„° ì—†ìŒ"
        
        lines = []
        if "stats" in data:
            stats = data["stats"]
            lines.append(f"* ì¡°íšŒ ê¸°ê°„ ë‚´ ìƒì‚° ìˆ˜: {stats.get('count', 0)}ê±´")
            ct = stats.get("ct", {})
            if ct.get("avg"):
                lines.append(f"* í‰ê·  CT: {ct['avg']:.2f}ì´ˆ (ìµœëŒ€: {ct.get('max', 0):.2f}, ìµœì†Œ: {ct.get('min', 0):.2f})")
            loadsum = stats.get("loadSum", {})
            if loadsum.get("avg"):
                lines.append(f"* í‰ê·  LoadSum: {loadsum['avg']:.2f}")
        
        if "recent_products" in data and data["recent_products"]:
            lines.append(f"\nìµœê·¼ ìƒì‚° ì´ë ¥ ({len(data['recent_products'])}ê±´):")
            for p in data["recent_products"][:5]:
                lines.append(f"  - {p.get('productNo', 'N/A')}: ê²°ê³¼={p.get('productResult', '-')}")
        
        return "\n".join(lines)
    
    def _format_abnormal_data(self, data: Dict) -> str:
        """ì´ìƒê°ì§€ ë°ì´í„° í¬ë§·íŒ…"""
        if not data:
            return "ë°ì´í„° ì—†ìŒ"
        
        lines = []
        if "summary" in data:
            summary = data["summary"]
            lines.append(f"* ì´ ì´ìƒê°ì§€ ê±´ìˆ˜: {summary.get('total', 0)}ê±´")
            by_code = summary.get("by_code", {})
            if by_code:
                lines.append(f"* ìœ í˜•ë³„ ë°œìƒ: {', '.join([f'{k}={v}ê±´' for k, v in by_code.items()])}")
        
        if "recent_abnormals" in data and data["recent_abnormals"]:
            lines.append(f"\nìµœê·¼ ì´ìƒê°ì§€ ì´ë ¥ ({len(data['recent_abnormals'])}ê±´):")
            for a in data["recent_abnormals"][:5]:
                lines.append(f"  - {a.get('abnormalCode', 'N/A')}: ê°’={a.get('abnormalValue', '-')}, ê³µêµ¬={a.get('abnormalTool', '-')}")
        
        return "\n".join(lines)
    
    def _format_machine_data(self, data: Dict) -> str:
        """ì„¤ë¹„ ë°ì´í„° í¬ë§·íŒ…"""
        if not data:
            return "ë°ì´í„° ì—†ìŒ"
        
        if "machine" in data:
            m = data["machine"]
            lines = [
                f"* ì„¤ë¹„ ì½”ë“œ: {m.get('machineCode', 'N/A')}",
                f"* ì„¤ë¹„ëª…: {m.get('machineName', 'N/A')}",
                f"* ê³µì •: {m.get('opCode', 'N/A')}",
                f"* IP/Port: {m.get('machineIp', 'N/A')}:{m.get('machinePort', 'N/A')}"
            ]
            
            # ì„ê³„ì¹˜ ì •ë³´ (ìƒì„¸)
            if "threshold" in data and data["threshold"]:
                t = data["threshold"]
                lines.append("\n[ì„ê³„ì¹˜ ì„¤ì •]")
                lines.append(f"* CT ì„ê³„ì¹˜: {t.get('minThresholdCt', 0):,.0f} ~ {t.get('maxThresholdCt', 0):,.0f}")
                lines.append(f"* LoadSum ì„ê³„ì¹˜: {t.get('minThresholdLoad', 0):,.0f} ~ {t.get('maxThresholdLoad', 0):,.0f}")
                
                # ê³µêµ¬ë³„ ì„ê³„ì¹˜
                tool_thresholds = []
                for i in range(1, 5):
                    key = f"tool{i}Threshold"
                    if key in t and t[key]:
                        tool_thresholds.append(f"T{i}: {t[key]}")
                if tool_thresholds:
                    lines.append(f"* ê³µêµ¬ë³„ ì„ê³„ì¹˜: {', '.join(tool_thresholds)}")
                
                # ë¹„ê³ 
                if t.get("remark"):
                    lines.append(f"* ë¹„ê³ : {t.get('remark')}")
                if t.get("selected"):
                    lines.append(f"* ì„ íƒ ìƒíƒœ: {t.get('selected')}")
            
            # ê³µêµ¬ ì •ë³´ (ìƒì„¸)
            if "tools" in data and data["tools"]:
                lines.append(f"\n[ë“±ë¡ëœ ê³µêµ¬ ({len(data['tools'])}ê°œ)]")
                for tool in data["tools"]:
                    tool_info = f"* {tool.get('toolCode', 'N/A')}: {tool.get('toolName', 'N/A')}"
                    tool_info += f" (ìµœëŒ€ {tool.get('maxCount', 0)}íšŒ"
                    if tool.get('warnRate'):
                        tool_info += f", ê²½ê³  {tool.get('warnRate')}%"
                    tool_info += ")"
                    if tool.get('subToolCode'):
                        tool_info += f" [ì„œë¸Œì½”ë“œ: {tool.get('subToolCode')}]"
                    lines.append(tool_info)
            
            # ê°€ë™ ì‹œê°„ ì •ë³´ (ìƒì„¸)
            if "runtime" in data and data["runtime"]:
                r = data["runtime"]
                hours = r.get('period_hours', 24)
                
                # ê°€ë™ë¥ ì— ë”°ë¼ ìƒíƒœ ì•„ì´ì½˜ í‘œì‹œ
                status_icon = "ğŸŸ¢" if r.get('operating_rate', 0) > 80 else "ğŸŸ¡" if r.get('operating_rate', 0) > 50 else "ğŸ”´"
                
                lines.append(f"\n[ìµœê·¼ {hours}ì‹œê°„ ê°€ë™ í˜„í™©]")
                lines.append(f"* ê°€ë™ ì‹œê°„: {r.get('runtime_hours', 0)}ì‹œê°„ ({r.get('runtime_minutes', 0)}ë¶„)")
                lines.append(f"* ê°€ë™ë¥ : {status_icon} {r.get('operating_rate', 0)}%")
            
            # ê³µêµ¬ ì‚¬ìš©ëŸ‰ (ê³„ì‚°ëœ ê°’)
            if "tool_counts" in data and data["tool_counts"]:
                lines.append(f"\n[ê³µêµ¬ ì‚¬ìš©ëŸ‰ í˜„í™©]")
                for tc in data["tool_counts"]:
                    # ìƒíƒœì— ë”°ë¥¸ ì•„ì´ì½˜
                    status = tc.get('status', 'OK')
                    icon = "ğŸŸ¢" if status == "OK" else "ğŸŸ¡" if status == "WARN" else "ğŸ”´"
                    
                    lines.append(f"* {tc.get('toolCode')}: {tc.get('useCount', 0)}/{tc.get('maxCount', 0)}íšŒ ({tc.get('usageRate', 0)}%) {icon}")
            
            return "\n".join(lines)
        
        if "machines" in data:
            lines = [f"ì´ {data.get('total_count', 0)}ëŒ€ ì„¤ë¹„:"]
            for m in data["machines"]:
                machine_info = f"  - {m.get('machineCode', 'N/A')}: {m.get('machineName', 'N/A')}"
                
                # í•´ë‹¹ ì„¤ë¹„ì˜ ê³µêµ¬ ì •ë³´ë„ í‘œì‹œ
                tools = m.get("tools", [])
                if tools:
                    tool_names = [t.get('toolCode', 'N/A') for t in tools[:5]]  # ìµœëŒ€ 5ê°œë§Œ
                    machine_info += f" | ê³µêµ¬: {', '.join(tool_names)}"
                    if len(tools) > 5:
                        machine_info += f" ì™¸ {len(tools)-5}ê°œ"
                else:
                    machine_info += " | ê³µêµ¬: ì—†ìŒ"
                
                # ê³µêµ¬ ì‚¬ìš©ëŸ‰ ìš”ì•½ í‘œì‹œ (ì¶”ê°€)
                if "tool_counts" in m and m["tool_counts"]:
                    status_counts = {"OK": 0, "WARN": 0, "ERROR": 0}
                    for tc in m["tool_counts"]:
                        status = tc.get("status", "OK")
                        if status in status_counts:
                            status_counts[status] += 1
                    
                    icons = []
                    if status_counts["ERROR"] > 0: icons.append(f"ğŸ”´{status_counts['ERROR']}")
                    if status_counts["WARN"] > 0: icons.append(f"ğŸŸ¡{status_counts['WARN']}")
                    if status_counts["OK"] > 0: icons.append(f"ğŸŸ¢{status_counts['OK']}")
                    
                    if icons:
                        machine_info += f" | ìƒíƒœ: {' '.join(icons)}"
                
                lines.append(machine_info)
            return "\n".join(lines)
        
        return "ë°ì´í„° ì—†ìŒ"
    
    def _format_tool_data(self, data: Dict) -> str:
        """ê³µêµ¬ ë°ì´í„° í¬ë§·íŒ…"""
        if not data:
            return "ë°ì´í„° ì—†ìŒ"
        
        lines = []
        
        # ì‚¬ìš© í†µê³„
        if "usage_stats" in data and data["usage_stats"]:
            stats = data["usage_stats"]
            period = stats[0].get('period_hours', 24) if stats else 24
            lines.append(f"[ìµœê·¼ {period}ì‹œê°„ ê³µêµ¬ ì‚¬ìš© í†µê³„]")
            for s in stats:
                 lines.append(f"* ê³µêµ¬ {s.get('tool_code')}: {s.get('total_use_count')}íšŒ ì‚¬ìš©")
            lines.append("")
        
        if "tool" in data:
            t = data["tool"]
            lines.append(f"* ê³µêµ¬ ì½”ë“œ: {t.get('toolCode', 'N/A')}")
            lines.append(f"* ê³µêµ¬ëª…: {t.get('toolName', 'N/A')}")
            lines.append(f"* ìµœëŒ€ ìˆ˜ëª…: {t.get('maxCount', 0)}íšŒ")
            return "\n".join(lines)
        
        if "tools" in data:
            lines.append(f"ê³µêµ¬ ëª©ë¡ ({len(data['tools'])}ê°œ):")
            for t in data["tools"]:
                lines.append(f"  - {t.get('toolCode', 'N/A')}: {t.get('toolName', 'N/A')} (ìµœëŒ€ {t.get('maxCount', 0)}íšŒ)")
            return "\n".join(lines)
            
        if lines:
            return "\n".join(lines)
        
        return "ë°ì´í„° ì—†ìŒ"
    
    def _format_sensor_data(self, data: Dict) -> str:
        """ì„¼ì„œ ë°ì´í„° í¬ë§·íŒ…"""
        if not data:
            return "ë°ì´í„° ì—†ìŒ"
        
        lines = []
        
        # [NEW] ë‹¤ì¤‘ í•„ë“œ í†µê³„ ì²˜ë¦¬ (stats_{Field})
        stats_keys = sorted([k for k in data.keys() if k.startswith("stats_")])
        for k in stats_keys:
            field_name = k.replace("stats_", "")
            item = data[k]
            p_type = item.get("type")
            res_data = item.get("data")
            
            if not res_data: continue
            
            # Daily Stats
            if p_type == "daily_stats":
                daily_list = res_data.get("daily", [])
                total = res_data.get("total", {})
                if daily_list:
                    lines.append(f"\n[ì¼ë³„ {field_name} í†µê³„ (ìµœê·¼ {total.get('period_days', len(daily_list))}ì¼)]")
                    for d in daily_list:
                        day = d.get("day", "N/A")
                        vals = []
                        if d.get("mean") is not None: vals.append(f"í‰ê·  {d['mean']}")
                        if d.get("max") is not None: vals.append(f"ìµœëŒ€ {d['max']}")
                        if d.get("min") is not None: vals.append(f"ìµœì†Œ {d['min']}")
                        lines.append(f"* {day}: {', '.join(vals)}")
                if total:
                    lines.append(f"* ì „ì²´ ìš”ì•½: í‰ê·  {total.get('mean')}, ìµœëŒ€ {total.get('max')}, ìµœì†Œ {total.get('min')}")

            # Weekly Stats
            elif p_type == "weekly_stats":
                weekly_list = res_data.get("weekly", [])
                total = res_data.get("total", {})
                if weekly_list:
                    lines.append(f"\n[ì£¼ë³„ {field_name} í†µê³„ (ìµœê·¼ {total.get('period_weeks', len(weekly_list))}ì£¼)]")
                    for w in weekly_list:
                        week = w.get("week", "N/A")
                        vals = []
                        if w.get("mean") is not None: vals.append(f"í‰ê·  {w['mean']}")
                        if w.get("max") is not None: vals.append(f"ìµœëŒ€ {w['max']}")
                        if w.get("min") is not None: vals.append(f"ìµœì†Œ {w['min']}")
                        lines.append(f"* {week}: {', '.join(vals)}")
                if total:
                    lines.append(f"* ì „ì²´ ìš”ì•½: í‰ê·  {total.get('mean')}, ìµœëŒ€ {total.get('max')}, ìµœì†Œ {total.get('min')}")

            # Monthly Stats
            elif p_type == "monthly_stats":
                monthly_list = res_data.get("monthly", [])
                total = res_data.get("total", {})
                if monthly_list:
                    lines.append(f"\n[ì›”ë³„ {field_name} í†µê³„ (ìµœê·¼ {total.get('period_months', len(monthly_list))}ê°œì›”)]")
                    for m in monthly_list:
                        month = m.get("month", "N/A")
                        vals = []
                        if m.get("mean") is not None: vals.append(f"í‰ê·  {m['mean']}")
                        if m.get("max") is not None: vals.append(f"ìµœëŒ€ {m['max']}")
                        if m.get("min") is not None: vals.append(f"ìµœì†Œ {m['min']}")
                        lines.append(f"* {month}: {', '.join(vals)}")
                if total:
                    lines.append(f"* ì „ì²´ ìš”ì•½: í‰ê·  {total.get('mean')}, ìµœëŒ€ {total.get('max')}, ìµœì†Œ {total.get('min')}")

            # Raw Stats
            elif p_type == "raw_stats":
                stats = res_data
                if stats.get("mean") is not None:
                    lines.append(f"\n[ì „ì²´ {field_name} í†µê³„ ({stats.get('hours')}ì‹œê°„)]")
                    lines.append(f"* í‰ê· : {stats['mean']:.1f}, ìµœëŒ€: {stats['max']:.1f}, ìµœì†Œ: {stats['min']:.1f}")

        if "current_status" in data:
            status = data["current_status"]
            lines.append(f"* ê°€ë™ ìƒíƒœ: {status.get('run_status', 'N/A')}")
            lines.append(f"* í˜„ì¬ ë¶€í•˜: {status.get('current_load', 'N/A')}")
            lines.append(f"* ì´ì†¡ ì†ë„: {status.get('current_feed', 'N/A')}")
            lines.append(f"* FOV: {status.get('fov', 'N/A')}%, SOV: {status.get('sov', 'N/A')}%")
        
        if "running_stats" in data:
            stats = data["running_stats"]
            if stats.get("mean") is not None:
                 lines.append(f"\n[ê°€ë™ ì¤‘ Load í†µê³„ ({stats.get('hours')}ì‹œê°„)]")
                 lines.append(f"* í‰ê· : {stats['mean']:.1f}")
                 lines.append(f"* ìµœëŒ€: {stats['max']:.1f}")
                 lines.append(f"* ìµœì†Œ: {stats['min']:.1f}")
        
        if "raw_stats" in data:
            stats = data["raw_stats"]
            if stats.get("mean") is not None:
                 lines.append(f"\n[ì „ì²´ Load í†µê³„ ({stats.get('hours')}ì‹œê°„)]")
                 lines.append(f"* í‰ê· : {stats['mean']:.1f}")
                 lines.append(f"* ìµœëŒ€: {stats['max']:.1f}")
                 lines.append(f"* ìµœì†Œ: {stats['min']:.1f}")
        
        if "trend" in data:
            trend = data["trend"]
            if trend:
                lines.append(f"\n[Load íŠ¸ë Œë“œ ({len(trend)}ê°œ í¬ì¸íŠ¸)]")
                for t in trend[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                    time_str = t.get('time', 'N/A')
                    if hasattr(time_str, 'strftime'):
                        time_str = time_str.strftime("%H:%M")
                    lines.append(f"* {time_str}: {t.get('value', 'N/A'):.1f}")
                if len(trend) > 10:
                    lines.append(f"  ... ì™¸ {len(trend) - 10}ê°œ")
        
        if "runtime" in data:
            rt = data["runtime"]
            lines.append(f"\n[ê°€ë™ ì‹œê°„/ë¥  ({rt.get('period_hours', 24)}ì‹œê°„)]")
            lines.append(f"* ê°€ë™ ì‹œê°„: {rt.get('runtime_hours', 0)}ì‹œê°„ ({rt.get('runtime_minutes', 0)}ë¶„)")
            lines.append(f"* ê°€ë™ë¥ : {rt.get('operating_rate', 0)}%")
        
        if "daily_runtime" in data:
            daily_data = data["daily_runtime"]
            daily_list = daily_data.get("daily", [])
            total = daily_data.get("total", {})
            
            if daily_list:
                lines.append(f"\n[ì¼ë³„ ê°€ë™ë¥  (ìµœê·¼ {total.get('period_days', len(daily_list))}ì¼)]")
                for d in daily_list:
                    date_str = d.get("date", "N/A")
                    hours = d.get("runtime_hours", 0)
                    rate = d.get("operating_rate", 0)
                    # ê°€ë™ë¥ ì— ë”°ë¥¸ ìƒíƒœ ì•„ì´ì½˜
                    icon = "ğŸŸ¢" if rate > 80 else "ğŸŸ¡" if rate > 50 else "ğŸ”´"
                    lines.append(f"* {date_str}: {hours}ì‹œê°„ ({rate}%) {icon}")
            
            if total:
                lines.append(f"\n[ì´ ê°€ë™ë¥ ]")
                lines.append(f"* ê¸°ê°„: {total.get('period_days', 0)}ì¼")
                lines.append(f"* ì´ ê°€ë™ ì‹œê°„: {total.get('runtime_hours', 0)}ì‹œê°„")
                lines.append(f"* í‰ê·  ê°€ë™ë¥ : {total.get('operating_rate', 0)}%")
        
        if "weekly_runtime" in data:
            weekly_data = data["weekly_runtime"]
            weekly_list = weekly_data.get("weekly", [])
            total = weekly_data.get("total", {})
            
            if weekly_list:
                lines.append(f"\n[ì£¼ë³„ ê°€ë™ë¥  (ìµœê·¼ {total.get('period_weeks', len(weekly_list))}ì£¼)]")
                for w in weekly_list:
                    week_start = w.get("week_start", "N/A")
                    hours = w.get("runtime_hours", 0)
                    rate = w.get("operating_rate", 0)
                    icon = "ğŸŸ¢" if rate > 80 else "ğŸŸ¡" if rate > 50 else "ğŸ”´"
                    lines.append(f"* {week_start} ì£¼: {hours}ì‹œê°„ ({rate}%) {icon}")
            
            if total:
                lines.append(f"\n[ì´ ê°€ë™ë¥ ]")
                lines.append(f"* ê¸°ê°„: {total.get('period_weeks', 0)}ì£¼")
                lines.append(f"* ì´ ê°€ë™ ì‹œê°„: {total.get('runtime_hours', 0)}ì‹œê°„")
                lines.append(f"* í‰ê·  ê°€ë™ë¥ : {total.get('operating_rate', 0)}%")
        
        if "monthly_runtime" in data:
            monthly_data = data["monthly_runtime"]
            monthly_list = monthly_data.get("monthly", [])
            total = monthly_data.get("total", {})
            
            if monthly_list:
                lines.append(f"\n[ì›”ë³„ ê°€ë™ë¥  (ìµœê·¼ {total.get('period_months', len(monthly_list))}ê°œì›”)]")
                for m in monthly_list:
                    month = m.get("month", "N/A")
                    hours = m.get("runtime_hours", 0)
                    rate = m.get("operating_rate", 0)
                    icon = "ğŸŸ¢" if rate > 80 else "ğŸŸ¡" if rate > 50 else "ğŸ”´"
                    lines.append(f"* {month}: {hours}ì‹œê°„ ({rate}%) {icon}")
            
            if total:
                lines.append(f"\n[ì´ ê°€ë™ë¥ ]")
                lines.append(f"* ê¸°ê°„: {total.get('period_months', 0)}ê°œì›”")
                lines.append(f"* ì´ ê°€ë™ ì‹œê°„: {total.get('runtime_hours', 0)}ì‹œê°„")
                lines.append(f"* í‰ê·  ê°€ë™ë¥ : {total.get('operating_rate', 0)}%")
        
        # ê¸°ê°„ë³„ Stats í¬ë§·íŒ…
        if "daily_stats" in data:
            daily_data = data["daily_stats"]
            daily_list = daily_data.get("daily", [])
            total = daily_data.get("total", {})
            
            if daily_list:
                lines.append(f"\n[ì¼ë³„ {total.get('field', 'Load')} í†µê³„ (ìµœê·¼ {total.get('period_days', len(daily_list))}ì¼)]")
                for d in daily_list:
                    day = d.get("day", "N/A")
                    mean = d.get("mean", 0) or 0
                    max_v = d.get("max", 0) or 0
                    min_v = d.get("min", 0) or 0
                    lines.append(f"* {day}: í‰ê·  {mean}, ìµœëŒ€ {max_v}, ìµœì†Œ {min_v}")
            
            if total:
                lines.append(f"\n[ì „ì²´ í†µê³„]")
                lines.append(f"* í‰ê· : {total.get('mean', 0)}, ìµœëŒ€: {total.get('max', 0)}, ìµœì†Œ: {total.get('min', 0)}")
        
        if "weekly_stats" in data:
            weekly_data = data["weekly_stats"]
            weekly_list = weekly_data.get("weekly", [])
            total = weekly_data.get("total", {})
            
            if weekly_list:
                lines.append(f"\n[ì£¼ë³„ {total.get('field', 'Load')} í†µê³„ (ìµœê·¼ {total.get('period_weeks', len(weekly_list))}ì£¼)]")
                for w in weekly_list:
                    week = w.get("week", "N/A")
                    mean = w.get("mean", 0) or 0
                    max_v = w.get("max", 0) or 0
                    min_v = w.get("min", 0) or 0
                    lines.append(f"* {week}: í‰ê·  {mean}, ìµœëŒ€ {max_v}, ìµœì†Œ {min_v}")
            
            if total:
                lines.append(f"\n[ì „ì²´ í†µê³„]")
                lines.append(f"* í‰ê· : {total.get('mean', 0)}, ìµœëŒ€: {total.get('max', 0)}, ìµœì†Œ: {total.get('min', 0)}")
        
        if "monthly_stats" in data:
            monthly_data = data["monthly_stats"]
            monthly_list = monthly_data.get("monthly", [])
            total = monthly_data.get("total", {})
            
            if monthly_list:
                lines.append(f"\n[ì›”ë³„ {total.get('field', 'Load')} í†µê³„ (ìµœê·¼ {total.get('period_months', len(monthly_list))}ê°œì›”)]")
                for m in monthly_list:
                    month = m.get("month", "N/A")
                    mean = m.get("mean", 0) or 0
                    max_v = m.get("max", 0) or 0
                    min_v = m.get("min", 0) or 0
                    lines.append(f"* {month}: í‰ê·  {mean}, ìµœëŒ€ {max_v}, ìµœì†Œ {min_v}")
            
            if total:
                lines.append(f"\n[ì „ì²´ í†µê³„]")
                lines.append(f"* í‰ê· : {total.get('mean', 0)}, ìµœëŒ€: {total.get('max', 0)}, ìµœì†Œ: {total.get('min', 0)}")

        # CT í†µê³„ í¬ë§·íŒ…
        if "daily_ct_stats" in data:
            ct_data = data["daily_ct_stats"]
            daily_list = ct_data.get("daily", [])
            total = ct_data.get("total", {})
            
            if daily_list:
                lines.append(f"\n[ì¼ë³„ CT(Cycle Time) í†µê³„ (ìµœê·¼ {total.get('period_days', len(daily_list))}ì¼)]")
                for d in daily_list:
                    lines.append(f"* {d.get('date')}: í‰ê·  {d.get('mean_ct', 0)}ì´ˆ")
            
            if total:
                lines.append(f"\n[ì „ì²´ CT í†µê³„]")
                lines.append(f"* í‰ê·  CT: {total.get('mean', 0)}ì´ˆ")
                lines.append(f"* ìµœëŒ€ CT: {total.get('max', 0)}ì´ˆ, ìµœì†Œ CT: {total.get('min', 0)}ì´ˆ")
                lines.append(f"* ì´ ìƒì‚° íšŸìˆ˜(ì§‘ê³„): {total.get('count', 0)}íšŒ")

        if "weekly_ct_stats" in data:
            ct_data = data["weekly_ct_stats"]
            weekly_list = ct_data.get("weekly", [])
            total = ct_data.get("total", {})
            
            if weekly_list:
                lines.append(f"\n[ì£¼ë³„ CT(Cycle Time) í†µê³„ (ìµœê·¼ {total.get('period_weeks', len(weekly_list))}ì£¼)]")
                for w in weekly_list:
                    lines.append(f"* {w.get('week_start')} ì£¼: í‰ê·  {w.get('mean_ct', 0)}ì´ˆ")
            
            if total:
                lines.append(f"\n[ì „ì²´ CT í†µê³„]")
                lines.append(f"* í‰ê·  CT: {total.get('mean', 0)}ì´ˆ")
                lines.append(f"* ìµœëŒ€ CT: {total.get('max', 0)}ì´ˆ, ìµœì†Œ CT: {total.get('min', 0)}ì´ˆ")
                lines.append(f"* ì´ ìƒì‚° íšŸìˆ˜(ì§‘ê³„): {total.get('count', 0)}íšŒ")

        if "monthly_ct_stats" in data:
            ct_data = data["monthly_ct_stats"]
            monthly_list = ct_data.get("monthly", [])
            total = ct_data.get("total", {})
            
            if monthly_list:
                lines.append(f"\n[ì›”ë³„ CT(Cycle Time) í†µê³„ (ìµœê·¼ {total.get('period_months', len(monthly_list))}ê°œì›”)]")
                for m in monthly_list:
                    lines.append(f"* {m.get('month')}: í‰ê·  {m.get('mean_ct', 0)}ì´ˆ")
            
            if total:
                lines.append(f"\n[ì „ì²´ CT í†µê³„]")
                lines.append(f"* í‰ê·  CT: {total.get('mean', 0)}ì´ˆ")
                lines.append(f"* ìµœëŒ€ CT: {total.get('max', 0)}ì´ˆ, ìµœì†Œ CT: {total.get('min', 0)}ì´ˆ")
                lines.append(f"* ì´ ìƒì‚° íšŸìˆ˜(ì§‘ê³„): {total.get('count', 0)}íšŒ")
        
        return "\n".join(lines)

    async def _handle_general_conversation(self, original_q: str, aware_q: str, hist_key: str, start_time: float) -> QueryResponse:
        """ë°ì´í„° ì†ŒìŠ¤ê°€ ì—†ì„ ì‹œ ì¼ë°˜ ëŒ€í™”ë¡œ ì‘ë‹µ"""
        answer = await self.answer_generator.generate_general_conversation_response(aware_q)
        self.conversation_manager.add_to_history(hist_key, original_q, answer, [], 0.8)
        return QueryResponse(answer=answer, sources=[], confidence=0.8, processing_time=time.time() - start_time)

    def _create_simple_response(self, text: str, req: QueryRequest, key: str, start: float) -> QueryResponse:
        """ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„± ë° íˆìŠ¤í† ë¦¬ ê¸°ë¡"""
        self.conversation_manager.add_to_history(key, req.question, text, [], 1.0)
        return QueryResponse(answer=text, sources=[], confidence=1.0, processing_time=time.time() - start)

    async def health_check(self) -> Dict[str, Any]:
        """ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸"""
        return {
            "status": "healthy" if self._initialized else "initializing",
            "mongodb": await self.mongodb.health_check(),
            "influxdb": await self.influxdb.health_check(),
            "retriever": await self.retriever.vector_store.health_check()
        }

    async def cleanup(self):
        """ìì› ì •ë¦¬"""
        await asyncio.gather(
            self.mongodb.close(),
            self.retriever.vector_store.cleanup(),
            self.gemini.cleanup(),
            return_exceptions=True
        )
        self._initialized = False


# ì „ì—­ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
hybrid_rag_engine = HybridRAGEngine()
