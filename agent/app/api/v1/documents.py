from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from typing import Dict, Any, Optional, Tuple
import logging
import time
import hashlib
import shutil
import os
from pathlib import Path

# TOKENIZERS_PARALLELISM ê²½ê³  í•´ê²°
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class ProcessingProgress:
    """ë¬¸ì„œ ì²˜ë¦¬ ì§„í–‰ ìƒí™©ì„ ì¶”ì í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, document_id: str, filename: str):
        self.document_id = document_id
        self.filename = filename
        self.current_step = ""
        self.step_progress = 0.0
        self.total_steps = 6
        self.current_step_index = 0
        
        self.steps = [
            "ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ",
            "ğŸ“– PDF íŒŒì‹±", 
            "âœ‚ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹",
            "ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¶”ì¶œ",
            "ğŸ‘ï¸ OCR ì²˜ë¦¬", 
            "ğŸ§  ì„ë² ë”© ìƒì„± ë° ë²¡í„° ì €ì¥"
        ]
        
    def start_step(self, step_index: int):
        """ë‹¨ê³„ ì‹œì‘"""
        self.current_step_index = step_index
        self.current_step = self.steps[step_index]
        self.step_progress = 0.0
        self._log_progress()
        self._send_websocket_progress()
        
    def update_step_progress(self, progress: float):
        """í˜„ì¬ ë‹¨ê³„ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        self.step_progress = min(100.0, max(0.0, progress))
        self._log_progress()
        self._send_websocket_progress()
        
    def complete_step(self):
        """í˜„ì¬ ë‹¨ê³„ ì™„ë£Œ"""
        self.step_progress = 100.0
        self._log_progress()
        self._send_websocket_progress()
    
    def _send_websocket_progress(self):
        """WebSocketìœ¼ë¡œ ì§„í–‰ë¥  ì „ì†¡ (ë¹„ë™ê¸°)"""
        try:
            import asyncio
            from app.core.websocket_manager import progress_websocket
            
            # í˜„ì¬ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ìˆëŠ”ì§€ í™•ì¸
            try:
                loop = asyncio.get_running_loop()
                # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
                loop.create_task(progress_websocket.send_progress(
                    self.document_id,
                    self.current_step,
                    self.step_progress,
                    f"{self.current_step_index + 1}/{self.total_steps} - {self.step_progress:.1f}%"
                ))
            except RuntimeError:
                # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ë¬´ì‹œ
                pass
        except Exception as e:
            # WebSocket ì—ëŸ¬ëŠ” ë¬´ì‹œ (ë¡œê·¸ ì¶œë ¥ì€ ê³„ì†)
            pass
        
    def _log_progress(self):
        """ì§„í–‰ ìƒí™©ì„ ë¡œê·¸ë¡œ ì¶œë ¥"""
        overall_progress = (self.current_step_index + self.step_progress / 100.0) / self.total_steps * 100
        
        progress_bar = self._create_progress_bar(self.step_progress)
        
        # í„°ë¯¸ë„ê³¼ ë¡œê·¸ ëª¨ë‘ì— ì¶œë ¥
        progress_msg = f"ğŸ“‹ ì²˜ë¦¬ ì¤‘: {self.filename}"
        step_msg = f"ğŸ”„ {self.current_step}"
        step_progress_msg = f"ğŸ“Š ë‹¨ê³„ ì§„í–‰ë¥ : {progress_bar} {self.step_progress:.1f}%"
        overall_progress_msg = f"ğŸ“ˆ ì „ì²´ ì§„í–‰ë¥ : {overall_progress:.1f}% ({self.current_step_index + 1}/{self.total_steps})"
        separator = "=" * 70
        
        # í„°ë¯¸ë„ ì‹¤ì‹œê°„ ì¶œë ¥
        print(f"\n{progress_msg}", flush=True)
        print(step_msg, flush=True)
        print(step_progress_msg, flush=True)
        print(overall_progress_msg, flush=True)
        print(separator, flush=True)
        
        # ë¡œê·¸ íŒŒì¼ì—ë„ ê¸°ë¡
        logger.info(f"\n{progress_msg}")
        logger.info(step_msg)
        logger.info(step_progress_msg)
        logger.info(overall_progress_msg)
        logger.info(separator)
        
    def _create_progress_bar(self, progress: float, width: int = 30) -> str:
        """ì§„í–‰ë¥  ë°” ìƒì„±"""
        filled = int(width * progress / 100)
        bar = "â–ˆ" * filled + "â–‘" * (width - filled)
        return f"[{bar}]"

from app.core.document_processor import document_processor
from app.core.vector_store import vector_store
from app.models.schemas import ProcessingResult, DocumentUploadResponse, DocumentDeleteResponse
from app.core.config import settings

logger = logging.getLogger(__name__)
router = APIRouter()

# ë¬¸ì„œ ì²˜ë¦¬ê¸° (ì „ì—­ ì¸ìŠ¤í„´ìŠ¤)
processor = None


@router.post("/upload", response_model=DocumentUploadResponse)
async def upload_document(
    file: UploadFile = File(...),
    user_id: str = Form("anonymous"),
    document_type: Optional[str] = Form(None)
) -> DocumentUploadResponse:
    """PDF, Word, í…ìŠ¤íŠ¸ íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬"""
    start_time = time.time()
    
    try:
        # íŒŒì¼ í™•ì¥ì ê²€ì¦
        allowed_extensions = {'.pdf', '.docx', '.doc', '.txt', '.md'}
        file_extension = Path(file.filename).suffix.lower()
        
        if file_extension not in allowed_extensions:
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(allowed_extensions)}"
            )
        
        # íŒŒì¼ í¬ê¸° ê²€ì¦ (50MB ì œí•œ)
        max_size = getattr(settings, 'MAX_FILE_SIZE', 52428800)  # 50MB
        if hasattr(file, 'size') and file.size > max_size:
            raise HTTPException(
                status_code=413, 
                detail=f"íŒŒì¼ì´ ë„ˆë¬´ í½ë‹ˆë‹¤. ìµœëŒ€ {max_size // 1024 // 1024}MBê¹Œì§€ ì§€ì›í•©ë‹ˆë‹¤."
            )
        
        # ê³ ìœ  ë¬¸ì„œ ID ìƒì„±
        file_hash = hashlib.md5(f"{user_id}_{file.filename}_{time.time()}".encode()).hexdigest()
        
        # ì§„í–‰ ìƒí™© ì¶”ì  ì‹œì‘
        progress = ProcessingProgress(file_hash, file.filename)
        progress.start_step(0)  # íŒŒì¼ ì—…ë¡œë“œ
        
        # íŒŒì¼ ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ì½ê¸°
        file_content = await file.read()
        progress.complete_step()
        
        print(f"\nâœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file.filename} (í¬ê¸°: {len(file_content):,} bytes)", flush=True)
        logger.info(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file.filename} (í¬ê¸°: {len(file_content)} bytes, ì‚¬ìš©ì: {user_id})")
        
        # ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° DB ì €ì¥
        print(f"\nğŸš€ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file.filename}", flush=True)
        logger.info(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file.filename}")
        try:
            processing_result = await _process_and_store_document_from_memory(
                file_content=file_content,
                file_extension=file_extension,
                user_id=user_id,
                document_id=file_hash,
                original_filename=file.filename,
                progress=progress
            )
            
            print(f"\nâœ… ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ì €ì¥ ì™„ë£Œ: {file.filename}", flush=True)
            logger.info(f"ë¬¸ì„œ ì²˜ë¦¬ ë° ë²¡í„° ì €ì¥ ì™„ë£Œ: {file.filename}")
            
            return DocumentUploadResponse(
                document_id=file_hash,
                status="completed",
                text_chunks=processing_result.get("text_chunks", 0),
                image_chunks=processing_result.get("image_chunks", 0),
                total_embeddings=processing_result.get("total_embeddings", 0),
                processing_time=time.time() - start_time
            )
            
        except Exception as processing_error:
            logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {processing_error}")
            
            # ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œì—ë„ íŒŒì¼ì€ ì—…ë¡œë“œëœ ìƒíƒœë¡œ ìœ ì§€ (ì¬ì²˜ë¦¬ ê°€ëŠ¥)
            error_detail = str(processing_error)
            if "embedding" in error_detail.lower():
                error_msg = "ì„ë² ë”© ìƒì„± ì‹¤íŒ¨"
            elif "vector" in error_detail.lower() or "qdrant" in error_detail.lower():
                error_msg = "ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨"
            elif "datetime" in error_detail.lower():
                error_msg = "ë‚ ì§œ ì²˜ë¦¬ ì˜¤ë¥˜"
            else:
                error_msg = "ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜"
            
            return DocumentUploadResponse(
                document_id=file_hash,
                status="failed",
                text_chunks=0,
                image_chunks=0,
                total_embeddings=0,
                processing_time=time.time() - start_time
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.post("/process-document", response_model=Dict[str, Any])
async def process_document(
    file_path: str,
    user_id: str
) -> Dict[str, Any]:
    """Backendì—ì„œ í˜¸ì¶œí•˜ëŠ” ë¬¸ì„œ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸"""
    global processor
    
    start_time = time.time()
    
    try:
        # ì…ë ¥ ê²€ì¦
        if not file_path.strip():
            raise HTTPException(status_code=400, detail="íŒŒì¼ ê²½ë¡œê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        if not user_id.strip():
            raise HTTPException(status_code=400, detail="ì‚¬ìš©ì IDê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not Path(file_path).exists():
            raise HTTPException(status_code=404, detail=f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        
        logger.info(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file_path} (ì‚¬ìš©ì: {user_id})")
        
        # ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
        if processor is None:
            # TODO: ì‹¤ì œ MultiModalDocumentProcessor êµ¬í˜„ í›„ ì‚¬ìš©
            # processor = MultiModalDocumentProcessor()
            # await processor.initialize()
            pass
        
        # ì„ì‹œ êµ¬í˜„: ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        result = await _process_simple_document(file_path, user_id)
        
        # ë²¡í„° DBì— ì €ì¥
        collection_name = f"documents_{user_id}"
        
        # ì„ë² ë”©ì´ ìˆëŠ” ì²­í¬ë“¤ë§Œ ì €ì¥
        all_chunks = result.text_chunks + result.image_chunks
        if all_chunks:
            stored_count = await vector_store.store_embeddings(collection_name, all_chunks)
        else:
            stored_count = 0
        
        processing_time = time.time() - start_time
        
        # ë¬¸ì„œ ID ìƒì„±
        document_id = hashlib.md5(f"{file_path}_{user_id}".encode()).hexdigest()
        
        response = {
            "document_id": document_id,
            "status": "processed",
            "text_chunks": len(result.text_chunks),
            "image_chunks": len(result.image_chunks),
            "total_embeddings": stored_count,
            "processing_time": processing_time
        }
        
        logger.info(f"ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {response}")
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


async def _process_and_store_document_from_memory(
    file_content: bytes,
    file_extension: str,
    user_id: str, 
    document_id: str, 
    original_filename: str,
    progress: ProcessingProgress
) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ì˜ íŒŒì¼ ë‚´ìš©ì„ ì§ì ‘ ì²˜ë¦¬í•˜ê³  Qdrant ë²¡í„° DBì— ì €ì¥"""
    from app.core.embedding_manager import embedding_manager
    from app.models.schemas import DocumentChunk
    import uuid
    from datetime import datetime
    
    try:
        # ì´ë¯¸ ì´ˆê¸°í™”ëœ ì„œë¹„ìŠ¤ë“¤ ì‚¬ìš©
        from app.core.rag_engine import rag_engine
        
        # RAG ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if not rag_engine._initialized:
            print("\nğŸ”„ RAG ì—”ì§„ ì´ˆê¸°í™” ì‹œì‘...", flush=True)
            logger.info("RAG ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ. ì´ˆê¸°í™” ì‹œì‘...")
            await rag_engine.initialize()
            print("âœ… RAG ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!", flush=True)
        else:
            print("âœ… ì´ë¯¸ ì´ˆê¸°í™”ëœ RAG ì—”ì§„ ì‚¬ìš©", flush=True)
            logger.info("ì´ë¯¸ ì´ˆê¸°í™”ëœ RAG ì—”ì§„ ì‚¬ìš©")
        
        text_chunks = 0
        image_chunks = 0
        chunks = []
        
        if file_extension in ['.txt', '.md']:
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
            try:
                content = file_content.decode('utf-8')
            except UnicodeDecodeError:
                # UTF-8 ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
                try:
                    content = file_content.decode('cp949')  # í•œêµ­ì–´ ì¸ì½”ë”©
                except UnicodeDecodeError:
                    content = file_content.decode('latin-1', errors='ignore')
            
            chunks = await _process_text_content_from_string(
                content, document_id, original_filename, rag_engine.embedding_manager
            )
            text_chunks = len(chunks)
            
        elif file_extension == '.pdf':
            # PDF íŒŒì¼ì„ ì„ì‹œë¡œ ì €ì¥í•´ì„œ ì²˜ë¦¬ (PyMuPDF ë“±ì´ íŒŒì¼ ê²½ë¡œ í•„ìš”)
            import tempfile
            import os
            
            progress.start_step(1)  # PDF íŒŒì‹±
            
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
                temp_file.write(file_content)
                temp_path = temp_file.name
            
            progress.complete_step()
            
            try:
                chunks, image_count = await _process_pdf_with_images(
                    temp_path, document_id, original_filename, progress, rag_engine.embedding_manager
                )
                text_chunks = len([c for c in chunks if c.metadata.get('content_type') == 'text'])
                image_chunks = len([c for c in chunks if c.metadata.get('content_type') == 'image'])
            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
                    
        else:
            # ê¸°íƒ€ íŒŒì¼ì€ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬ ì‹œë„
            try:
                content = file_content.decode('utf-8', errors='ignore')
                chunks = await _process_text_content_from_string(
                    content, document_id, original_filename, rag_engine.embedding_manager
                )
                text_chunks = len(chunks)
            except Exception as e:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ ì²˜ë¦¬ ì‹¤íŒ¨: {file_extension}, {e}")
                chunks = []
        
        # ë²¡í„° DBì— ì €ì¥
        if chunks:
            progress.start_step(5)  # ë²¡í„° ì €ì¥
            progress.update_step_progress(50.0)
            
            await rag_engine.vector_store.add_documents(chunks, user_id)
            
            progress.complete_step()
            print(f"\nğŸ’¾ Qdrantì— {len(chunks):,}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ: {original_filename}", flush=True)
            logger.info(f"Qdrantì— {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ: {original_filename}")
        
        return {
            "text_chunks": text_chunks,
            "image_chunks": image_chunks,
            "total_embeddings": len(chunks)
        }
        
    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ íŒŒì¼ ì²˜ë¦¬ ë° ì €ì¥ ì‹¤íŒ¨: {e}")
        raise


async def _process_text_content_from_string(
    content: str, 
    document_id: str, 
    original_filename: str,
    embedding_manager
) -> list:
    """ë¬¸ìì—´ ì½˜í…ì¸ ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  ì„ë² ë”© ìƒì„± (íŒŒì¼ ê²½ë¡œ ì—†ì´)"""
    from app.models.schemas import DocumentChunk
    import uuid
    from datetime import datetime
    
    chunks = []
    chunk_size = 1000  # 1000ì ë‹¨ìœ„ë¡œ ì²­í‚¹
    
    for i in range(0, len(content), chunk_size):
        chunk_text = content[i:i+chunk_size].strip()
        if not chunk_text:
            continue
            
        # ì„ë² ë”© ìƒì„±
        embedding = await embedding_manager.embed_text(chunk_text)
        
        chunk = DocumentChunk(
            id=str(uuid.uuid4()),
            content=chunk_text,
            embedding=embedding,
            metadata={
                "document_id": document_id,
                "original_filename": original_filename,
                "chunk_index": len(chunks),
                "file_type": "text",
                "created_at": str(datetime.now())
            }
        )
        chunks.append(chunk)
    
    return chunks


async def _process_and_store_document(
    file_path: str, 
    user_id: str, 
    document_id: str, 
    original_filename: str
) -> Dict[str, Any]:
    """ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  Qdrant ë²¡í„° DBì— ì €ì¥"""
    from app.core.embedding_manager import embedding_manager
    from app.models.schemas import DocumentChunk
    import uuid
    from datetime import datetime
    
    try:
        # ì„ë² ë”© ë§¤ë‹ˆì €ì™€ ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™”
        await embedding_manager.initialize()
        await vector_store.initialize()
        
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì²˜ë¦¬
        file_extension = Path(file_path).suffix.lower()
        text_chunks = 0
        image_chunks = 0
        
        if file_extension == '.txt' or file_extension == '.md':
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            chunks = await _process_text_content(
                content, file_path, document_id, original_filename
            )
            text_chunks = len(chunks)
            
        elif file_extension == '.pdf':
            # PDF íŒŒì¼ ì²˜ë¦¬ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + OCR)
            chunks, image_count = await _process_pdf_with_images(
                file_path, document_id, original_filename
            )
            text_chunks = len([c for c in chunks if c.metadata.get('content_type') == 'text'])
            image_chunks = len([c for c in chunks if c.metadata.get('content_type') == 'image'])
                    
        else:
            # ê¸°íƒ€ íŒŒì¼ì€ í…ìŠ¤íŠ¸ë¡œ ì½ê¸° ì‹œë„
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                chunks = await _process_text_content(
                    content, file_path, document_id, original_filename
                )
                text_chunks = len(chunks)
            except:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")
                chunks = []
        
        # ë²¡í„° DBì— ì €ì¥
        if chunks:
            await vector_store.add_documents(chunks, user_id)
            logger.info(f"Qdrantì— {len(chunks)}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ: {original_filename}")
        
        return {
            "text_chunks": text_chunks,
            "image_chunks": image_chunks,
            "total_embeddings": len(chunks)
        }
        
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ë° ì €ì¥ ì‹¤íŒ¨: {e}")
        raise


async def _process_text_content(
    content: str, 
    file_path: str, 
    document_id: str, 
    original_filename: str
) -> list:
    """í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê³  ì„ë² ë”© ìƒì„±"""
    from app.core.embedding_manager import embedding_manager
    from app.models.schemas import DocumentChunk
    import uuid
    from datetime import datetime
    
    chunks = []
    chunk_size = 1000  # 1000ì ë‹¨ìœ„ë¡œ ì²­í‚¹
    
    for i in range(0, len(content), chunk_size):
        chunk_text = content[i:i+chunk_size].strip()
        if not chunk_text:
            continue
            
        # ì„ë² ë”© ìƒì„±
        embedding = await embedding_manager.embed_text(chunk_text)
        
        chunk = DocumentChunk(
            id=str(uuid.uuid4()),
            content=chunk_text,
            embedding=embedding,
            metadata={
                "document_id": document_id,
                "file_path": file_path,
                "original_filename": original_filename,
                "chunk_index": len(chunks),
                "file_type": "text",
                "created_at": str(datetime.now())
            }
        )
        chunks.append(chunk)
    
    return chunks


async def _process_pdf_with_images(
    file_path: str, 
    document_id: str, 
    original_filename: str,
    progress: ProcessingProgress,
    embedding_manager
) -> Tuple[list, int]:
    """ê³ ê¸‰ PDF ì²˜ë¦¬: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ì²˜ë¦¬ (OCR í¬í•¨)"""
    from app.core.document_processor import document_processor
    from app.models.schemas import DocumentChunk
    import uuid
    from datetime import datetime
    import fitz  # PyMuPDF
    
    chunks = []
    image_count = 0
    
    try:
        # PyMuPDFë¥¼ ì‚¬ìš©í•´ì„œ PDF ì—´ê¸° (ì´ë¯¸ì§€ ì¶”ì¶œ ê°€ëŠ¥)
        pdf_document = fitz.open(file_path)
        total_pages = len(pdf_document)
        
        progress.start_step(2)  # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹ ë‹¨ê³„
        for page_num in range(total_pages):
            page = pdf_document[page_num]
            
            # í˜ì´ì§€ë³„ ì§„í–‰ë¥  ê³„ì‚°
            text_progress = (page_num / total_pages) * 100
            progress.update_step_progress(text_progress)
            
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            page_text = page.get_text().strip()
            if page_text:
                # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ê¸°
                chunk_size = 1500
                for i in range(0, len(page_text), chunk_size):
                    chunk_text = page_text[i:i+chunk_size].strip()
                    if not chunk_text:
                        continue
                    
                    embedding = await embedding_manager.embed_text(chunk_text)
                    
                    chunk = DocumentChunk(
                        id=str(uuid.uuid4()),
                        content=chunk_text,
                        embedding=embedding,
                        metadata={
                            "document_id": document_id,
                            "file_path": file_path,
                            "original_filename": original_filename,
                            "page": page_num + 1,
                            "chunk_index": len(chunks),
                            "content_type": "text",
                            "file_type": "pdf",
                            "created_at": str(datetime.now())
                        }
                    )
                    chunks.append(chunk)
        
        progress.complete_step()  # í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ ë‹¨ê³„
        progress.start_step(3)  # ì´ë¯¸ì§€ ì¶”ì¶œ
        
        total_images = 0
        # ì „ì²´ ì´ë¯¸ì§€ ìˆ˜ ê³„ì‚°
        for page_num in range(total_pages):
            page = pdf_document[page_num]
            total_images += len(page.get_images())
        
        processed_images = 0
        
        # OCR ì²˜ë¦¬ ë‹¨ê³„
        if total_images > 0:
            progress.complete_step()  # ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ
            progress.start_step(4)  # OCR ì²˜ë¦¬ ì‹œì‘
            
            for page_num in range(total_pages):
                page = pdf_document[page_num]
                image_list = page.get_images()
                
                for img_index, img in enumerate(image_list):
                    try:
                        # OCR ì§„í–‰ë¥  ê³„ì‚°
                        ocr_progress = (processed_images / total_images) * 100
                        progress.update_step_progress(ocr_progress)
                        
                        # ì´ë¯¸ì§€ ì¶”ì¶œ
                        xref = img[0]
                        pix = fitz.Pixmap(pdf_document, xref)
                        
                        if pix.n - pix.alpha < 4:  # GRAY ë˜ëŠ” RGB
                            # PIL Imageë¡œ ë³€í™˜
                            img_data = pix.tobytes("png")
                            
                            # OCR ìˆ˜í–‰
                            ocr_text = await _perform_ocr_on_image(img_data)
                            
                            if ocr_text and ocr_text.strip():  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ëª¨ë“  ê²°ê³¼ ì €ì¥
                                # OCR ê²°ê³¼ë¥¼ ì²­í¬ë¡œ ì €ì¥
                                try:
                                    embedding = await embedding_manager.embed_text(ocr_text)
                                    
                                    chunk = DocumentChunk(
                                        id=str(uuid.uuid4()),
                                        content=ocr_text,
                                        embedding=embedding,
                                        metadata={
                                            "document_id": document_id,
                                            "original_filename": original_filename,
                                            "page": page_num + 1,
                                            "image_index": img_index,
                                            "chunk_index": len(chunks),
                                            "content_type": "image",
                                            "ocr_engine": "tesseract",
                                            "file_type": "pdf",
                                            "created_at": str(datetime.now())
                                        }
                                    )
                                    chunks.append(chunk)
                                    image_count += 1
                                    
                                    logger.info(f"PDF ì´ë¯¸ì§€ OCR ì„±ê³µ: í˜ì´ì§€ {page_num + 1}, ì´ë¯¸ì§€ {img_index + 1} - {len(ocr_text)} ë¬¸ì")
                                except Exception as embed_error:
                                    logger.warning(f"OCR í…ìŠ¤íŠ¸ ì„ë² ë”© ì‹¤íŒ¨: {embed_error}")
                            else:
                                logger.debug(f"ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ë¯¸ë°œê²¬: í˜ì´ì§€ {page_num + 1}, ì´ë¯¸ì§€ {img_index + 1}")
                        
                        pix = None
                        processed_images += 1
                        
                    except Exception as e:
                        logger.warning(f"PDF ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ (í˜ì´ì§€ {page_num + 1}, ì´ë¯¸ì§€ {img_index + 1}): {e}")
                        processed_images += 1
                        continue
            
            progress.complete_step()  # OCR ì²˜ë¦¬ ì™„ë£Œ
        else:
            progress.complete_step()  # ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ (ì´ë¯¸ì§€ ì—†ìŒ)
            progress.start_step(4)  # OCR ë‹¨ê³„ ê±´ë„ˆë›°ê¸°
            progress.complete_step()
        
        pdf_document.close()
        
        logger.info(f"PDF ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ (ì´ë¯¸ì§€ {image_count}ê°œ í¬í•¨)")
        return chunks, image_count
        
    except Exception as e:
        logger.warning(f"PDF ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬: {e}")
        # Fallback: ê¸°ë³¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
        try:
            chunks = await _process_pdf_file(file_path, document_id, original_filename)
            logger.info(f"PDF í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
            return chunks, 0
        except Exception as fallback_e:
            logger.error(f"PDF í…ìŠ¤íŠ¸ ì²˜ë¦¬ë„ ì‹¤íŒ¨: {fallback_e}")
            return [], 0


async def _perform_ocr_on_image(image_data: bytes) -> str:
    """ì´ë¯¸ì§€ ë°ì´í„°ì— OCRì„ ìˆ˜í–‰ - ëª¨ë“  ì´ë¯¸ì§€ ì²˜ë¦¬"""
    try:
        # pytesseract ì§ì ‘ ì‚¬ìš©
        import pytesseract
        from PIL import Image
        import io
        
        # ì´ë¯¸ì§€ ë¡œë“œ
        image = Image.open(io.BytesIO(image_data))
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OCR ì„±ëŠ¥ í–¥ìƒ)
        # ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ í™•ëŒ€
        if image.width < 100 or image.height < 100:
            # 2ë°° í™•ëŒ€
            new_size = (image.width * 2, image.height * 2)
            image = image.resize(new_size, Image.Resampling.LANCZOS)
        
        # ì´ë¯¸ì§€ ëª¨ë“œ ìµœì í™”
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # OCR ìˆ˜í–‰ (ë‹¤ì–‘í•œ ì„¤ì •ìœ¼ë¡œ ì‹œë„)
        ocr_configs = [
            '--psm 3',  # ê¸°ë³¸ ìë™ í˜ì´ì§€ ë¶„í• 
            '--psm 6',  # ë‹¨ì¼ ë¸”ë¡
            '--psm 8',  # ë‹¨ì¼ ë‹¨ì–´
            '--psm 7',  # ë‹¨ì¼ í…ìŠ¤íŠ¸ ë¼ì¸
            '--psm 13', # ì›ì‹œ ë¼ì¸, Tesseract-specific ì²˜ë¦¬ ì—†ìŒ
            ''          # ê¸°ë³¸ ì„¤ì •
        ]
        
        # ì–¸ì–´ë³„ ì‹œë„
        languages = ['kor+eng', 'eng', 'kor']
        
        best_text = ""
        max_length = 0
        
        for lang in languages:
            for config in ocr_configs:
                try:
                    if lang and config:
                        text = pytesseract.image_to_string(image, lang=lang, config=config)
                    elif lang:
                        text = pytesseract.image_to_string(image, lang=lang)
                    elif config:
                        text = pytesseract.image_to_string(image, config=config)
                    else:
                        text = pytesseract.image_to_string(image)
                    
                    # ê°€ì¥ ê¸´ ê²°ê³¼ë¥¼ ì„ íƒ
                    if text and len(text.strip()) > max_length:
                        max_length = len(text.strip())
                        best_text = text.strip()
                        
                    # ì¶©ë¶„íˆ ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì°¾ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                    if len(text.strip()) > 50:
                        return text.strip()
                        
                except Exception as e:
                    continue
        
        return best_text if best_text else ""
        
    except ImportError:
        logger.warning("pytesseract ë¯¸ì„¤ì¹˜. ì´ë¯¸ì§€ OCRì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return ""
    except Exception as e:
        logger.debug(f"ì´ë¯¸ì§€ OCR ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (ì²˜ë¦¬ ê³„ì†)
        return ""


async def _delete_document_from_vector_db(collection_name: str, document_id: str) -> int:
    """ë²¡í„° DBì—ì„œ íŠ¹ì • document_idë¥¼ ê°€ì§„ ëª¨ë“  ì ë“¤ ì‚­ì œ"""
    try:
        # ë¨¼ì € í•´ë‹¹ document_idë¥¼ ê°€ì§„ ëª¨ë“  ì ë“¤ ì°¾ê¸°
        from app.core.embedding_manager import embedding_manager
        await embedding_manager.initialize()
        
        # ë”ë¯¸ ê²€ìƒ‰ìœ¼ë¡œ ëª¨ë“  ì  ê°€ì ¸ì˜¤ê¸°
        test_embedding = await embedding_manager.embed_text("test")
        all_docs = await vector_store.search_similar(
            collection_name=collection_name,
            query_vector=test_embedding,
            limit=10000,  # ì¶©ë¶„íˆ í° ìˆ˜
            score_threshold=0.0
        )
        
        # document_idê°€ ì¼ì¹˜í•˜ëŠ” ì ë“¤ì˜ ID ìˆ˜ì§‘
        points_to_delete = []
        for doc in all_docs:
            if doc.metadata.get("document_id") == document_id:
                points_to_delete.append(doc.document_id)  # Qdrant point ID
        
        # ì ë“¤ ì‚­ì œ
        if points_to_delete:
            # Qdrantì—ì„œ ì ë“¤ ì‚­ì œ
            await vector_store.client.delete(
                collection_name=collection_name,
                points_selector={"points": points_to_delete}
            )
            logger.info(f"ë²¡í„° DBì—ì„œ {len(points_to_delete)}ê°œ ì  ì‚­ì œ ì™„ë£Œ: {document_id}")
        
        return len(points_to_delete)
        
    except Exception as e:
        logger.error(f"ë²¡í„° DB ì‚­ì œ ì‹¤íŒ¨: {e}")
        return 0


async def _delete_document_by_filename(collection_name: str, filename: str) -> int:
    """ë²¡í„° DBì—ì„œ íŠ¹ì • filenameì„ ê°€ì§„ ëª¨ë“  ì ë“¤ ì‚­ì œ"""
    try:
        # Qdrantì—ì„œ ì§ì ‘ í•„í„°ë§ìœ¼ë¡œ ì°¾ê¸° (ë” íš¨ìœ¨ì )
        from qdrant_client.models import Filter, FieldCondition, MatchValue
        
        # ë‹¤ì–‘í•œ í•„í„° ì¡°ê±´ ì‹œë„
        filters_to_try = [
            # original_filenameìœ¼ë¡œ ì •í™•íˆ ë§¤ì¹­
            Filter(must=[FieldCondition(key="original_filename", match=MatchValue(value=filename))]),
            # filenameìœ¼ë¡œ ì •í™•íˆ ë§¤ì¹­  
            Filter(must=[FieldCondition(key="filename", match=MatchValue(value=filename))]),
        ]
        
        total_deleted = 0
        
        for i, filter_condition in enumerate(filters_to_try):
            try:
                logger.info(f"ì‚­ì œ ì‹œë„ {i+1}: í•„í„° ì¡°ê±´ìœ¼ë¡œ '{filename}' ê²€ìƒ‰")
                
                # ì¡°ê±´ì— ë§ëŠ” ì ë“¤ ê²€ìƒ‰
                search_result = await vector_store.client.scroll(
                    collection_name=collection_name,
                    scroll_filter=filter_condition,
                    limit=10000,
                    with_payload=True,
                    with_vectors=False
                )
                
                points_found = search_result[0] if search_result else []
                logger.info(f"í•„í„° {i+1}ë¡œ ì°¾ì€ ì  ìˆ˜: {len(points_found)}")
                
                if points_found:
                    # ì ë“¤ì˜ ID ìˆ˜ì§‘
                    point_ids = [point.id for point in points_found]
                    
                    # ì‹¤ì œ ì‚­ì œ ì‹¤í–‰
                    delete_result = await vector_store.client.delete(
                        collection_name=collection_name,
                        points_selector={"points": point_ids}
                    )
                    
                    deleted_count = len(point_ids)
                    total_deleted += deleted_count
                    
                    logger.info(f"í•„í„° {i+1}ë¡œ {deleted_count}ê°œ ì  ì‚­ì œ ì™„ë£Œ")
                    
                    # ì²« ë²ˆì§¸ ì„±ê³µí•œ í•„í„°ë¡œ ì‚­ì œëìœ¼ë©´ ë‚˜ë¨¸ì§€ëŠ” ì‹œë„í•˜ì§€ ì•ŠìŒ
                    if deleted_count > 0:
                        logger.info(f"'{filename}' ì‚­ì œ ì„±ê³µ: ì´ {total_deleted}ê°œ ì  ì‚­ì œë¨")
                        return total_deleted
                        
            except Exception as filter_error:
                logger.warning(f"í•„í„° {i+1} ì‚­ì œ ì‹œë„ ì‹¤íŒ¨: {filter_error}")
                continue
        
        # í•„í„°ë§ìœ¼ë¡œ ì•ˆ ë˜ë©´ ì „ì²´ ê²€ìƒ‰ í›„ ë§¤ì¹­ (fallback)
        if total_deleted == 0:
            logger.info("í•„í„°ë§ ì‹¤íŒ¨, ì „ì²´ ê²€ìƒ‰ìœ¼ë¡œ fallback")
            
            from app.core.embedding_manager import embedding_manager
            await embedding_manager.initialize()
            
            # ë”ë¯¸ ê²€ìƒ‰ìœ¼ë¡œ ëª¨ë“  ì  ê°€ì ¸ì˜¤ê¸°
            test_embedding = await embedding_manager.embed_text("test")
            all_docs = await vector_store.search_similar(
                collection_name=collection_name,
                query_vector=test_embedding,
                limit=10000,
                score_threshold=0.0
            )
            
            logger.info(f"ì „ì²´ ê²€ìƒ‰ìœ¼ë¡œ {len(all_docs)}ê°œ ë¬¸ì„œ í™•ì¸")
            
            # filenameì´ ì¼ì¹˜í•˜ëŠ” ì ë“¤ ì°¾ê¸°
            points_to_delete = []
            
            for doc in all_docs:
                doc_filename = doc.metadata.get("filename", "")
                doc_original_filename = doc.metadata.get("original_filename", "")
                
                # ë‹¨ìˆœí•˜ê³  í™•ì‹¤í•œ ë§¤ì¹­
                if (doc_original_filename == filename or 
                    doc_filename == filename or
                    os.path.basename(doc_original_filename) == filename or
                    os.path.basename(doc_filename) == filename):
                    
                    points_to_delete.append(doc.document_id)
                    logger.info(f"ë§¤ì¹­ ë°œê²¬: original='{doc_original_filename}', filename='{doc_filename}'")
            
            # ì‚­ì œ ì‹¤í–‰
            if points_to_delete:
                await vector_store.client.delete(
                    collection_name=collection_name,
                    points_selector={"points": points_to_delete}
                )
                total_deleted = len(points_to_delete)
                logger.info(f"Fallbackìœ¼ë¡œ {total_deleted}ê°œ ì  ì‚­ì œ ì™„ë£Œ")
            else:
                logger.warning(f"'{filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        return total_deleted
        
    except Exception as e:
        logger.error(f"ë²¡í„° DB íŒŒì¼ëª… ê¸°ë°˜ ì‚­ì œ ì‹¤íŒ¨: {e}")
        return 0


async def _process_pdf_file(
    file_path: str, 
    document_id: str, 
    original_filename: str
) -> list:
    """PDF íŒŒì¼ ì²˜ë¦¬"""
    import PyPDF2
    from app.core.embedding_manager import embedding_manager
    from app.models.schemas import DocumentChunk
    import uuid
    from datetime import datetime
    
    chunks = []
    
    with open(file_path, 'rb') as pdf_file:
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        for page_num, page in enumerate(pdf_reader.pages):
            try:
                page_text = page.extract_text().strip()
                if not page_text:
                    continue
                
                # í˜ì´ì§€ë³„ë¡œ ì²­í‚¹ (í° í˜ì´ì§€ëŠ” ë” ë‚˜ëˆŒ ìˆ˜ ìˆìŒ)
                chunk_size = 1500
                for i in range(0, len(page_text), chunk_size):
                    chunk_text = page_text[i:i+chunk_size].strip()
                    if not chunk_text:
                        continue
                    
                    # ì„ë² ë”© ìƒì„±
                    embedding = await embedding_manager.embed_text(chunk_text)
                    
                    chunk = DocumentChunk(
                        id=str(uuid.uuid4()),
                        content=chunk_text,
                        embedding=embedding,
                        metadata={
                            "document_id": document_id,
                            "file_path": file_path,
                            "original_filename": original_filename,
                            "page": page_num + 1,
                            "chunk_index": len(chunks),
                            "file_type": "pdf",
                            "created_at": str(datetime.now())
                        }
                    )
                    chunks.append(chunk)
                    
            except Exception as e:
                logger.warning(f"PDF í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
    
    return chunks


async def _process_simple_document(file_path: str, user_id: str) -> ProcessingResult:
    """ì„ì‹œ êµ¬í˜„: ê°„ë‹¨í•œ ë¬¸ì„œ ì²˜ë¦¬"""
    from app.core.embedding_manager import embedding_manager
    from app.models.schemas import DocumentChunk
    import uuid
    from datetime import datetime
    
    try:
        # ì„ë² ë”© ë§¤ë‹ˆì € ì´ˆê¸°í™”
        await embedding_manager.initialize()
        
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ê°„ë‹¨í•œ ì²˜ë¦¬
        file_extension = Path(file_path).suffix.lower()
        
        if file_extension == '.txt':
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ê°„ë‹¨í•œ ì²­í‚¹ (1000ì ë‹¨ìœ„)
            chunks = []
            for i in range(0, len(content), 1000):
                chunk_text = content[i:i+1000]
                if chunk_text.strip():
                    # ì„ë² ë”© ìƒì„±
                    embedding = await embedding_manager.embed_text(chunk_text)
                    
                    chunk = DocumentChunk(
                        id=str(uuid.uuid4()),
                        content=chunk_text,
                        embedding=embedding,
                        metadata={
                            "file_path": file_path,
                            "chunk_index": len(chunks),
                            "file_type": "text"
                        }
                    )
                    chunks.append(chunk)
            
            return ProcessingResult(
                text_chunks=chunks,
                image_chunks=[],
                total_embeddings=len(chunks)
            )
        
        else:
            # ë‹¤ë¥¸ íŒŒì¼ í˜•ì‹ì€ ì•„ì§ ë¯¸êµ¬í˜„
            raise HTTPException(
                status_code=400,
                detail=f"ì•„ì§ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_extension}"
            )
            
    except Exception as e:
        logger.error(f"ê°„ë‹¨ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise


@router.get("/status/{document_id}")
async def get_document_status(document_id: str):
    """ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    # TODO: ì‹¤ì œ ë¬¸ì„œ ìƒíƒœ ì¶”ì  ì‹œìŠ¤í…œ êµ¬í˜„
    return {
        "document_id": document_id,
        "status": "completed",
        "message": "ë¬¸ì„œ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"
    }


@router.get("/list")
async def list_documents(user_id: str = "anonymous") -> Dict[str, Any]:
    """ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ - ë²¡í„° DBì—ì„œ ì‹¤ì œ ì €ì¥ëœ ë¬¸ì„œ í™•ì¸"""
    try:
        # ë²¡í„° DB ì´ˆê¸°í™”
        await vector_store.initialize()
        
        # ì‚¬ìš©ìë³„ ì»¬ë ‰ì…˜ëª…
        collection_name = f"documents_{user_id}"
        
        try:
            # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
            await vector_store.ensure_collection(collection_name)
            
            # ë²¡í„° DBì—ì„œ ëª¨ë“  ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            from app.core.embedding_manager import embedding_manager
            await embedding_manager.initialize()
            
            # ë”ë¯¸ ì„ë² ë”©ìœ¼ë¡œ ëª¨ë“  ë¬¸ì„œ ê²€ìƒ‰ (score_threshold=0.0ìœ¼ë¡œ ëª¨ë“  ê²°ê³¼ ë°˜í™˜)
            test_embedding = await embedding_manager.embed_text("test")
            all_docs = await vector_store.search_similar(
                collection_name=collection_name,
                query_vector=test_embedding,
                limit=1000,  # ì¶©ë¶„íˆ í° ìˆ˜
                score_threshold=0.0  # ëª¨ë“  ë¬¸ì„œ ë°˜í™˜
            )
            
            # ë¬¸ì„œë³„ë¡œ ê·¸ë£¹í™”
            doc_groups = {}
            for doc in all_docs:
                doc_id = doc.metadata.get("document_id", "unknown")
                filename = doc.metadata.get("original_filename", "Unknown")
                
                if doc_id not in doc_groups:
                    doc_groups[doc_id] = {
                        "document_id": doc_id,
                        "original_filename": filename,
                        "file_path": doc.metadata.get("file_path", ""),
                        "file_type": doc.metadata.get("file_type", "unknown"),
                        "created_at": doc.metadata.get("created_at", ""),
                        "chunks": 0,
                        "total_content_length": 0
                    }
                
                doc_groups[doc_id]["chunks"] += 1
                doc_groups[doc_id]["total_content_length"] += len(doc.content)
            
            # ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            uploaded_files = []
            for doc_info in doc_groups.values():
                uploaded_files.append({
                    "filename": f"{doc_info['document_id']}_{doc_info['original_filename']}",
                    "original_name": doc_info["original_filename"],
                    "document_id": doc_info["document_id"],
                    "chunks": doc_info["chunks"],
                    "content_length": doc_info["total_content_length"],
                    "uploaded_at": doc_info["created_at"][:19] if doc_info["created_at"] else "",
                    "file_type": doc_info["file_type"],
                    "stored_in_vector_db": True
                })
            
            return {
                "files": uploaded_files,
                "total_count": len(uploaded_files),
                "collection_name": collection_name,
                "total_chunks": sum(doc["chunks"] for doc in uploaded_files)
            }
            
        except Exception as vector_error:
            logger.error(f"ë²¡í„° DBì—ì„œ ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {vector_error}")
            raise HTTPException(
                status_code=500,
                detail=f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(vector_error)}"
            )
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.delete("/delete/{document_id}")
async def delete_document(document_id: str, user_id: str = "anonymous") -> DocumentDeleteResponse:
    """ì—…ë¡œë“œëœ ë¬¸ì„œ ì‚­ì œ - ë²¡í„° DBì—ì„œë§Œ ì‚­ì œ (íŒŒì¼ ì‹œìŠ¤í…œ ì‚¬ìš© ì•ˆí•¨)"""
    try:
        # ë²¡í„° DBì—ì„œ ë¬¸ì„œ ì‚­ì œ
        deleted_from_vector_db = False
        deleted_count = 0
        
        try:
            # ë²¡í„° DB ì´ˆê¸°í™”
            await vector_store.initialize()
            collection_name = f"documents_{user_id}"
            
            # í•´ë‹¹ document_idë¥¼ ê°€ì§„ ëª¨ë“  ì ë“¤ ì‚­ì œ
            deleted_count = await _delete_document_from_vector_db(collection_name, document_id)
            
            if deleted_count > 0:
                deleted_from_vector_db = True
                logger.info(f"ë²¡í„° DBì—ì„œ {deleted_count}ê°œ ì²­í¬ ì‚­ì œ: {document_id}")
            else:
                logger.warning(f"ì‚­ì œí•  ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {document_id}")
                
        except Exception as vector_error:
            logger.error(f"ë²¡í„° DB ì‚­ì œ ì‹¤íŒ¨: {vector_error}")
            raise HTTPException(
                status_code=500,
                detail=f"ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(vector_error)}"
            )
        
        if deleted_from_vector_db:
            message = f"ë¬¸ì„œ '{document_id}'ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ({deleted_count}ê°œ ì²­í¬ ì‚­ì œë¨)"
            success = True
        else:
            message = f"ë¬¸ì„œ '{document_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì‚­ì œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            success = False
            
        return DocumentDeleteResponse(
            message=message, 
            deleted_chunks=deleted_count,
            success=success
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.delete("/clear-all")
async def clear_all_documents(user_id: str = "anonymous") -> DocumentDeleteResponse:
    """ì‚¬ìš©ìì˜ ëª¨ë“  ë¬¸ì„œë¥¼ ë²¡í„° DBì—ì„œ ì‚­ì œ"""
    try:
        deleted_count = 0
        
        try:
            # ë²¡í„° DB ì´ˆê¸°í™”
            await vector_store.initialize()
            collection_name = f"documents_{user_id}"
            
            # ì»¬ë ‰ì…˜ ì „ì²´ ì‚­ì œ ì‹œë„
            try:
                # ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                import asyncio
                collections = await asyncio.to_thread(vector_store.client.get_collections)
                collection_exists = any(col.name == collection_name for col in collections.collections)
                
                if collection_exists:
                    # ì»¬ë ‰ì…˜ ì‚­ì œ
                    await asyncio.to_thread(vector_store.client.delete_collection, collection_name)
                    logger.info(f"ì»¬ë ‰ì…˜ '{collection_name}' ì „ì²´ ì‚­ì œ ì™„ë£Œ")
                    
                    # ìƒˆë¡œìš´ ë¹ˆ ì»¬ë ‰ì…˜ ìƒì„±
                    await vector_store._create_collection(collection_name)
                    logger.info(f"ìƒˆë¡œìš´ ë¹ˆ ì»¬ë ‰ì…˜ '{collection_name}' ìƒì„± ì™„ë£Œ")
                    
                    deleted_count = "ì „ì²´"  # ì „ì²´ ì‚­ì œë¥¼ í‘œì‹œ
                    message = f"ì‚¬ìš©ì '{user_id}'ì˜ ëª¨ë“  ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
                    success = True
                else:
                    message = f"ì‚¬ìš©ì '{user_id}'ì˜ ë¬¸ì„œ ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    success = False
                    
            except Exception as collection_error:
                logger.warning(f"ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨, ê°œë³„ ì  ì‚­ì œë¡œ ì „í™˜: {collection_error}")
                
                # ì»¬ë ‰ì…˜ ì‚­ì œê°€ ì‹¤íŒ¨í•˜ë©´ ëª¨ë“  ì  ê°œë³„ ì‚­ì œ
                from app.core.embedding_manager import embedding_manager
                await embedding_manager.initialize()
                
                # ë”ë¯¸ ê²€ìƒ‰ìœ¼ë¡œ ëª¨ë“  ì  ê°€ì ¸ì˜¤ê¸°
                test_embedding = await embedding_manager.embed_text("test")
                all_docs = await vector_store.search_similar(
                    collection_name=collection_name,
                    query_vector=test_embedding,
                    limit=50000,  # ë§¤ìš° í° ìˆ˜
                    score_threshold=0.0
                )
                
                if all_docs:
                    # ëª¨ë“  ì ì˜ ID ìˆ˜ì§‘
                    all_point_ids = [doc.document_id for doc in all_docs]
                    
                    # ë°°ì¹˜ë¡œ ì‚­ì œ (Qdrant ì œí•œ ê³ ë ¤)
                    batch_size = 1000
                    total_deleted = 0
                    
                    for i in range(0, len(all_point_ids), batch_size):
                        batch_ids = all_point_ids[i:i + batch_size]
                        from qdrant_client.models import PointIdsList
                        await asyncio.to_thread(
                            vector_store.client.delete,
                            collection_name=collection_name,
                            points_selector=PointIdsList(points=batch_ids)
                        )
                        total_deleted += len(batch_ids)
                        logger.info(f"ë°°ì¹˜ ì‚­ì œ ì§„í–‰: {total_deleted}/{len(all_point_ids)}")
                    
                    deleted_count = total_deleted
                    message = f"ì‚¬ìš©ì '{user_id}'ì˜ ëª¨ë“  ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ({total_deleted}ê°œ ì²­í¬ ì‚­ì œë¨)"
                    success = True
                else:
                    message = f"ì‚¬ìš©ì '{user_id}'ì˜ ë¬¸ì„œê°€ ì´ë¯¸ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
                    success = False
                
        except Exception as vector_error:
            logger.error(f"ë²¡í„° DB ì „ì²´ ì‚­ì œ ì‹¤íŒ¨: {vector_error}")
            raise HTTPException(
                status_code=500,
                detail=f"ì „ì²´ ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(vector_error)}"
            )
        
        return DocumentDeleteResponse(
            message=message,
            deleted_chunks=deleted_count if isinstance(deleted_count, int) else 0,
            success=success
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì „ì²´ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ì „ì²´ ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.delete("/delete-by-name/{filename}")
async def delete_document_by_name(filename: str, user_id: str = "anonymous") -> DocumentDeleteResponse:
    """íŒŒì¼ëª…ìœ¼ë¡œ ë¬¸ì„œ ì‚­ì œ - ë²¡í„° DBì—ì„œë§Œ ì‚­ì œ"""
    try:
        # ë²¡í„° DBì—ì„œ í•´ë‹¹ íŒŒì¼ëª…ì„ ê°€ì§„ ë¬¸ì„œë“¤ ê²€ìƒ‰
        deleted_from_vector_db = False
        deleted_count = 0
        
        try:
            # ë²¡í„° DB ì´ˆê¸°í™”
            await vector_store.initialize()
            collection_name = f"documents_{user_id}"
            
            # filenameì„ ê¸°ì¤€ìœ¼ë¡œ ì‚­ì œ
            deleted_count = await _delete_document_by_filename(collection_name, filename)
            
            if deleted_count > 0:
                deleted_from_vector_db = True
                logger.info(f"ë²¡í„° DBì—ì„œ {deleted_count}ê°œ ì²­í¬ ì‚­ì œ (íŒŒì¼ëª…: {filename})")
            else:
                logger.warning(f"ì‚­ì œí•  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {filename}")
                
        except Exception as vector_error:
            logger.error(f"ë²¡í„° DB ì‚­ì œ ì‹¤íŒ¨: {vector_error}")
            raise HTTPException(
                status_code=500,
                detail=f"ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(vector_error)}"
            )
        
        if deleted_from_vector_db:
            message = f"íŒŒì¼ '{filename}'ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. ({deleted_count}ê°œ ì²­í¬ ì‚­ì œë¨)"
            success = True
        else:
            message = f"íŒŒì¼ '{filename}'ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì‚­ì œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            success = False
            
        return DocumentDeleteResponse(
            message=message, 
            deleted_chunks=deleted_count,
            success=success
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@router.get("/vector-status")
async def get_vector_status() -> Dict[str, Any]:
    """ë²¡í„° DB(Qdrant) ìƒíƒœ ì¡°íšŒ"""
    try:
        await vector_store.initialize()
        
        # Qdrant ì»¬ë ‰ì…˜ ì •ë³´ ì¡°íšŒ
        status = await vector_store.get_collection_info()
        
        return {
            "qdrant_status": "connected",
            "collection_info": status,
            "message": "ë²¡í„° DBê°€ ì •ìƒì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤"
        }
        
    except Exception as e:
        logger.error(f"ë²¡í„° DB ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {
            "qdrant_status": "error",
            "collection_info": None,
            "message": f"ë²¡í„° DB ì—°ê²° ì˜¤ë¥˜: {str(e)}"
        }


@router.get("/search-test")
async def test_vector_search(query: str = "í…ŒìŠ¤íŠ¸") -> Dict[str, Any]:
    """ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
    try:
        from app.core.embedding_manager import embedding_manager
        from app.core.rag_engine import rag_engine
        
        # RAG ì—”ì§„ ì´ˆê¸°í™” ë° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        await rag_engine.initialize()
        
        # ì„ë² ë”© ìƒì„±
        query_embedding = await embedding_manager.embed_text(query)
        
        # ë²¡í„° ê²€ìƒ‰
        results = await vector_store.search_similar(
            collection_name="documents_test_user",
            query_vector=query_embedding,
            limit=3,
            score_threshold=0.0  # ëª¨ë“  ê²°ê³¼ ë°˜í™˜
        )
        
        return {
            "query": query,
            "results_count": len(results),
            "results": [
                {
                    "content": result.content[:200] + "..." if len(result.content) > 200 else result.content,
                    "score": result.score,
                    "metadata": result.metadata
                }
                for result in results
            ]
        }
        
    except Exception as e:
        logger.error(f"ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"ë²¡í„° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}"
        )