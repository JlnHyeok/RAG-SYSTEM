# RAG (Retrieval-Augmented Generation) ì‹œìŠ¤í…œ ê°œë… ì •ë¦¬

## 1. RAGê°€ ë“±ì¥í•œ ë°°ê²½ê³¼ í•„ìš”ì„±

### 1.1 ê¸°ì¡´ AI ì‹œìŠ¤í…œì˜ í•œê³„

**ì „í†µì ì¸ LLMì˜ ê·¼ë³¸ì  ë¬¸ì œë“¤**

1. **í™˜ê° í˜„ìƒ (Hallucination)**

   ```
   ë¬¸ì œ: LLMì´ ê·¸ëŸ´ë“¯í•˜ì§€ë§Œ í‹€ë¦° ì •ë³´ë¥¼ ìì‹ ìˆê²Œ ì œê³µ
   ì›ì¸: í›ˆë ¨ ë°ì´í„°ì— ì—†ê±°ë‚˜ ë¶€ì •í™•í•œ íŒ¨í„´ í•™ìŠµ

   ì˜ˆì‹œ:
   Q: "ìš°ë¦¬ íšŒì‚¬ì˜ 2024ë…„ Q3 ë§¤ì¶œì€?"
   A: "ì•½ 500ë§Œ ë‹¬ëŸ¬ë¡œ ì¶”ì •ë©ë‹ˆë‹¤." (ì™„ì „íˆ ì§€ì–´ë‚¸ ë‹µë³€)

   Q: "ì´ ê±´ì¶•ë„ë©´ì—ì„œ ë³´ì¼ëŸ¬ì‹¤ í¬ê¸°ëŠ”?"
   A: "ì•½ 20í‰ë°©ë¯¸í„°ë¡œ ë³´ì…ë‹ˆë‹¤." (ì´ë¯¸ì§€ë¥¼ ì œëŒ€ë¡œ ì½ì§€ ëª»í–ˆìŒì—ë„ ì¶”ì •ì¹˜ ì œê³µ)
   ```

2. **ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ ë¶€ì¡±**

   ```
   ë¬¸ì œ: ì¼ë°˜ì ì¸ LLMì€ íŠ¹ì • ê¸°ì—…/ì¡°ì§ì˜ ë‚´ë¶€ ì§€ì‹ ë¶€ì¡±
   ê²°ê³¼: íšŒì‚¬ ì •ì±…, ë‚´ë¶€ í”„ë¡œì„¸ìŠ¤, ì „ë¬¸ ê¸°ìˆ  ë¬¸ì„œ ì´í•´ ë¶ˆê°€

   ì˜ˆì‹œ:
   Q: "ìš°ë¦¬ íšŒì‚¬ íœ´ê°€ ì •ì±…ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
   A: "ì¼ë°˜ì ì¸ íœ´ê°€ ì •ì±…ì— ëŒ€í•´ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤..." (íšŒì‚¬ë³„ íŠ¹ìˆ˜ì„± ë¬´ì‹œ)

   Q: "ì´ ê¸°ê³„ì„¤ê³„ë„ë©´ì˜ ì¹˜ìˆ˜ ê³µì°¨ëŠ”?"
   A: "ë„ë©´ì„ ì§ì ‘ í•´ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." (ì „ë¬¸ ë„ë©´ í•´ì„ ëŠ¥ë ¥ ë¶€ì¬)

   Q: "PDF ë‚´ CAD ì´ë¯¸ì§€ì˜ ë¶€í’ˆ ë²ˆí˜¸ëŠ”?"
   A: "ì´ë¯¸ì§€ ë‚´ í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." (OCR í•œê³„)
   ```

3. **ë¹„ìš©ê³¼ í™•ì¥ì„± ë¬¸ì œ**
   ```
   ê¸°ì¡´ í•´ê²°ì±…: LLM íŒŒì¸íŠœë‹ ë˜ëŠ” ì¬í•™ìŠµ
   ë¬¸ì œì :
   - ë§¤ë²ˆ ìˆ˜ì‹­ë§Œ ë‹¬ëŸ¬ì˜ GPU ë¹„ìš©
   - ëª‡ ì£¼ì—ì„œ ëª‡ ë‹¬ì˜ í•™ìŠµ ì‹œê°„
   - ìƒˆë¡œìš´ ì •ë³´ ì¶”ê°€ ì‹œ ì „ì²´ ì¬í•™ìŠµ í•„ìš”
   - ë°°í¬ ë° ìœ ì§€ë³´ìˆ˜ ë³µì¡ì„±
   ```

### 1.2 RAG ë“±ì¥ì˜ í•„ì—°ì„±

**2020ë…„ Facebook AIì˜ RAG ë…¼ë¬¸ ë“±ì¥ ë°°ê²½**

Facebook AI Research(í˜„ Meta AI)ê°€ 2020ë…„ì— ë°œí‘œí•œ "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks" ë…¼ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ì¸ì‚¬ì´íŠ¸ì—ì„œ ì¶œë°œí–ˆìŠµë‹ˆë‹¤:

```python
# ì´ ì½”ë“œëŠ” ê¸°ì¡´ LLMê³¼ RAG ì‹œìŠ¤í…œì˜ ì ‘ê·¼ ë°©ì‹ ì°¨ì´ë¥¼ ê°œë…ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤

# ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì  - ëª¨ë“  ì§€ì‹ì„ ëª¨ë¸ì— ì••ì¶•
class TraditionalLLM:
    """ì „í†µì ì¸ LLMì€ ëª¨ë“  ì§€ì‹ì„ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ì €ì¥í•˜ëŠ” ë°©ì‹"""
    def __init__(self):
        self.parameters = "175B"  # GPT-3 ìˆ˜ì¤€ì˜ íŒŒë¼ë¯¸í„° ìˆ˜
        self.knowledge = "ëª¨ë“  ì§€ì‹ì´ íŒŒë¼ë¯¸í„°ì— ì••ì¶•ë¨"  # í›ˆë ¨ ì‹œì ì˜ ì§€ì‹ë§Œ í¬í•¨
        self.update_cost = "ë§¤ìš° ë†’ìŒ"  # ìƒˆ ì§€ì‹ ì¶”ê°€ ì‹œ ì „ì²´ ì¬í•™ìŠµ í•„ìš”

    def answer_question(self, question):
        """íŒŒë¼ë¯¸í„°ì— ì €ì¥ëœ ì§€ì‹ìœ¼ë¡œë§Œ ë‹µë³€ ìƒì„± (ì™¸ë¶€ ì°¸ì¡° ë¶ˆê°€)"""
        return self.generate_from_parameters(question)

# RAGì˜ í˜ì‹ ì  ì ‘ê·¼
class RAGSystem:
    """RAGëŠ” ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ì™€ ì—°ë™í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì •ë³´ë¥¼ í™œìš©"""
    def __init__(self):
        self.llm = "ë” ì‘ì€ ëª¨ë¸ë„ ê°€ëŠ¥"  # ëª¨ë“  ì§€ì‹ì„ ì €ì¥í•  í•„ìš” ì—†ì–´ íš¨ìœ¨ì 
        self.external_knowledge = "ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•œ ì™¸ë¶€ DB"  # ë²¡í„° DB, ë¬¸ì„œ ì €ì¥ì†Œ ë“±
        self.update_cost = "ë‚®ìŒ"  # ìƒˆ ë¬¸ì„œ ì¶”ê°€ë§Œìœ¼ë¡œ ì§€ì‹ ì—…ë°ì´íŠ¸ ê°€ëŠ¥

    def answer_question(self, question):
        """ì§ˆë¬¸ì— ë§ëŠ” ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì•„ì„œ í•¨ê»˜ LLMì— ì „ë‹¬í•˜ëŠ” 2ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤"""
        # 1. ì™¸ë¶€ ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰
        relevant_docs = self.retrieve(question)
        # 2. ê²€ìƒ‰ëœ ì§€ì‹ê³¼ ì§ˆë¬¸ì„ í•¨ê»˜ LLMì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
        return self.generate_with_context(question, relevant_docs)
```

### 1.3 RAGì˜ í˜ì‹ ì  ì•„ì´ë””ì–´

**"ê¸°ì–µí•˜ì§€ ë§ê³  ì°¾ì•„ë¼" íŒ¨ëŸ¬ë‹¤ì„**

```
ì¸ê°„ì˜ ë¬¸ì œí•´ê²° ê³¼ì •:
1. ëª¨ë“  ê²ƒì„ ì•”ê¸°í•˜ì§€ ì•ŠìŒ
2. í•„ìš”í•  ë•Œ ì±…, ì¸í„°ë„·, ì „ë¬¸ê°€, ë„ë©´, ë§¤ë‰´ì–¼ì— ë¬¸ì˜
3. ì°¾ì€ ì •ë³´(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ë„í‘œ)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ êµ¬ì„±
4. ë³µì¡í•œ ê¸°ìˆ ë¬¸ì„œë‚˜ ë„ë©´ì€ ì „ë¬¸ê°€ì™€ í•¨ê»˜ í•´ì„

RAGì˜ ì„¤ê³„ ì² í•™:
1. LLMì´ ëª¨ë“  ì§€ì‹ì„ ê¸°ì–µí•  í•„ìš” ì—†ìŒ
2. í•„ìš”í•  ë•Œ ê´€ë ¨ ë¬¸ì„œ(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, PDF)ë¥¼ ê²€ìƒ‰
3. ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ ìƒì„±
4. ì´ë¯¸ì§€ë‚˜ ë„ë©´ ê°™ì€ ë¹„ì •í˜• ë°ì´í„°ë„ í™œìš© ê°€ëŠ¥
```

## 2. RAG ì‹œìŠ¤í…œ ì „ì²´ êµ¬ì¡°ì™€ Agent ì—­í• 

### 2.1 RAG ì‹œìŠ¤í…œ 3ê³„ì¸µ ì•„í‚¤í…ì²˜

**ì „ì²´ ì‹œìŠ¤í…œì€ 3ê°œ ë…ë¦½ì ì¸ ì„œë¹„ìŠ¤ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Frontend        â”‚    â”‚       Backend         â”‚    â”‚        Agent          â”‚
â”‚    (SvelteKit)       â”‚â—„â”€â”€â–ºâ”‚      (NestJS)         â”‚â—„â”€â”€â–ºâ”‚   (Python/FastAPI)    â”‚
â”‚                      â”‚    â”‚                       â”‚    â”‚                       â”‚
â”‚   USER INTERFACE     â”‚    â”‚   API GATEWAY         â”‚    â”‚   AI LOGIC PROCESSING â”‚
â”‚   - Chat UI          â”‚    â”‚   - Authentication    â”‚    â”‚   - LLM Calls         â”‚
â”‚   - File Upload      â”‚    â”‚   - Data Management   â”‚    â”‚   - Embedding Creationâ”‚
â”‚   - Search Results   â”‚    â”‚   - Logging/Monitoringâ”‚    â”‚   - Vector Search     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ê° ì»´í¬ë„ŒíŠ¸ì˜ ì—­í• ê³¼ ì±…ì„

#### 2.2.1 Agent (AI ì²˜ë¦¬ ì—”ì§„) - í•µì‹¬!

**Agentê°€ ë‹´ê³  ìˆëŠ” ê²ƒë“¤:**

```python
# Agentì˜ ì „ì²´ êµ¬ì¡° - ëª¨ë“  AI ê´€ë ¨ ë¡œì§ì´ ì—¬ê¸° ì§‘ì¤‘
class RAGAgent:
    """RAG ì‹œìŠ¤í…œì˜ ë‘ë‡Œ ì—­í•  - ëª¨ë“  AI ì²˜ë¦¬ë¥¼ ë‹´ë‹¹"""

    def __init__(self):
        # 1. LLM í´ë¼ì´ì–¸íŠ¸ (í…ìŠ¤íŠ¸ ìƒì„±)
        self.gemini_client = self._init_gemini()

        # 2. ì„ë² ë”© ëª¨ë¸ë“¤ (ë²¡í„° ë³€í™˜) â† ì—¬ê¸°ì„œ ì„ë² ë”© ëª¨ë¸ ê´€ë¦¬!
        self.embedding_models = {
            "text": SentenceTransformer('all-MiniLM-L6-v2'),
            "multimodal": CLIPModel.from_pretrained("openai/clip-vit-base-patch32"),
            "korean": SentenceTransformer('jhgan/ko-sroberta-multitask')
        }

        # 3. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        self.vector_db = QdrantClient("localhost", port=6333)

        # 4. ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        self.document_processor = MultiModalDocumentProcessor()

        # 5. OCR ì—”ì§„ë“¤ (ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ)
        self.ocr_engines = {
            "tesseract": pytesseract,
            "paddleocr": PaddleOCR(),
            "easyocr": easyocr.Reader(['ko', 'en'])
        }

        # 6. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (í™”ì§ˆ ê°œì„ )
        self.image_enhancer = ImageEnhancer()

    def process_document(self, file_path: str):
        """ë¬¸ì„œ ì—…ë¡œë“œ ì‹œ í˜¸ì¶œ - ë²¡í„°í™”í•´ì„œ DBì— ì €ì¥"""
        # 1. íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
        if file_path.endswith('.pdf'):
            content = self._process_pdf(file_path)
        elif file_path.endswith(('.jpg', '.png')):
            content = self._process_image(file_path)

        # 2. ì„ë² ë”© ìƒì„± (Agent ë‚´ë¶€ì—ì„œ ì²˜ë¦¬)
        embeddings = self.embedding_models["text"].encode(content)

        # 3. ë²¡í„° DBì— ì €ì¥
        self.vector_db.upsert(
            collection_name="documents",
            points=[{"id": uuid.uuid4(), "vector": embeddings, "payload": {"content": content}}]
        )

    def query(self, user_question: str) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬ - RAGì˜ í•µì‹¬ ë¡œì§"""
        # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜ (ì„ë² ë”©)
        question_vector = self.embedding_models["text"].encode([user_question])

        # 2. ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰ (ë²¡í„° ìœ ì‚¬ë„)
        search_results = self.vector_db.search(
            collection_name="documents",
            query_vector=question_vector[0],
            limit=5
        )

        # 3. ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ + ì§ˆë¬¸ì„ LLMì— ì „ë‹¬
        context = "\n".join([result.payload["content"] for result in search_results])

        # 4. Geminië¡œ ìµœì¢… ë‹µë³€ ìƒì„±
        prompt = f"ì»¨í…ìŠ¤íŠ¸: {context}\nì§ˆë¬¸: {user_question}\në‹µë³€:"
        response = self.gemini_client.generate_content(prompt)

        return response.text
```

#### 2.2.2 Backend (ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§)

**Agentì™€ Frontendë¥¼ ì—°ê²°í•˜ëŠ” ì¤‘ê°„ ê³„ì¸µ:**

```python
# Backendì˜ ì—­í•  - Agentë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ê´€ë¦¬
@Controller('rag')
class RAGController:
    """NestJS Backend - API ì—”ë“œí¬ì¸íŠ¸ ì œê³µ"""

    def __init__(self):
        self.agent_client = HTTPClient("http://agent-service:8000")  # Agent í˜¸ì¶œ
        self.user_service = UserService()
        self.document_service = DocumentService()

    @Post('upload')
    async def upload_document(self, file: File, user_id: str):
        """íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬"""
        # 1. ì‚¬ìš©ì ê¶Œí•œ í™•ì¸
        if not await self.user_service.check_permission(user_id):
            raise UnauthorizedException()

        # 2. íŒŒì¼ ì €ì¥
        file_path = await self.save_file(file)

        # 3. Agentì—ê²Œ ë¬¸ì„œ ì²˜ë¦¬ ìš”ì²­ (ì—¬ê¸°ì„œ ì„ë² ë”© ì²˜ë¦¬ë¨)
        result = await self.agent_client.post('/process-document', {
            'file_path': file_path,
            'user_id': user_id
        })

        # 4. ë©”íƒ€ë°ì´í„° DBì— ì €ì¥
        await self.document_service.save_metadata(file_path, user_id, result)

        return {"status": "success", "document_id": result.document_id}

    @Post('query')
    async def query(self, question: str, user_id: str):
        """ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬"""
        # 1. ì‚¬ìš©ì ê¶Œí•œ í™•ì¸
        await self.user_service.validate_user(user_id)

        # 2. Agentì—ê²Œ ì§ˆë¬¸ ì „ë‹¬
        answer = await self.agent_client.post('/query', {
            'question': question,
            'user_id': user_id
        })

        # 3. ëŒ€í™” ì´ë ¥ ì €ì¥
        await self.chat_service.save_conversation(user_id, question, answer)

        return {"answer": answer, "timestamp": new Date()}
```

#### 2.2.3 Frontend (ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤)

**ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ë³´ëŠ” í™”ë©´:**

```svelte
<!-- SvelteKit Frontend - ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ -->
<script>
    import { onMount } from 'svelte';

    let messages = [];
    let userInput = '';
    let isLoading = false;

    async function sendMessage() {
        if (!userInput.trim()) return;

        // ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages = [...messages, { type: 'user', content: userInput }];
        const question = userInput;
        userInput = '';
        isLoading = true;

        try {
            // Backend API í˜¸ì¶œ (Backendì´ Agent í˜¸ì¶œ)
            const response = await fetch('/api/rag/query', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ question })
            });

            const result = await response.json();

            // AI ì‘ë‹µ ì¶”ê°€
            messages = [...messages, { type: 'ai', content: result.answer }];
        } catch (error) {
            messages = [...messages, { type: 'error', content: 'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.' }];
        } finally {
            isLoading = false;
        }
    }

    async function uploadFile(event) {
        const file = event.target.files[0];
        if (!file) return;

        const formData = new FormData();
        formData.append('file', file);

        // Backendì— íŒŒì¼ ì—…ë¡œë“œ (Backendì´ Agentì—ê²Œ ì²˜ë¦¬ ìš”ì²­)
        const response = await fetch('/api/rag/upload', {
            method: 'POST',
            body: formData
        });

        const result = await response.json();
        messages = [...messages, { type: 'system', content: `íŒŒì¼ "${file.name}" ì—…ë¡œë“œ ì™„ë£Œ` }];
    }
</script>

<div class="chat-container">
    <!-- íŒŒì¼ ì—…ë¡œë“œ -->
    <input type="file" on:change={uploadFile} accept=".pdf,.jpg,.png" />

    <!-- ì±„íŒ… ë©”ì‹œì§€ë“¤ -->
    {#each messages as message}
        <div class="message {message.type}">
            {message.content}
        </div>
    {/each}

    <!-- ì…ë ¥ì°½ -->
    <form on:submit|preventDefault={sendMessage}>
        <input bind:value={userInput} placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..." />
        <button type="submit" disabled={isLoading}>
            {isLoading ? 'ì²˜ë¦¬ì¤‘...' : 'ì „ì†¡'}
        </button>
    </form>
</div>
```

### 2.3 ì„ë² ë”© ëª¨ë¸ì€ ì–¸ì œ ì–´ë””ì„œ ê°œë°œí•˜ë‚˜?

**ë‹µ: Agent ê°œë°œí•  ë•Œ í•¨ê»˜ ì‘ì„±í•©ë‹ˆë‹¤!**

```python
# Agent ì„œë¹„ìŠ¤ ê°œë°œ ì‹œ í¬í•¨ë˜ëŠ” ì„ë² ë”© ê´€ë ¨ ì½”ë“œë“¤

# 1. requirements.txt (Agent ì˜ì¡´ì„±)
"""
fastapi==0.104.1
sentence-transformers==2.2.2  # ì„ë² ë”© ëª¨ë¸
qdrant-client==1.6.4          # ë²¡í„° DB
torch==2.1.0                  # GPU ê°€ì†
transformers==4.35.0          # í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸
google-generativeai==0.3.0    # Gemini API
pytesseract==0.3.10           # OCR
opencv-python==4.8.1          # ì´ë¯¸ì§€ ì²˜ë¦¬
"""

# 2. Agent ì„œë¹„ìŠ¤ì˜ ì„ë² ë”© ê´€ë¦¬ì
class EmbeddingManager:
    """ì„ë² ë”© ëª¨ë¸ë“¤ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self):
        # ì—¬ëŸ¬ ì„ë² ë”© ëª¨ë¸ì„ ìš©ë„ë³„ë¡œ ê´€ë¦¬
        self.models = {}
        self._load_models()

    def _load_models(self):
        """ì‹œì‘í•  ë•Œ ëª¨ë“  ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        print("ì„ë² ë”© ëª¨ë¸ë“¤ ë¡œë”© ì¤‘...")

        # í…ìŠ¤íŠ¸ ì„ë² ë”© (ê°€ì¥ ë§ì´ ì‚¬ìš©)
        self.models['text'] = SentenceTransformer('all-MiniLM-L6-v2')

        # í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”©
        self.models['korean'] = SentenceTransformer('jhgan/ko-sroberta-multitask')

        # ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© (í…ìŠ¤íŠ¸+ì´ë¯¸ì§€)
        self.models['clip'] = SentenceTransformer('clip-ViT-B-32')

        print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    def embed_text(self, text: str, model_type: str = 'text'):
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        return self.models[model_type].encode([text])[0]

    def embed_batch(self, texts: List[str], model_type: str = 'text'):
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œë²ˆì— ë²¡í„°ë¡œ ë³€í™˜ (ì„±ëŠ¥ ìµœì í™”)"""
        return self.models[model_type].encode(texts)

# 3. FastAPI Agent ì„œë¹„ìŠ¤ ë©”ì¸ ì½”ë“œ
from fastapi import FastAPI, UploadFile
import google.generativeai as genai

app = FastAPI(title="RAG Agent Service")

# ì „ì—­ ê°ì²´ë“¤ (ì„œë²„ ì‹œì‘ì‹œ í•œë²ˆë§Œ ì´ˆê¸°í™”)
embedding_manager = EmbeddingManager()  # ì„ë² ë”© ëª¨ë¸ë“¤
vector_db = QdrantClient("localhost", port=6333)  # ë²¡í„° DB
genai.configure(api_key="YOUR_GEMINI_API_KEY")  # Gemini ì„¤ì •
gemini_model = genai.GenerativeModel('gemini-pro')

@app.post("/process-document")
async def process_document(file_path: str, user_id: str):
    """Backendì—ì„œ í˜¸ì¶œ - ë¬¸ì„œë¥¼ ì²˜ë¦¬í•´ì„œ ë²¡í„° DBì— ì €ì¥"""

    # 1. ë¬¸ì„œ ë‚´ìš© ì¶”ì¶œ
    content = extract_content_from_file(file_path)

    # 2. ì„ë² ë”© ìƒì„± (ì—¬ê¸°ì„œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©!)
    embeddings = embedding_manager.embed_text(content, 'korean')

    # 3. ë²¡í„° DBì— ì €ì¥
    document_id = str(uuid.uuid4())
    vector_db.upsert(
        collection_name=f"user_{user_id}",
        points=[{
            "id": document_id,
            "vector": embeddings.tolist(),
            "payload": {"content": content, "file_path": file_path}
        }]
    )

    return {"document_id": document_id, "status": "processed"}

@app.post("/query")
async def query(question: str, user_id: str):
    """Backendì—ì„œ í˜¸ì¶œ - ì§ˆë¬¸ì— ë‹µë³€ ìƒì„±"""

    # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜ (ì„ë² ë”©)
    question_vector = embedding_manager.embed_text(question, 'korean')

    # 2. ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
    search_results = vector_db.search(
        collection_name=f"user_{user_id}",
        query_vector=question_vector.tolist(),
        limit=5,
        score_threshold=0.7
    )

    # 3. ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    context = "\n".join([hit.payload["content"] for hit in search_results])

    # 4. Geminië¡œ ë‹µë³€ ìƒì„±
    prompt = f"""
    ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:

    ì»¨í…ìŠ¤íŠ¸:
    {context}

    ì§ˆë¬¸: {question}

    ë‹µë³€:
    """

    response = gemini_model.generate_content(prompt)

    return {
        "answer": response.text,
        "sources": [hit.payload["file_path"] for hit in search_results]
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2.4 ê°œë°œ ìˆœì„œì™€ íƒ€ì„ë¼ì¸

```
ì£¼ì°¨ë³„ ê°œë°œ ê³„íš:

1-2ì£¼ì°¨: Agent ê¸°ë°˜ ì‘ì—… (ê°€ì¥ ì¤‘ìš”!)
â”œâ”€â”€ ì„ë² ë”© ëª¨ë¸ ì„ íƒ ë° í…ŒìŠ¤íŠ¸
â”œâ”€â”€ Gemini API ì—°ë™
â”œâ”€â”€ Qdrant ë²¡í„° DB ì…‹ì—…
â”œâ”€â”€ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
â””â”€â”€ RAG í•µì‹¬ ë¡œì§ ì™„ì„±

3ì£¼ì°¨: Backend ê°œë°œ
â”œâ”€â”€ NestJS API ì„œë²„ êµ¬ì¶•
â”œâ”€â”€ Agent ì„œë¹„ìŠ¤ í˜¸ì¶œ ë¡œì§
â”œâ”€â”€ íŒŒì¼ ì—…ë¡œë“œ/ê´€ë¦¬
â””â”€â”€ ì‚¬ìš©ì ì¸ì¦/ê¶Œí•œ

4ì£¼ì°¨: Frontend ê°œë°œ
â”œâ”€â”€ SvelteKit ì±„íŒ… UI
â”œâ”€â”€ íŒŒì¼ ì—…ë¡œë“œ ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ Backend API ì—°ë™
â””â”€â”€ ì‚¬ìš©ì ê²½í—˜ ê°œì„ 

5ì£¼ì°¨: í†µí•© í…ŒìŠ¤íŠ¸ & ìµœì í™”
â”œâ”€â”€ ì „ì²´ ì‹œìŠ¤í…œ ì—°ë™ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ ì„±ëŠ¥ ìµœì í™” (ì„ë² ë”©, ê²€ìƒ‰ ì†ë„)
â”œâ”€â”€ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ëª¨ë‹ˆí„°ë§
â””â”€â”€ ë°°í¬ ì¤€ë¹„
```

**í•µì‹¬ í¬ì¸íŠ¸:**

- **ì„ë² ë”© ëª¨ë¸ = Agent ê°œë°œì˜ í•µì‹¬ ë¶€ë¶„**
- **Agentê°€ ëª¨ë“  AI ë¡œì§ì„ ë‹´ë‹¹** (LLM + ì„ë² ë”© + ë²¡í„°ê²€ìƒ‰)
- **BackendëŠ” ë‹¨ìˆœíˆ Agentë¥¼ í˜¸ì¶œí•˜ëŠ” ì—­í• **
- **FrontendëŠ” ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ë§Œ ë‹´ë‹¹**

## 3. RAGì˜ í•µì‹¬ ê°œë…

RAG(Retrieval-Augmented Generation)ëŠ” **ê²€ìƒ‰ ì¦ê°• ìƒì„±** ê¸°ìˆ ë¡œ, ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ê²€ìƒ‰(Retrieve)í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ì •í™•í•˜ê³  ë§¥ë½ì— ë§ëŠ” ë‹µë³€ì„ ìƒì„±(Generate)í•˜ëŠ” AI ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

## 3. RAGì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œ

### 3.1 ë¬¸ì„œ ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬ (Document Processing)

**ë©€í‹°ëª¨ë‹¬(Multimodal)ì´ë€?**

ë©€í‹°ëª¨ë‹¬ì€ **ì—¬ëŸ¬ ê°€ì§€ í˜•íƒœì˜ ë°ì´í„°ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” ëŠ¥ë ¥**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

```
ëª¨ë‹¬(Modal) = ì •ë³´ ì „ë‹¬ ë°©ì‹ì˜ ì¢…ë¥˜
- í…ìŠ¤íŠ¸ ëª¨ë‹¬: ê¸€ì, ë¬¸ì¥ìœ¼ë¡œ ëœ ì •ë³´
- ì´ë¯¸ì§€ ëª¨ë‹¬: ì‚¬ì§„, ê·¸ë¦¼, ë„ë©´ìœ¼ë¡œ ëœ ì •ë³´
- ìŒì„± ëª¨ë‹¬: ì†Œë¦¬ë¡œ ì „ë‹¬ë˜ëŠ” ì •ë³´
- ë¹„ë””ì˜¤ ëª¨ë‹¬: ì›€ì§ì´ëŠ” ì˜ìƒ ì •ë³´

ë©€í‹°ëª¨ë‹¬ = ì´ëŸ° ì—¬ëŸ¬ ë°©ì‹ì„ í•¨ê»˜ ì²˜ë¦¬
```

**ì¼ìƒ ìƒí™œì˜ ë©€í‹°ëª¨ë‹¬ ì˜ˆì‹œ:**

- ğŸ“± **ìŠ¤ë§ˆíŠ¸í°**: í„°ì¹˜(ì†ê°€ë½) + ìŒì„±(ì‹œë¦¬) + í™”ë©´(ì‹œê°)ì„ í•¨ê»˜ ì‚¬ìš©
- ğŸ¥ **ìœ íŠœë¸Œ**: ì˜ìƒ + ìŒì„± + ìë§‰ì„ ë™ì‹œì— ì²˜ë¦¬
- ğŸ—ºï¸ **ë‚´ë¹„ê²Œì´ì…˜**: ì§€ë„ + ìŒì„± ì•ˆë‚´ + GPS ë°ì´í„° í†µí•©

**ê¸°ì¡´ AI vs ë©€í‹°ëª¨ë‹¬ AI:**

```
ê¸°ì¡´ AI (ë‹¨ì¼ ëª¨ë‹¬):
í…ìŠ¤íŠ¸ AI â†’ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
ì´ë¯¸ì§€ AI â†’ ì´ë¯¸ì§€ë§Œ ì²˜ë¦¬
ìŒì„± AI â†’ ìŒì„±ë§Œ ì²˜ë¦¬

ë©€í‹°ëª¨ë‹¬ AI:
í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ì´í•´
â†’ "ì´ ë„ë©´ì—ì„œ ë³´ì¼ëŸ¬ì‹¤ í¬ê¸°ëŠ”?"
   (ì´ë¯¸ì§€ ì† ì¹˜ìˆ˜ + í…ìŠ¤íŠ¸ ì„¤ëª… í†µí•© ë¶„ì„)
```

**íšŒì‚¬ RAG ì‹œìŠ¤í…œì˜ ë©€í‹°ëª¨ë‹¬ í•„ìš”ì„±:**

```
ê¸°ì¡´ ë°©ì‹: PDF â†’ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ â†’ ì œí•œì  ë‹µë³€
ë©€í‹°ëª¨ë‹¬: PDF â†’ í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ë„ë©´ + í‘œ ëª¨ë‘ í™œìš© â†’ ì™„ì „í•œ ë‹µë³€
```

- **ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›**: PDF, Word, PowerPoint, ì´ë¯¸ì§€ (JPG, PNG), CAD íŒŒì¼
- **ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + í‘œ/ì°¨íŠ¸ + ë„ë©´ í†µí•© ì²˜ë¦¬
- **OCR ë° ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ**: ì €í™”ì§ˆ ë„ë©´ ë° ìŠ¤ìº” ë¬¸ì„œ ì²˜ë¦¬
- **ë©”íƒ€ë°ì´í„° ì¶”ì¶œ**: ì œëª©, ì €ì, ë‚ ì§œ, ì¹´í…Œê³ ë¦¬, ë„ë©´ ë²ˆí˜¸

### 3.2 ê²€ìƒ‰ (Retrieval)

- **Vector Database**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
- **Multimodal Embedding**: í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ í†µí•© ì„ë² ë”© ëª¨ë¸
- **Hybrid Search**: í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ + ì´ë¯¸ì§€ ìœ ì‚¬ë„ + ë©”íƒ€ë°ì´í„° í•„í„°ë§

### 3.3 ì¦ê°• (Augmentation)

- **Context Enhancement**: ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
- **Image Description**: ì´ë¯¸ì§€ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ë³´ê°•
- **Multi-modal Prompt Engineering**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” í”„ë¡¬í”„íŠ¸ êµ¬ì„±

### 2.4 ìƒì„± (Generation)

- **Multimodal LLM**: GPT-4V, Claude 3, Gemini Pro Vision ë“± ì´ë¯¸ì§€ë„ ì²˜ë¦¬ ê°€ëŠ¥í•œ ëª¨ë¸
- **Response Optimization**: í…ìŠ¤íŠ¸ ë‹µë³€ + ê´€ë ¨ ì´ë¯¸ì§€/ë„ë©´ ì°¸ì¡° ì œê³µ

## 3. RAG êµ¬í˜„ì„ ìœ„í•œ í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

### 3.1 ê²€ìƒ‰ (Retrieval) êµ¬í˜„ ê¸°ìˆ 

#### 3.1.1 ë²¡í„° ì„ë² ë”© ê¸°ìˆ 

**ì™œ ë²¡í„°ê°€ í•„ìš”í•œê°€?**

```
ë¬¸ì œ: ì»´í“¨í„°ëŠ” "ì‚¬ê³¼"ì™€ "apple"ì´ ê°™ì€ ì˜ë¯¸ì¸ì§€ ì•Œ ìˆ˜ ì—†ìŒ
í•´ê²°: ì˜ë¯¸ë¥¼ ìˆ˜ì¹˜(ë²¡í„°)ë¡œ ë³€í™˜í•˜ì—¬ ìœ ì‚¬ë„ ê³„ì‚° ê°€ëŠ¥

"ì‚¬ê³¼" â†’ [0.2, 0.8, 0.1, 0.4, ...]
"apple" â†’ [0.3, 0.7, 0.2, 0.5, ...]  # ìœ ì‚¬í•œ ë²¡í„°
"ìë™ì°¨" â†’ [0.9, 0.1, 0.8, 0.2, ...]  # ë‹¤ë¥¸ ë²¡í„°
```

**ì„ë² ë”© ëª¨ë¸ ê¸°ìˆ  ìš”êµ¬ì‚¬í•­**

```python
# ì´ ì½”ë“œëŠ” RAG ì‹œìŠ¤í…œì—ì„œ ì„ë² ë”© ëª¨ë¸ì´ ì§ë©´í•˜ëŠ” ì£¼ìš” ê¸°ìˆ ì  ê³¼ì œë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤
# ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ì„ë² ë”© ëª¨ë¸ì„ ì„ íƒí•  ë•Œ ê³ ë ¤í•´ì•¼ í•  í•µì‹¬ ìš”ì†Œë“¤

class EmbeddingChallenges:
    """ì„ë² ë”© ëª¨ë¸ ì„ íƒ ì‹œ ê³ ë ¤í•´ì•¼ í•  ê¸°ìˆ ì  ë„ì „ê³¼ì œì™€ ìš”êµ¬ì‚¬í•­ ì •ì˜"""
    def __init__(self):
        # RAG ì‹œìŠ¤í…œì—ì„œ ì„ë² ë”© ëª¨ë¸ì´ ë°˜ë“œì‹œ í•´ê²°í•´ì•¼ í•˜ëŠ” í•µì‹¬ ê³¼ì œë“¤
        self.challenges = {
            "ë‹¤êµ­ì–´ ì§€ì›": "í•œêµ­ì–´, ì˜ì–´, ì¼ë³¸ì–´ ë“±ì„ ê°™ì€ ë²¡í„° ê³µê°„ì— ë§¤í•‘ (ê¸€ë¡œë²Œ ì„œë¹„ìŠ¤ í•„ìˆ˜)",
            "ë„ë©”ì¸ ì ì‘": "ì˜ë£Œ, ë²•ë¥ , ê¸°ìˆ  ë¬¸ì„œì˜ ì „ë¬¸ ìš©ì–´ë¥¼ ì •í™•íˆ ì´í•´ (íŒŒì¸íŠœë‹ ê³ ë ¤)",
            "ë¬¸ë§¥ ì´í•´": "ê°™ì€ ë‹¨ì–´ë„ ë¬¸ë§¥ì— ë”°ë¼ ë‹¤ë¥¸ ì˜ë¯¸ë¡œ ì²˜ë¦¬ (ë™ìŒì´ì˜ì–´ í•´ê²°)",
            "í™•ì¥ì„±": "ìˆ˜ë°±ë§Œ ë¬¸ì„œë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ (ë°°ì¹˜ ì²˜ë¦¬, GPU ìµœì í™”)"
        }

    def technical_requirements(self):
        return {
            "ëª¨ë¸ ì•„í‚¤í…ì²˜": ["BERT", "Sentence-BERT", "E5", "BGE", "OpenAI Ada"],
            "ë²¡í„° ì°¨ì›": "384~1536ì°¨ì› (ì •í™•ë„ vs ì†ë„ íŠ¸ë ˆì´ë“œì˜¤í”„)",
            "ì²˜ë¦¬ ì†ë„": "ë¬¸ì„œ 1ê°œë‹¹ 10~100ms ì´í•˜",
            "ë©”ëª¨ë¦¬ íš¨ìœ¨": "GPU ë©”ëª¨ë¦¬ ìµœì í™” (ë°°ì¹˜ ì²˜ë¦¬, ì–‘ìí™”)"
        }
```

**êµ¬í˜„ ê¸°ìˆ  ìŠ¤íƒ**

```python
# ì´ ì½”ë“œëŠ” í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì„ë² ë”© ì„œë¹„ìŠ¤ì˜ êµ¬ì¡°ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤
# ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ë„ë©”ì¸ì— ë§ëŠ” ëª¨ë¸ì„ ì„ íƒí•˜ê³ , ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„

# ì„ë² ë”© íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ì˜ˆì‹œ
from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Dict

class ProductionEmbeddingService:
    """ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì¤‘ ëª¨ë‹¬ ì„ë² ë”© ìƒì„± ì„œë¹„ìŠ¤"""
    def __init__(self):
        # ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ ì˜µì…˜
        self.text_models = {
            "multilingual": "paraphrase-multilingual-MiniLM-L12-v2",  # 50+ ì–¸ì–´ ì§€ì›
            "korean": "jhgan/ko-sroberta-multitask",  # í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸
            "english": "all-MiniLM-L6-v2",  # ì˜ì–´ ìµœì í™” ê²½ëŸ‰ ëª¨ë¸
            "openai": "text-embedding-ada-002",  # OpenAI API ì‚¬ìš©
            "google": "textembedding-gecko@001"  # Google Vertex AI ì„ë² ë”© (Geminiì™€ í˜¸í™˜ì„± ìš°ìˆ˜)
        }

        # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ í†µí•©)
        self.multimodal_models = {
            "clip": "openai/clip-vit-base-patch32",  # í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ í†µí•©
            "layoutlm": "microsoft/layoutlm-base-uncased",  # ë¬¸ì„œ ë ˆì´ì•„ì›ƒ ì´í•´
            "blip": "Salesforce/blip-image-captioning-base"  # ì´ë¯¸ì§€ ìºí”„ì…”ë‹
        }

        # OCR ì—”ì§„ (ì €í™”ì§ˆ ë„ë©´ ì²˜ë¦¬ìš©)
        self.ocr_engines = {
            "tesseract": "pytesseract",  # ì˜¤í”ˆì†ŒìŠ¤, ê¸°ë³¸ OCR
            "paddleocr": "paddlepaddle/paddleocr",  # ì„±ëŠ¥ ì¢‹ìŒ, ë‹¤êµ­ì–´ ì§€ì›
            "aws_textract": "boto3",  # AWS ê´€ë¦¬í˜•, ë†’ì€ ì •í™•ë„
            "easyocr": "easyocr"  # ì‚¬ìš© ê°„í¸, ì ë‹¹í•œ ì„±ëŠ¥
        }

        # ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
        self.batch_size = 32  # GPU ë©”ëª¨ë¦¬ì™€ ì²˜ë¦¬ ì†ë„ì˜ ê· í˜•ì 
        self.max_length = 512  # í† í° ê¸¸ì´ ì œí•œìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œì–´

    def process_multimodal_document(self, file_path: str, file_type: str) -> Dict:
        """ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ - PDF, ì´ë¯¸ì§€, CAD íŒŒì¼ ë“±"""

        if file_type.lower() == 'pdf':
            return self._process_pdf_with_images(file_path)
        elif file_type.lower() in ['jpg', 'jpeg', 'png', 'tiff']:
            return self._process_image_document(file_path)
        elif file_type.lower() in ['dwg', 'dxf']:  # CAD íŒŒì¼
            return self._process_cad_document(file_path)
        else:
            return self._process_text_document(file_path)

    def _process_pdf_with_images(self, pdf_path: str) -> Dict:
        """ì´ë¯¸ì§€ê°€ í¬í•¨ëœ PDF ë¬¸ì„œ ì²˜ë¦¬"""
        import fitz  # PyMuPDF

        doc = fitz.open(pdf_path)
        results = {
            "text_content": [],
            "images": [],
            "mixed_embeddings": []
        }

        for page_num in range(len(doc)):
            page = doc.load_page(page_num)

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = page.get_text()
            if text.strip():
                results["text_content"].append({
                    "page": page_num + 1,
                    "text": text,
                    "embedding": self._embed_text(text)
                })

            # ì´ë¯¸ì§€ ì¶”ì¶œ ë° OCR ì²˜ë¦¬
            image_list = page.get_images()
            for img_index, img in enumerate(image_list):
                image_data = self._extract_image_from_pdf(doc, img)

                # ì €í™”ì§ˆ ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„  ì‹œë„
                enhanced_image = self._enhance_image_quality(image_data)

                # ë‹¤ì¤‘ OCR ì—”ì§„ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                ocr_results = self._multi_ocr_extraction(enhanced_image)

                # ì´ë¯¸ì§€ ìºí”„ì…˜ ìƒì„±
                image_caption = self._generate_image_caption(enhanced_image)

                results["images"].append({
                    "page": page_num + 1,
                    "image_index": img_index,
                    "ocr_text": ocr_results["best_result"],
                    "caption": image_caption,
                    "confidence": ocr_results["confidence"],
                    "embedding": self._embed_multimodal(ocr_results["best_result"], image_caption)
                })

        return results    def load_model(self, model_type: str):
        """ëª¨ë¸ ë¡œë“œ ë° ìµœì í™”"""
        if model_type == "openai":
            import openai
            return openai
        else:
            model = SentenceTransformer(self.models[model_type])
            # GPU ì‚¬ìš© ê°€ëŠ¥ì‹œ GPUë¡œ ì´ë™
            if torch.cuda.is_available():
                model = model.to('cuda')
            return model

    def embed_documents(self, texts: List[str], model_type: str = "multilingual") -> np.ndarray:
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë¬¸ì„œë“¤ì„ ë²¡í„°í™”"""
        model = self.load_model(model_type)

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        processed_texts = [self.preprocess_text(text) for text in texts]

        # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        embeddings = []
        for i in range(0, len(processed_texts), self.batch_size):
            batch = processed_texts[i:i + self.batch_size]
            batch_embeddings = model.encode(
                batch,
                batch_size=self.batch_size,
                show_progress_bar=True,
                convert_to_numpy=True,
                normalize_embeddings=True  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìµœì í™”
            )
            embeddings.append(batch_embeddings)

        return np.vstack(embeddings)

    def preprocess_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ê¸¸ì´ ì œí•œ
        if len(text) > self.max_length:
            text = text[:self.max_length]

        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text).strip()


    def _multi_ocr_extraction(self, image_data) -> Dict:
        """ì—¬ëŸ¬ OCR ì—”ì§„ì„ ì‚¬ìš©í•˜ì—¬ ìµœì  ê²°ê³¼ ì„ íƒ"""
        ocr_results = {}

        try:
            # Tesseract OCR
            import pytesseract
            tesseract_result = pytesseract.image_to_string(
                image_data,
                config='--psm 6 --oem 3'
            )
            ocr_results["tesseract"] = {
                "text": tesseract_result,
                "confidence": self._calculate_text_confidence(tesseract_result)
            }
        except Exception as e:
            ocr_results["tesseract"] = {"text": "", "confidence": 0}

        try:
            # PaddleOCR (ì ì€ ë…¸ì´ì¦ˆì— ê°•í•¨)
            from paddleocr import PaddleOCR
            paddle_ocr = PaddleOCR(use_angle_cls=True, lang='en')
            paddle_result = paddle_ocr.ocr(image_data)

            combined_text = ""
            total_confidence = 0
            for line in paddle_result:
                for word_info in line:
                    combined_text += word_info[1][0] + " "
                    total_confidence += word_info[1][1]

            ocr_results["paddleocr"] = {
                "text": combined_text.strip(),
                "confidence": total_confidence / len(paddle_result) if paddle_result else 0
            }
        except Exception as e:
            ocr_results["paddleocr"] = {"text": "", "confidence": 0}

        # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ê²°ê³¼ ì„ íƒ
        best_engine = max(ocr_results, key=lambda x: ocr_results[x]["confidence"])

        return {
            "best_result": ocr_results[best_engine]["text"],
            "confidence": ocr_results[best_engine]["confidence"],
            "all_results": ocr_results,
            "best_engine": best_engine
        }

    def _enhance_image_quality(self, image_data):
        """ì €í™”ì§ˆ ë„ë©´ ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„ """
        import cv2
        import numpy as np

        # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        if len(image_data.shape) == 3:
            gray = cv2.cvtColor(image_data, cv2.COLOR_BGR2GRAY)
        else:
            gray = image_data

        # ë…¸ì´ì¦ˆ ì œê±°
        denoised = cv2.fastNlMeansDenoising(gray)

        # ì´ë¯¸ì§€ ì„ ëª…í™” (ì–¸ìƒ¤í”„ ë§ˆìŠ¤í¬ ì‚¬ìš©)
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(denoised, -1, kernel)

        # ì»´íŠ¸ë˜ìŠ¤íŠ¸ í–¥ìƒ (CLAHE - Contrast Limited Adaptive Histogram Equalization)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        enhanced = clahe.apply(sharpened)

        # ì´ì§„í™” (í…ìŠ¤íŠ¸ ì¶”ì¶œ í–¥ìƒì„ ìœ„í•´)
        _, binary = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

        return binary

    def _calculate_text_confidence(self, text: str) -> float:
        """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆ í‰ê°€"""
        if not text or len(text.strip()) < 3:
            return 0.0

        # ì˜ì–´ ë‹¨ì–´ì™€ ìˆ«ìì˜ ë¹„ìœ¨
        import re
        words = re.findall(r'\b[a-zA-Z]+\b', text)
        numbers = re.findall(r'\d+', text)
        special_chars = len(re.findall(r'[^\w\s]', text))

        total_chars = len(text)
        if total_chars == 0:
            return 0.0

        # ì ìˆ˜ ê³„ì‚°
        word_score = len(words) * 0.4
        number_score = len(numbers) * 0.3

        # íŠ¹ìˆ˜ë¬¸ìê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ê°ì  (OCR ì˜¤ë¥˜ ê°€ëŠ¥ì„±)
        noise_penalty = min(special_chars / total_chars, 0.3)

        confidence = min((word_score + number_score) / total_chars - noise_penalty, 1.0)
        return max(confidence, 0.0)
```

#### 3.1.2 ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê¸°ìˆ 

**ì™œ ì¼ë°˜ DBê°€ ì•„ë‹Œ ë²¡í„° DBê°€ í•„ìš”í•œê°€?**

```sql
-- ì´ SQL ì˜ˆì‹œëŠ” ê¸°ì¡´ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ì™€ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ ê·¼ë³¸ì  ì°¨ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤
-- ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ì€ ê¸°ì¡´ DBë¡œëŠ” íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” ìƒˆë¡œìš´ ê²€ìƒ‰ íŒ¨ëŸ¬ë‹¤ì„

-- ê¸°ì¡´ SQLë¡œëŠ” ë¶ˆê°€ëŠ¥í•œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
SELECT * FROM documents
WHERE similarity(embedding, query_embedding) > 0.8
-- âŒ ì¼ë°˜ RDBMSëŠ” ê³ ì°¨ì› ë²¡í„° ê°„ ìœ ì‚¬ë„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ì—†ìŒ
--    ì¸ë±ìŠ¤ë„ ì—†ê³ , ìˆ˜í•™ì  ê±°ë¦¬ ê³„ì‚°ë„ ìµœì í™”ë˜ì§€ ì•ŠìŒ

-- ë²¡í„° DBì—ì„œëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ê°€ëŠ¥í•œ ê²€ìƒ‰
SELECT * FROM documents
ORDER BY embedding <-> query_embedding  -- <-> ëŠ” ì½”ì‚¬ì¸ ê±°ë¦¬ ì—°ì‚°ì
LIMIT 10
-- âœ… HNSW, IVF ë“± ì „ìš© ì¸ë±ìŠ¤ë¡œ ë°€ë¦¬ì„¸ì»¨ë“œ ë‚´ k-NN ê²€ìƒ‰ ê°€ëŠ¥
```

**ë²¡í„° DB ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­**

```python
# ì´ ì½”ë“œëŠ” ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ì‹œ ê³ ë ¤í•´ì•¼ í•  í•µì‹¬ ê¸°ìˆ  ìš”ì†Œë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤
# ì‹¤ì œ í”„ë¡œë•ì…˜ì—ì„œ ì„±ëŠ¥ê³¼ ì •í™•ë„ë¥¼ ë³´ì¥í•˜ê¸° ìœ„í•œ í•„ìˆ˜ ì²´í¬í¬ì¸íŠ¸

class VectorDBRequirements:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì˜ í•µì‹¬ ê¸°ìˆ  ìš”êµ¬ì‚¬í•­ê³¼ ì„±ëŠ¥ ì§€í‘œ ì •ì˜"""
    def __init__(self):
        # ë²¡í„° ê²€ìƒ‰ì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ë“¤ - ê°ê° ë‹¤ë¥¸ ìš©ë„ì™€ ì„±ëŠ¥ íŠ¹ì„±
        self.core_algorithms = {
            "ì¸ë±ì‹±": {
                "HNSW": "ê°€ì¥ ë„ë¦¬ ì‚¬ìš©, ë†’ì€ ì •í™•ë„ì™€ ì†ë„ (Qdrant, Weaviate ë“± ì±„íƒ)",
                "LSH": "í•´ì‹œ ê¸°ë°˜, ì´ˆëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ì í•©í•˜ì§€ë§Œ ì •í™•ë„ ë‹¤ì†Œ ë‚®ìŒ",
                "IVF": "í´ëŸ¬ìŠ¤í„° ê¸°ë°˜, ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ì§€ë§Œ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì¡°ì • í•„ìš”",
                "Annoy": "íŠ¸ë¦¬ ê¸°ë°˜, ì½ê¸° ì „ìš© ì›Œí¬ë¡œë“œì— ìµœì í™”"
            },
            # ë²¡í„° ê°„ ê±°ë¦¬ ì¸¡ì • ë°©ë²• - ë°ì´í„° íƒ€ì…ì— ë”°ë¼ ì„ íƒ
            "ê±°ë¦¬_ì¸¡ì •": {
                "Cosine": "í…ìŠ¤íŠ¸ ì„ë² ë”©ì— ìµœì  (ë°©í–¥ ì¤‘ì‹œ, í¬ê¸° ë¬´ì‹œ)",
                "Euclidean": "ì´ë¯¸ì§€, ìŒì„± ë°ì´í„°ì— ì í•© (ì ˆëŒ€ ê±°ë¦¬)",
                "Dot Product": "ì •ê·œí™”ëœ ë²¡í„°ì—ì„œ ë¹ ë¥¸ ì—°ì‚°",
                "Manhattan": "ê³ ì°¨ì› í¬ì†Œ ë²¡í„°ì—ì„œ ë…¸ì´ì¦ˆì— ê°•í•¨"
            }
        }

    def performance_requirements(self):
        return {
            "ì‘ë‹µì†ë„": {
                "ëª©í‘œ": "99%ile < 100ms",
                "ì˜í–¥ìš”ì¸": ["ì¸ë±ìŠ¤ íƒ€ì…", "ë²¡í„° ì°¨ì›", "ë°ì´í„° í¬ê¸°", "í•˜ë“œì›¨ì–´"]
            },
            "ì •í™•ë„": {
                "ëª©í‘œ": "Recall@10 > 95%",
                "ì¸¡ì •ë²•": "ì‹¤ì œ ìµœê·¼ì ‘ ì´ì›ƒ ëŒ€ë¹„ ê²€ìƒ‰ ê²°ê³¼ ë¹„êµ"
            },
            "í™•ì¥ì„±": {
                "ìˆ˜í‰í™•ì¥": "ìƒ¤ë”©, ë³µì œë¥¼ í†µí•œ ë¶„ì‚° ì²˜ë¦¬",
                "ìˆ˜ì§í™•ì¥": "ë©”ëª¨ë¦¬, CPU ì¦ì„¤ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ"
            }
        }
```

**ì‹¤ì œ ë²¡í„° DB êµ¬í˜„ ì˜ˆì‹œ**

```python
# Qdrant ë²¡í„° DB êµ¬í˜„
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

class ProductionVectorDB:
    def __init__(self, host="localhost", port=6333):
        self.client = QdrantClient(host=host, port=port, prefer_grpc=True)
        self.collection_name = "documents"

    def setup_collection(self, vector_size=768):
        """ì»¬ë ‰ì…˜ ìƒì„± ë° ìµœì í™”"""
        # ì»¬ë ‰ì…˜ ìƒì„±
        self.client.create_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(
                size=vector_size,
                distance=Distance.COSINE,  # í…ìŠ¤íŠ¸ì— ìµœì 
                hnsw_config={
                    "m": 16,  # ì—°ê²°ì„± vs ë©”ëª¨ë¦¬ íŠ¸ë ˆì´ë“œì˜¤í”„
                    "ef_construct": 100,  # êµ¬ì¶• ì‹œ ì •í™•ë„
                }
            ),
            optimizers_config={
                "default_segment_number": 2,  # ë³‘ë ¬ ì²˜ë¦¬
                "indexing_threshold": 20000,  # ì¸ë±ì‹± íŠ¸ë¦¬ê±°
            }
        )

    def insert_documents(self, documents: List[Dict]):
        """ëŒ€ëŸ‰ ë¬¸ì„œ ì‚½ì… (ë°°ì¹˜ ìµœì í™”)"""
        points = []
        for doc in documents:
            points.append(PointStruct(
                id=doc["id"],
                vector=doc["embedding"],
                payload={
                    "content": doc["content"],
                    "metadata": doc["metadata"]
                }
            ))

        # ë°°ì¹˜ ì‚½ì… (ì„±ëŠ¥ ìµœì í™”)
        self.client.upsert(
            collection_name=self.collection_name,
            points=points
        )

    def search_similar(self, query_vector: List[float], top_k=10, filter_conditions=None):
        """ìœ ì‚¬ë„ ê²€ìƒ‰"""
        search_result = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            query_filter=filter_conditions,
            limit=top_k,
            score_threshold=0.7,  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
            with_payload=True
        )

        return [
            {
                "id": hit.id,
                "score": hit.score,
                "content": hit.payload["content"],
                "metadata": hit.payload["metadata"]
            }
            for hit in search_result
        ]
```

### 3.2 ì¦ê°• (Augmentation) êµ¬í˜„ ê¸°ìˆ 

#### 3.2.1 ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ê¸°ìˆ 

**ì™œ ë‹¨ìˆœíˆ ë¬¸ì„œë¥¼ ë¶™ì´ë©´ ì•ˆ ë˜ëŠ”ê°€?**

```python
# âŒ ì˜ëª»ëœ ë°©ì‹ - ë‹¨ìˆœ ì—°ê²°
def naive_context_building(query, retrieved_docs):
    context = "\n".join([doc["content"] for doc in retrieved_docs])
    prompt = f"Context: {context}\nQuestion: {query}\nAnswer:"
    return prompt  # ë¬¸ì œ: í† í° ì œí•œ, ë…¸ì´ì¦ˆ, ìˆœì„œ ë¬´ì‹œ

# âœ… ì˜¬ë°”ë¥¸ ë°©ì‹ - ì§€ëŠ¥ì  êµ¬ì„±
def intelligent_context_building(query, retrieved_docs):
    # 1. ê´€ë ¨ì„± ê¸°ë°˜ ì¬ìˆœì„œ
    reranked_docs = rerank_by_relevance(query, retrieved_docs)

    # 2. ì¤‘ë³µ ì œê±°
    deduplicated_docs = remove_semantic_duplicates(reranked_docs)

    # 3. í† í° ì œí•œ ë‚´ì—ì„œ ìµœì  ì„ íƒ
    selected_docs = select_within_token_limit(deduplicated_docs, max_tokens=2000)

    # 4. êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    return build_structured_prompt(query, selected_docs)
```

**ê³ ê¸‰ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ê¸°ìˆ **

```python
# ì´ ì½”ë“œëŠ” ë‹¨ìˆœíˆ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ë‚˜ì—´í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼,
# ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì„ ë‹¤ì‹œ í‰ê°€í•˜ê³  ì¤‘ë³µì„ ì œê±°í•˜ì—¬ ìµœì ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ” ê³ ê¸‰ ê¸°ë²•

class AdvancedContextBuilder:
    """ì§ˆë¬¸ê³¼ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
    def __init__(self):
        self.reranker = CrossEncoder('ms-marco-MiniLM-L-6-v2')  # ì§ˆë¬¸-ë¬¸ì„œ ê°„ ê´€ë ¨ì„± ë‹¤ì‹œ í‰ê°€
        self.max_tokens = 4000  # LLM ì»¨í…ìŠ¤íŠ¸ ì°½ í¬ê¸° ì œí•œ
        self.overlap_threshold = 0.8  # ì˜ë¯¸ì  ì¤‘ë³µ íŒë‹¨ ì„ê³„ê°’

    def build_context(self, query: str, retrieved_docs: List[Dict]) -> str:
        """ë‹¨ê³„ë³„ ì§€ëŠ¥ì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± í”„ë¡œì„¸ìŠ¤"""

        # 1ë‹¨ê³„: ì˜ë¯¸ì  ì¬ìˆœìœ„ (ë²¡í„° ê²€ìƒ‰ë³´ë‹¤ ë” ì •ë°€í•œ Cross-Encoder ì‚¬ìš©)
        query_doc_pairs = [[query, doc["content"]] for doc in retrieved_docs]
        rerank_scores = self.reranker.predict(query_doc_pairs)

        # ë†’ì€ ê´€ë ¨ì„± ìˆœì„œë¡œ ì •ë ¬
        scored_docs = list(zip(retrieved_docs, rerank_scores))
        scored_docs.sort(key=lambda x: x[1], reverse=True)

        # 2ë‹¨ê³„: ì˜ë¯¸ì  ì¤‘ë³µ ì œê±° (ë¹„ìŠ·í•œ ë‚´ìš© ë¬¸ì„œë“¤ í•„í„°ë§)
        final_docs = []
        for doc, score in scored_docs:
            if not self._is_semantic_duplicate(doc, final_docs):
                final_docs.append((doc, score))

        # 3ë‹¨ê³„: í† í° ì œí•œ ë‚´ì—ì„œ ìµœì  ì„ íƒ (ê¸´ ë¬¸ì„œë“¤ì„ ìš°ì„ ìˆœìœ„ì™€ ë‚´ìš© ê¸°ì—¬ë„ë¡œ ì„ ë³„)
        selected_docs = self._select_within_token_limit(final_docs)

        # 4ë‹¨ê³„: ì‚¬ìš©ì ì¹œí™”ì ì´ê³  LLMì´ ì´í•´í•˜ê¸° ì‰¬ìš´ êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        return self._build_structured_context(query, selected_docs)    def _is_semantic_duplicate(self, doc, existing_docs, threshold=0.8):
        """ì˜ë¯¸ì  ì¤‘ë³µ ê²€ì‚¬"""
        doc_embedding = self.embedder.encode(doc["content"])

        for existing_doc, _ in existing_docs:
            existing_embedding = self.embedder.encode(existing_doc["content"])
            similarity = cosine_similarity([doc_embedding], [existing_embedding])[0][0]

            if similarity > threshold:
                return True
        return False

    def _build_structured_context(self, query: str, docs: List[Tuple]) -> str:
        """êµ¬ì¡°í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        context_parts = []

        # ë©”íƒ€ë°ì´í„° í¬í•¨ ì»¨í…ìŠ¤íŠ¸
        for i, (doc, score) in enumerate(docs, 1):
            context_parts.append(f"""
ë¬¸ì„œ {i} (ê´€ë ¨ë„: {score:.3f}):
ì¶œì²˜: {doc.get('source', 'Unknown')}
ì œëª©: {doc.get('title', 'Untitled')}
ë‚´ìš©: {doc['content']}
""")

        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        return f"""
ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹µë³€ ì‹œ ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œë“¤:
{''.join(context_parts)}

ì§ˆë¬¸: {query}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ì œê³µëœ ë¬¸ì„œì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
3. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
4. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ë¼ë©´ ì†”ì§íˆ "ì œê³µëœ ë¬¸ì„œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”

ë‹µë³€:
"""
```

#### 3.2.2 í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ìˆ 

**RAG ì „ìš© í”„ë¡¬í”„íŠ¸ íŒ¨í„´**

```python
# ì´ ì½”ë“œëŠ” ë‹¤ì–‘í•œ ìƒí™©ì— ë§ëŠ” RAG í”„ë¡¬í”„íŠ¸ íŒ¨í„´ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤
# ê° íŒ¨í„´ì€ íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, LLMì´ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ë„ë¡ ìœ ë„

class RAGPromptPatterns:
    """ì‚¬ìš© ëª©ì ì— ë”°ë¼ ìµœì í™”ëœ ë‹¤ì–‘í•œ RAG í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ëª¨ìŒ"""
    def __init__(self):
        # ê° íŒ¨í„´ì˜ ì‚¬ìš© ëª©ì ê³¼ íŠ¹ì§•ì„ ë§¤í•‘
        self.patterns = {
            "basic_rag": self.basic_rag_pattern,  # ê¸°ë³¸ì ì¸ ì§ˆì˜ì‘ë‹µìš©
            "chain_of_thought": self.cot_pattern,  # ë³µì¡í•œ ì¶”ë¡ ì´ í•„ìš”í•œ ê²½ìš°
            "structured_output": self.structured_pattern,  # JSON ë“± êµ¬ì¡°í™”ëœ ì¶œë ¥ í•„ìš”ì‹œ
            "multi_document": self.multi_doc_pattern,  # ì—¬ëŸ¬ ë¬¸ì„œ ê°„ ë¹„êµ ë° ì¢…í•© ë¶„ì„
            "multimodal_rag": self.multimodal_pattern,  # í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ í†µí•© ì²˜ë¦¬
            "technical_drawing": self.technical_drawing_pattern  # ë„ë©´ í•´ì„ ì „ìš©
        }

    def basic_rag_pattern(self, query, context):
        """ê°€ì¥ ê¸°ë³¸ì ì¸ RAG íŒ¨í„´ - ëŒ€ë¶€ë¶„ì˜ ì¼ë°˜ì  ì§ˆë¬¸ì— ì í•©"""
        return f"""
ë‹¹ì‹ ì€ ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {query}

ê·œì¹™:
- ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œì˜ ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
- ë‹µë³€ì˜ ê·¼ê±°ë¥¼ ëª…ì‹œí•˜ì„¸ìš”

ë‹µë³€:
"""

    def cot_pattern(self, query, context):
        """ì‚¬ê³  ê³¼ì •ì„ í¬í•¨í•œ íŒ¨í„´ (ë³µì¡í•œ ì¶”ë¡ ìš©)"""
        return f"""
ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ë‹¨ê³„ë³„ë¡œ ì¶”ë¡ í•´ì„œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {query}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. ë¬¸ì„œ ë¶„ì„:
   - ê´€ë ¨ ì •ë³´ ìš”ì•½
   - í•µì‹¬ ë°ì´í„° ì¶”ì¶œ

2. ì¶”ë¡  ê³¼ì •:
   - ë…¼ë¦¬ì  ì—°ê²°ê³ ë¦¬
   - ë‹¨ê³„ë³„ ë¶„ì„

3. ìµœì¢… ë‹µë³€:
   - ê²°ë¡ 
   - ê·¼ê±° ë¬¸ì„œ ëª…ì‹œ

ë‹µë³€:
"""

    def structured_pattern(self, query, context):
        """êµ¬ì¡°í™”ëœ ì¶œë ¥ìš© íŒ¨í„´"""
        return f"""
ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {query}

ì¶œë ¥ í˜•ì‹:
{{
  "answer": "ë‹µë³€ ë‚´ìš©",
  "confidence": "ë†’ìŒ/ë³´í†µ/ë‚®ìŒ",
  "sources": ["ë¬¸ì„œ1", "ë¬¸ì„œ2"],
  "key_facts": ["ì£¼ìš” ì‚¬ì‹¤1", "ì£¼ìš” ì‚¬ì‹¤2"],
  "limitations": "ë‹µë³€ì˜ í•œê³„ì "
}}

JSON ë‹µë³€:
"""

    def multimodal_pattern(self, query, text_context, image_descriptions):
        """ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ìš© íŒ¨í„´ - í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì •ë³´ í†µí•©"""
        return f"""
ë‹¹ì‹ ì€ í…ìŠ¤íŠ¸ ë¬¸ì„œì™€ ì´ë¯¸ì§€ë¥¼ ëª¨ë‘ ë¶„ì„í•˜ì—¬ ì¢…í•©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

í…ìŠ¤íŠ¸ ë¬¸ì„œ:
{text_context}

ì´ë¯¸ì§€ ì •ë³´:
{image_descriptions}

ì§ˆë¬¸: {query}

ë‹µë³€ ì§€ì¹¨:
1. í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ëª¨ë‘ í™œìš©í•˜ì„¸ìš”
2. ì´ë¯¸ì§€ì—ì„œ ì½ì€ ì •ë³´ì™€ í…ìŠ¤íŠ¸ ì •ë³´ ê°„ ì¼ì¹˜/ë¶ˆì¼ì¹˜ë¥¼ í™•ì¸í•˜ì„¸ìš”
3. ì´ë¯¸ì§€ê°€ ì €í™”ì§ˆì´ì–´ì„œ ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ëª…ì‹œí•˜ì„¸ìš”
4. ë‹µë³€ì— ì°¸ì¡°í•œ ë¬¸ì„œ í˜ì´ì§€ì™€ ì´ë¯¸ì§€ë¥¼ ëª…ì‹œí•˜ì„¸ìš”

ë‹µë³€:
"""

    def technical_drawing_pattern(self, query, drawing_text, drawing_metadata):
        """ê¸°ìˆ  ë„ë©´ í•´ì„ ì „ìš© íŒ¨í„´ - ê±´ì¶•/ê¸°ê³„ ë„ë©´ ë¶„ì„"""
        return f"""
ë‹¹ì‹ ì€ ê¸°ìˆ  ë„ë©´ì„ ì „ë¬¸ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ì—”ì§€ë‹ˆì–´ë§ AIì…ë‹ˆë‹¤.

ë„ë©´ì—ì„œ ì¶”ì¶œëœ ì •ë³´:
{drawing_text}

ë„ë©´ ë©”íƒ€ë°ì´í„°:
{drawing_metadata}

ì§ˆë¬¸: {query}

ë„ë©´ í•´ì„ ì§€ì¹¨:
1. ì¹˜ìˆ˜ì™€ ê³µì°¨ ì •ë³´ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”
2. ê¸°í˜¸ì™€ í‘œì¤€ ê·œê²©ì„ í•´ì„í•˜ì„¸ìš”
3. ë„ë©´ì˜ ìŠ¤ì¼€ì¼ê³¼ ë‹¨ìœ„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”
4. OCRë¡œ ì½ê¸° ì–´ë ¤ìš´ ë¶€ë¶„ì€ "ë¶ˆí™•ì‹¤í•¨"ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”
5. ì•ˆì „ìƒ ì¤‘ìš”í•œ ì •ë³´ëŠ” ë°˜ë“œì‹œ í™•ì¸ì„ ê¶Œí•˜ì„¸ìš”

ì£¼ì˜ì‚¬í•­:
- ë„ë©´ì´ ì €í™”ì§ˆì¼ ê²½ìš° ì¤‘ìš”í•œ ì¹˜ìˆ˜ëŠ” ì›ë³¸ í™•ì¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤
- êµ¬ì¡°ì /ì•ˆì „ ê´€ë ¨ íŒë‹¨ì€ ì „ë¬¸ê°€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤

ë‹µë³€:
"""
```

### 3.3 ìƒì„± (Generation) êµ¬í˜„ ê¸°ìˆ 

#### 3.3.1 LLM ì„ íƒ ë° ìµœì í™”

**RAGìš© LLM ì„ íƒ ê¸°ì¤€**

```python
class LLMSelectionCriteria:
    def __init__(self):
        self.criteria = {
            "ì»¨í…ìŠ¤íŠ¸ ì°½ í¬ê¸°": {
                "ìš”êµ¬ì‚¬í•­": "ê¸´ ë¬¸ì„œë“¤ì„ í¬í•¨í•  ìˆ˜ ìˆëŠ” ì¶©ë¶„í•œ í† í° ìˆ˜",
                "ì˜µì…˜": {
                    "GPT-4 Turbo": "128K í† í°",
                    "Claude-3": "200K í† í°",
                    "Gemini Pro": "32K í† í°",
                    "LLaMA-2": "4K í† í° (í™•ì¥ ê°€ëŠ¥)"
                }
            },
            "instruction_following": {
                "ì¤‘ìš”ë„": "ë§¤ìš° ë†’ìŒ",
                "ì´ìœ ": "ì •í™•í•œ í˜•ì‹ê³¼ ê·œì¹™ ì¤€ìˆ˜ í•„ìš”",
                "ì¸¡ì •ë²•": "ë³µì¡í•œ ì§€ì‹œì‚¬í•­ ë”°ë¥´ê¸° í…ŒìŠ¤íŠ¸"
            },
            "ë¹„ìš©_ëŒ€ë¹„_ì„±ëŠ¥": {
                "ê³ ë ¤ì‚¬í•­": ["API ë¹„ìš©", "ì‘ë‹µ ì†ë„", "í’ˆì§ˆ"],
                "ê³„ì‚°": "ì›”ê°„ ì¿¼ë¦¬ ìˆ˜ Ã— í† í°ë‹¹ ë¹„ìš©"
            }
        }

    def calculate_monthly_cost(self, queries_per_month, avg_tokens_per_query):
        """ì›”ê°„ ìš´ì˜ ë¹„ìš© ê³„ì‚°"""
        costs = {
            "gpt-4-turbo": {"input": 0.01/1000, "output": 0.03/1000},
            "gpt-3.5-turbo": {"input": 0.001/1000, "output": 0.002/1000},
            "claude-3-sonnet": {"input": 0.003/1000, "output": 0.015/1000},
            "gemini-pro": {"input": 0.000125/1000, "output": 0.000375/1000}  # Google Gemini ìš”ê¸ˆ
        }

        results = {}
        for model, pricing in costs.items():
            input_cost = queries_per_month * avg_tokens_per_query * pricing["input"]
            output_cost = queries_per_month * 500 * pricing["output"]  # í‰ê·  500í† í° ì¶œë ¥
            results[model] = {
                "monthly_cost": input_cost + output_cost,
                "cost_per_query": (input_cost + output_cost) / queries_per_month
            }

        return results
```

## 4. LLMë³„ ì„ë² ë”© ëª¨ë¸ í•„ìš”ì„±

### 4.1 Gemini API ì‚¬ìš© ì‹œ ì„ë² ë”© ìš”êµ¬ì‚¬í•­

**Gemini LLMì€ ë³„ë„ ì„ë² ë”© ëª¨ë¸ì´ ë°˜ë“œì‹œ í•„ìš”í•©ë‹ˆë‹¤**

```python
# ì œë¯¸ë‚˜ì´ API ì‚¬ìš© ì‹œ ì„ë² ë”© ëª¨ë¸ í†µí•© ì˜ˆì‹œ
# GeminiëŠ” í…ìŠ¤íŠ¸ ìƒì„±ë§Œ ë‹´ë‹¹í•˜ê³ , ë²¡í„° ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”©ì€ ë³„ë„ ëª¨ë¸ ì‚¬ìš©

class GeminiRAGSystem:
    """Gemini APIì™€ ì™¸ë¶€ ì„ë² ë”© ëª¨ë¸ì„ ê²°í•©í•œ RAG ì‹œìŠ¤í…œ"""
    def __init__(self):
        # GeminiëŠ” í…ìŠ¤íŠ¸ ìƒì„±ìš©
        self.gemini_client = self._init_gemini()

        # ì„ë² ë”©ì€ ë³„ë„ ëª¨ë¸ í•„ìš” (3ê°€ì§€ ì˜µì…˜)
        self.embedding_options = {
            "google_vertex": {
                "model": "textembedding-gecko@001",  # Google Vertex AI ì„ë² ë”©
                "dimension": 768,
                "cost": "$0.00002/1000 í† í°",
                "ì¥ì ": "Geminiì™€ ê°™ì€ Google ìƒíƒœê³„, ì¼ê´€ì„± ì¢‹ìŒ",
                "ë‹¨ì ": "ë³„ë„ ë¹„ìš© ë°œìƒ"
            },
            "openai": {
                "model": "text-embedding-ada-002",  # OpenAI ì„ë² ë”©
                "dimension": 1536,
                "cost": "$0.0001/1000 í† í°",
                "ì¥ì ": "ê²€ì¦ëœ ì„±ëŠ¥, ë‹¤êµ­ì–´ ì§€ì› ìš°ìˆ˜",
                "ë‹¨ì ": "ê²½ìŸì‚¬ ì„œë¹„ìŠ¤, Geminiì™€ ë‹¤ë¥¸ ë²¤ë”"
            },
            "local_model": {
                "model": "sentence-transformers/all-MiniLM-L6-v2",
                "dimension": 384,
                "cost": "ì¸í”„ë¼ ë¹„ìš©ë§Œ",
                "ì¥ì ": "ë¹„ìš© íš¨ìœ¨ì , ì™„ì „ ìì²´ ì œì–´",
                "ë‹¨ì ": "ìì²´ ìš´ì˜ ë¶€ë‹´, ì„±ëŠ¥ ìµœì í™” í•„ìš”"
            }
        }

    def _init_gemini(self):
        """Gemini API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        import google.generativeai as genai
        genai.configure(api_key="YOUR_GEMINI_API_KEY")
        return genai.GenerativeModel('gemini-pro')

    def query_with_rag(self, user_question: str) -> str:
        """RAG íŒŒì´í”„ë¼ì¸: ê²€ìƒ‰ â†’ Gemini ìƒì„±"""

        # 1ë‹¨ê³„: ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ë³„ë„ ëª¨ë¸ í•„ìš”!)
        question_embedding = self.embed_text(user_question)

        # 2ë‹¨ê³„: ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = self.vector_db.search(
            vector=question_embedding,
            limit=5
        )

        # 3ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ì™€ ì§ˆë¬¸ì„ Geminiì— ì „ë‹¬
        context = "\n".join([doc.content for doc in relevant_docs])
        prompt = f"""
        ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:

        ì»¨í…ìŠ¤íŠ¸:
        {context}

        ì§ˆë¬¸: {user_question}

        ë‹µë³€:
        """

        # 4ë‹¨ê³„: Geminië¡œ ìµœì¢… ë‹µë³€ ìƒì„±
        response = self.gemini_client.generate_content(prompt)
        return response.text

    def embed_text(self, text: str):
        """ì„ íƒí•œ ì„ë² ë”© ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ë²¡í„°í™”"""
        # Google Vertex AI ì„ë² ë”© ì‚¬ìš© ì˜ˆì‹œ
        if self.embedding_provider == "google_vertex":
            from vertexai.language_models import TextEmbeddingModel
            model = TextEmbeddingModel.from_pretrained("textembedding-gecko@001")
            embeddings = model.get_embeddings([text])
            return embeddings[0].values

        # OpenAI ì„ë² ë”© ì‚¬ìš© ì˜ˆì‹œ
        elif self.embedding_provider == "openai":
            import openai
            response = openai.Embedding.create(
                model="text-embedding-ada-002",
                input=text
            )
            return response['data'][0]['embedding']

        # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì˜ˆì‹œ
        else:
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer('all-MiniLM-L6-v2')
            return model.encode([text])[0]
```

### 4.2 ì™œ GeminiëŠ” ë³„ë„ ì„ë² ë”©ì´ í•„ìš”í•œê°€?

**API ê¸°ë°˜ LLMì˜ êµ¬ì¡°ì  íŠ¹ì§•**

```python
class LLMComparisonChart:
    """ê° LLM ì„œë¹„ìŠ¤ì˜ ì„ë² ë”© ì œê³µ ì—¬ë¶€ ë¹„êµ"""
    def __init__(self):
        self.llm_services = {
            "OpenAI": {
                "llm_models": ["GPT-4", "GPT-3.5"],
                "embedding_api": "âœ… text-embedding-ada-002 ì œê³µ",
                "í†µí•©ì„±": "ê°™ì€ APIì—ì„œ LLM + ì„ë² ë”© ëª¨ë‘ ì œê³µ",
                "ë¹„ìš©": "LLM: $0.01-0.03/1Kí† í°, ì„ë² ë”©: $0.0001/1Kí† í°"
            },
            "Google Gemini": {
                "llm_models": ["Gemini Pro", "Gemini Pro Vision"],
                "embedding_api": "âŒ ë³„ë„ Vertex AI ì„ë² ë”© ì‚¬ìš© í•„ìš”",
                "í†µí•©ì„±": "ì„œë¡œ ë‹¤ë¥¸ ì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬ë¨",
                "ë¹„ìš©": "LLM: $0.000125-0.000375/1Kí† í°, ì„ë² ë”©: $0.00002/1Kí† í°"
            },
            "Anthropic Claude": {
                "llm_models": ["Claude-3 Sonnet", "Claude-3 Haiku"],
                "embedding_api": "âŒ ì™¸ë¶€ ì„ë² ë”© ëª¨ë¸ í•„ìš”",
                "í†µí•©ì„±": "í…ìŠ¤íŠ¸ ìƒì„±ë§Œ ì œê³µ",
                "ë¹„ìš©": "LLM: $0.003-0.015/1Kí† í°, ì„ë² ë”©: ë³„ë„ ì„œë¹„ìŠ¤"
            }
        }

    def get_embedding_strategy(self, llm_choice: str) -> dict:
        """LLM ì„ íƒì— ë”°ë¥¸ ì„ë² ë”© ì „ëµ ê°€ì´ë“œ"""
        strategies = {
            "gemini": {
                "ê¶Œì¥ ì„ë² ë”©": "Google Vertex AI textembedding-gecko",
                "ì´ìœ ": "ê°™ì€ Google ìƒíƒœê³„ë¡œ í˜¸í™˜ì„± ìµœì ",
                "ëŒ€ì•ˆ1": "OpenAI text-embedding-ada-002 (ì„±ëŠ¥ ìš°ìˆ˜)",
                "ëŒ€ì•ˆ2": "ë¡œì»¬ SentenceTransformer (ë¹„ìš© ì ˆì•½)",
                "êµ¬í˜„ ë³µì¡ë„": "ì¤‘ê°„ (2ê°œ ì„œë¹„ìŠ¤ ì—°ë™)"
            },
            "openai": {
                "ê¶Œì¥ ì„ë² ë”©": "OpenAI text-embedding-ada-002",
                "ì´ìœ ": "ê°™ì€ APIë¡œ í†µí•© ê´€ë¦¬ ê°€ëŠ¥",
                "ëŒ€ì•ˆ": "ì—†ìŒ (OpenAI ì„ë² ë”©ì´ ìµœì )",
                "êµ¬í˜„ ë³µì¡ë„": "ë‚®ìŒ (ë‹¨ì¼ ì„œë¹„ìŠ¤)"
            },
            "claude": {
                "ê¶Œì¥ ì„ë² ë”©": "OpenAI text-embedding-ada-002",
                "ì´ìœ ": "ê²€ì¦ëœ ì„±ëŠ¥ê³¼ ì•ˆì •ì„±",
                "ëŒ€ì•ˆ": "ë¡œì»¬ ë‹¤êµ­ì–´ ëª¨ë¸",
                "êµ¬í˜„ ë³µì¡ë„": "ì¤‘ê°„ (2ê°œ ì„œë¹„ìŠ¤ ì—°ë™)"
            }
        }
        return strategies.get(llm_choice, "ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM")
```

### 4.3 Gemini + ì„ë² ë”© ëª¨ë¸ ë¹„ìš© ë¶„ì„

```python
class GeminiCostAnalysis:
    """Gemini ì‚¬ìš© ì‹œ ì„ë² ë”© í¬í•¨ ì´ ë¹„ìš© ê³„ì‚°"""
    def __init__(self):
        self.pricing = {
            "gemini_pro": {
                "input": 0.000125/1000,   # $0.000125 per 1K tokens
                "output": 0.000375/1000   # $0.000375 per 1K tokens
            },
            "embedding_options": {
                "vertex_ai": 0.00002/1000,      # Google Vertex AI
                "openai_ada": 0.0001/1000,      # OpenAI embedding
                "local_model": 0                 # ìì²´ ìš´ì˜ ì‹œ API ë¹„ìš© ì—†ìŒ
            }
        }

    def calculate_total_cost(self, monthly_queries: int):
        """ì›”ê°„ RAG ì‹œìŠ¤í…œ ìš´ì˜ ë¹„ìš© ê³„ì‚°"""
        # ê°€ì •: ì§ˆë¬¸ë‹¹ í‰ê·  1000í† í° ì…ë ¥, 500í† í° ì¶œë ¥, 5ê°œ ë¬¸ì„œ ê²€ìƒ‰

        scenarios = {}

        for embedding_type, embedding_cost in self.pricing["embedding_options"].items():
            # Gemini LLM ë¹„ìš©
            gemini_input_cost = monthly_queries * 1000 * self.pricing["gemini_pro"]["input"]
            gemini_output_cost = monthly_queries * 500 * self.pricing["gemini_pro"]["output"]

            # ì„ë² ë”© ë¹„ìš© (ì§ˆë¬¸ 1íšŒ + ë¬¸ì„œ ì„ë² ë”©)
            embedding_monthly_cost = monthly_queries * 1000 * embedding_cost

            total_cost = gemini_input_cost + gemini_output_cost + embedding_monthly_cost

            scenarios[f"gemini + {embedding_type}"] = {
                "gemini_cost": gemini_input_cost + gemini_output_cost,
                "embedding_cost": embedding_monthly_cost,
                "total_monthly": total_cost,
                "cost_per_query": total_cost / monthly_queries
            }

        return scenarios

# ì‚¬ìš© ì˜ˆì‹œ
analyzer = GeminiCostAnalysis()
costs = analyzer.calculate_total_cost(monthly_queries=10000)

print("ì›” 1ë§Œ ì¿¼ë¦¬ ê¸°ì¤€ ë¹„ìš© ë¹„êµ:")
for scenario, cost in costs.items():
    print(f"{scenario}: ì›” ${cost['total_monthly']:.2f} (ì¿¼ë¦¬ë‹¹ ${cost['cost_per_query']:.4f})")

"""
ì˜ˆìƒ ì¶œë ¥:
gemini + vertex_ai: ì›” $1.45 (ì¿¼ë¦¬ë‹¹ $0.0001)  â† ê°€ì¥ í†µí•©ì„± ì¢‹ìŒ
gemini + openai_ada: ì›” $2.45 (ì¿¼ë¦¬ë‹¹ $0.0002) â† ì„±ëŠ¥ ì¢‹ìŒ, ë¹„ìš© ì¡°ê¸ˆ ë†’ìŒ
gemini + local_model: ì›” $1.25 (ì¿¼ë¦¬ë‹¹ $0.0001) â† ê°€ì¥ ê²½ì œì , ìš´ì˜ ë¶€ë‹´
"""
```

**ê²°ë¡ : Gemini ì‚¬ìš© ì‹œ ê¶Œì¥ì‚¬í•­**

1. **Google Vertex AI ì„ë² ë”© ì¶”ì²œ** - ê°™ì€ ìƒíƒœê³„ë¡œ í˜¸í™˜ì„± ìš°ìˆ˜
2. **OpenAI ì„ë² ë”©ë„ ì¢‹ì€ ì„ íƒ** - ê²€ì¦ëœ ì„±ëŠ¥, ì•½ê°„ì˜ ì¶”ê°€ ë¹„ìš©
3. **ë¡œì»¬ ëª¨ë¸ì€ ëŒ€ìš©ëŸ‰ ì„œë¹„ìŠ¤ìš©** - ì´ˆê¸° ì…‹ì—… ë³µì¡í•˜ì§€ë§Œ ì¥ê¸°ì  ë¹„ìš© ì ˆì•½

**LLM ìµœì í™” ê¸°ìˆ **

```python
class LLMOptimization:
    def __init__(self, model_name="gemini-pro"):
        self.model = model_name
        self.cache = {}  # ì‘ë‹µ ìºì‹±

    def optimize_prompt_tokens(self, prompt: str) -> str:
        """í”„ë¡¬í”„íŠ¸ í† í° ìµœì í™”"""
        # 1. ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        prompt = re.sub(r'\s+', ' ', prompt).strip()

        # 2. ë°˜ë³µë˜ëŠ” íŒ¨í„´ ì••ì¶•
        prompt = self._compress_repetitive_patterns(prompt)

        # 3. í† í° ìˆ˜ ì²´í¬ ë° ì¡°ì •
        if self._count_tokens(prompt) > self.max_tokens:
            prompt = self._truncate_intelligently(prompt)

        return prompt

    def implement_response_caching(self, query_hash: str, response: str):
        """ì‘ë‹µ ìºì‹±ìœ¼ë¡œ ë¹„ìš© ì ˆì•½"""
        self.cache[query_hash] = {
            "response": response,
            "timestamp": time.time(),
            "hit_count": 0
        }

    def batch_processing(self, queries: List[str]) -> List[str]:
        """ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± í–¥ìƒ"""
        # ìœ ì‚¬í•œ ì¿¼ë¦¬ë“¤ì„ ê·¸ë£¹í™”
        grouped_queries = self._group_similar_queries(queries)

        responses = []
        for group in grouped_queries:
            # ê·¸ë£¹ ë‚´ ì¿¼ë¦¬ë“¤ì„ í•˜ë‚˜ì˜ ìš”ì²­ìœ¼ë¡œ ì²˜ë¦¬
            batch_response = self._process_query_batch(group)
            responses.extend(batch_response)

        return responses
```

### 3.4 ì‹œìŠ¤í…œ í†µí•© ê¸°ìˆ 

#### 3.4.1 ì‹¤ì‹œê°„ íŒŒì´í”„ë¼ì¸ êµ¬í˜„

**ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ í†µí•©**

```python
import asyncio
from typing import List, Dict, Any
import time

class ProductionRAGPipeline:
    def __init__(self):
        self.embedding_service = ProductionEmbeddingService()
        self.vector_db = ProductionVectorDB()
        self.context_builder = AdvancedContextBuilder()
        self.llm_optimizer = LLMOptimization()

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.metrics = {
            "total_queries": 0,
            "avg_response_time": 0,
            "cache_hit_rate": 0
        }

    async def process_query(self, query: str, user_context: Dict = None) -> Dict[str, Any]:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()

        try:
            # 1ë‹¨ê³„: ì¿¼ë¦¬ ë²¡í„°í™”
            query_embedding = await self._embed_query(query)

            # 2ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰
            retrieved_docs = await self._search_documents(query_embedding, user_context)

            # 3ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(query, retrieved_docs)

            # 4ë‹¨ê³„: LLM ìƒì„±
            response = await self._generate_response(query, context)

            # 5ë‹¨ê³„: í›„ì²˜ë¦¬
            final_response = self._post_process_response(response, retrieved_docs)

            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self._update_metrics(time.time() - start_time)

            return final_response

        except Exception as e:
            return self._handle_error(e, query)

    async def _embed_query(self, query: str) -> List[float]:
        """ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± (ìºì‹± í¬í•¨)"""
        query_hash = hash(query)

        if query_hash in self.embedding_cache:
            return self.embedding_cache[query_hash]

        embedding = self.embedding_service.embed_documents([query])[0]
        self.embedding_cache[query_hash] = embedding

        return embedding

    async def _search_documents(self, query_embedding: List[float],
                              user_context: Dict = None) -> List[Dict]:
        """ë¬¸ì„œ ê²€ìƒ‰ (í•„í„°ë§ í¬í•¨)"""
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í•„í„°ë§
        filter_conditions = None
        if user_context:
            filter_conditions = self._build_filter_conditions(user_context)

        # ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰
        search_results = self.vector_db.search_similar(
            query_vector=query_embedding,
            top_k=20,  # ì¬ìˆœìœ„ë¥¼ ìœ„í•´ ë” ë§ì´ ê²€ìƒ‰
            filter_conditions=filter_conditions
        )

        return search_results

    def _build_context(self, query: str, retrieved_docs: List[Dict]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        return self.context_builder.build_context(query, retrieved_docs)

    async def _generate_response(self, query: str, context: str) -> str:
        """LLM ì‘ë‹µ ìƒì„±"""
        # í”„ë¡¬í”„íŠ¸ ìµœì í™”
        optimized_prompt = self.llm_optimizer.optimize_prompt_tokens(
            self._build_final_prompt(query, context)
        )

        # LLM í˜¸ì¶œ (ë¹„ë™ê¸°)
        response = await self._call_llm_async(optimized_prompt)

        return response

    def _post_process_response(self, response: str, sources: List[Dict]) -> Dict[str, Any]:
        """ì‘ë‹µ í›„ì²˜ë¦¬"""
        return {
            "answer": response,
            "sources": [{"title": doc.get("title"), "url": doc.get("url")} for doc in sources[:3]],
            "confidence": self._calculate_confidence(response, sources),
            "response_time": f"{time.time() - self.start_time:.2f}s"
        }
```

ì´ì œ ì¶”ê°€ë¡œ í‰ê°€ ì§€í‘œì™€ ë„ì „ ê³¼ì œ ë¶€ë¶„ë„ ë³´ê°•í•˜ê² ìŠµë‹ˆë‹¤.

```mermaid
graph LR
    A[ì‚¬ìš©ì ì§ˆë¬¸] --> B[ì§ˆë¬¸ ë²¡í„°í™”]
    B --> C[ë²¡í„° DB ê²€ìƒ‰]
    C --> D[ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰]
    D --> E[ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±]
    E --> F[LLM ìƒì„±]
    F --> G[ìµœì¢… ë‹µë³€]
```

### 3.1 ë‹¨ê³„ë³„ í”„ë¡œì„¸ìŠ¤

1. **ì§ˆë¬¸ ì…ë ¥**: ì‚¬ìš©ìê°€ ìì—°ì–´ë¡œ ì§ˆë¬¸
2. **ë²¡í„°í™”**: ì§ˆë¬¸ì„ ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ë³€í™˜
3. **ìœ ì‚¬ë„ ê²€ìƒ‰**: ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
4. **ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±**: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
5. **ë‹µë³€ ìƒì„±**: LLMì´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
6. **ì‘ë‹µ ë°˜í™˜**: ìµœì¢… ë‹µë³€ì„ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬

## 4. RAGì˜ ì¥ì 

### 4.1 ì •í™•ì„± í–¥ìƒ

- ìµœì‹  ì •ë³´ ë°˜ì˜ ê°€ëŠ¥
- ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ í™œìš©
- í• ë£¨ì‹œë„¤ì´ì…˜(í™˜ê°) í˜„ìƒ ê°ì†Œ

### 4.2 ë¹„ìš© íš¨ìœ¨ì„±

- LLM ì¬í•™ìŠµ ë¶ˆí•„ìš”
- ì‹¤ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸ ê°€ëŠ¥
- ê³„ì‚° ë¹„ìš© ìµœì í™”

### 4.3 íˆ¬ëª…ì„±

- ë‹µë³€ì˜ ê·¼ê±° ì œì‹œ ê°€ëŠ¥
- ì¶œì²˜ ì¶”ì  ê°€ëŠ¥
- ì‹ ë¢°ì„± í–¥ìƒ

## 5. RAG vs ê¸°ì¡´ ë°©ì‹ ë¹„êµ

| êµ¬ë¶„          | ê¸°ì¡´ LLM    | RAG ì‹œìŠ¤í…œ      |
| ------------- | ----------- | --------------- |
| ì§€ì‹ ì—…ë°ì´íŠ¸ | ì¬í•™ìŠµ í•„ìš” | ì‹¤ì‹œê°„ ê°€ëŠ¥     |
| ë„ë©”ì¸ íŠ¹í™”   | ì œí•œì       | ë†’ìŒ            |
| ë¹„ìš©          | ë†’ìŒ        | ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ |
| íˆ¬ëª…ì„±        | ë‚®ìŒ        | ë†’ìŒ            |
| ì •í™•ì„±        | ë³´í†µ        | ë†’ìŒ            |

## 6. ì£¼ìš” ì‚¬ìš© ì‚¬ë¡€

### 6.1 ê¸°ì—… ë‚´ë¶€ QA ì‹œìŠ¤í…œ

- ì‚¬ë‚´ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ
- ì •ì±… ë° ê·œì • ì•ˆë‚´
- ê¸°ìˆ  ë¬¸ì„œ ê²€ìƒ‰

### 6.2 ê³ ê° ì§€ì›

- FAQ ìë™ ì‘ë‹µ
- ì œí’ˆ ì •ë³´ ì•ˆë‚´
- ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### 6.3 ì—°êµ¬ ë° í•™ìˆ 

- ë…¼ë¬¸ ê²€ìƒ‰ ë° ìš”ì•½
- ì—°êµ¬ ìë£Œ ë¶„ì„
- ë¬¸í—Œ ë¦¬ë·°

## 7. ì„±ëŠ¥ í‰ê°€ ì§€í‘œ

### 7.1 ê²€ìƒ‰ ì„±ëŠ¥

- **Recall**: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ìœ¨
- **Precision**: ê²€ìƒ‰ ì •í™•ë„
- **MRR (Mean Reciprocal Rank)**: í‰ê·  ì—­ìˆœìœ„

### 7.2 ìƒì„± ì„±ëŠ¥

- **BLEU Score**: ë²ˆì—­ í’ˆì§ˆ í‰ê°€
- **ROUGE Score**: ìš”ì•½ í’ˆì§ˆ í‰ê°€
- **Human Evaluation**: ì¸ê°„ í‰ê°€

### 7.3 ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥

- **Response Time**: ì‘ë‹µ ì‹œê°„
- **Relevance Score**: ê´€ë ¨ì„± ì ìˆ˜
- **User Satisfaction**: ì‚¬ìš©ì ë§Œì¡±ë„

## 8. ë„ì „ ê³¼ì œ

### 8.1 ê¸°ìˆ ì  ê³¼ì œ

- **ë²¡í„° í’ˆì§ˆ**: ì„ë² ë”© ëª¨ë¸ì˜ ì„±ëŠ¥
- **ê²€ìƒ‰ ì •í™•ë„**: ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ê²€ìƒ‰
- **ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´**: LLM ì…ë ¥ ì œí•œ

### 8.2 ìš´ì˜ì  ê³¼ì œ

- **ë°ì´í„° í’ˆì§ˆ**: ì†ŒìŠ¤ ë¬¸ì„œì˜ í’ˆì§ˆ ê´€ë¦¬
- **ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸**: ë¬¸ì„œ ë³€ê²½ ì‹œ ì¦‰ì‹œ ë°˜ì˜
- **í™•ì¥ì„±**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬

## 9. ìµœì‹  íŠ¸ë Œë“œ

### 9.1 Advanced RAG

- **Multi-hop Reasoning**: ë‹¤ë‹¨ê³„ ì¶”ë¡ 
- **Graph RAG**: ì§€ì‹ ê·¸ë˜í”„ í™œìš©
- **Agentic RAG**: ì—ì´ì „íŠ¸ ê¸°ë°˜ RAG

### 9.2 ê¸°ìˆ  ë°œì „

- **Dense Retrieval**: ë°€ì§‘ í‘œí˜„ ê¸°ë°˜ ê²€ìƒ‰
- **Hybrid Search**: í‚¤ì›Œë“œ + ë²¡í„° ê²€ìƒ‰
- **Adaptive RAG**: ì ì‘í˜• RAG

## 5. RAG ì‹œìŠ¤í…œ êµ¬í˜„ ì‹œ ê³ ë ¤ì‚¬í•­

### 5.1 ë³´ì•ˆ ë° ê°œì¸ì •ë³´ ì²˜ë¦¬

**ì™œ RAG ì‹œìŠ¤í…œì—ì„œ ë³´ì•ˆì´ ì¤‘ìš”í•œê°€?**

```python
class RAGSecurityChallenges:
    def __init__(self):
        self.security_concerns = {
            "ë°ì´í„° ìœ ì¶œ": {
                "ìœ„í—˜": "ë²¡í„° DBì—ì„œ ë¯¼ê° ì •ë³´ ê²€ìƒ‰ë  ìˆ˜ ìˆìŒ",
                "í•´ê²°ì±…": ["ì ‘ê·¼ ê¶Œí•œ ê´€ë¦¬", "ë°ì´í„° ë§ˆìŠ¤í‚¹", "ì¿¼ë¦¬ í•„í„°ë§"]
            },
            "í”„ë¡¬í”„íŠ¸ ì¸ì ì…˜": {
                "ìœ„í—˜": "ì•…ì˜ì  ì¿¼ë¦¬ë¡œ ì‹œìŠ¤í…œ ì¡°ì‘ ê°€ëŠ¥",
                "í•´ê²°ì±…": ["ì…ë ¥ ê²€ì¦", "í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿í™”", "ì¶œë ¥ í•„í„°ë§"]
            },
            "ëª¨ë¸ ì—­ì¶”ë¡ ": {
                "ìœ„í—˜": "ì„ë² ë”©ìœ¼ë¡œë¶€í„° ì›ë³¸ ë°ì´í„° ì¶”ì • ê°€ëŠ¥",
                "í•´ê²°ì±…": ["ì°¨ë¶„ ê°œì¸ì •ë³´ë³´í˜¸", "ë…¸ì´ì¦ˆ ì¶”ê°€", "ì ‘ê·¼ ë¡œê·¸ ê´€ë¦¬"]
            }
        }
```

### 5.2 ë¹„ìš© ìµœì í™” ì „ëµ

**í´ë¼ìš°ë“œ vs ì˜¨í”„ë ˆë¯¸ìŠ¤ ë¹„ìš© ë¶„ì„**

```python
# ì´ ì½”ë“œëŠ” RAG ì‹œìŠ¤í…œ ìš´ì˜ ë¹„ìš©ì„ ìƒì„¸íˆ ê³„ì‚°í•˜ê³  ìµœì í™” ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤
# ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ í™˜ê²½ì—ì„œ ROIë¥¼ ê³„ì‚°í•˜ê³  ë¹„ìš© íš¨ìœ¨ì ì¸ ë°©ì•ˆì„ ì°¾ì„ ë•Œ í™œìš©

class CostOptimization:
    """ìš´ì˜ ë¹„ìš© ë¶„ì„ ë° ìµœì í™” ì „ëµ ì œê³µ"""
    def calculate_monthly_costs(self, usage_stats: Dict) -> Dict:
        """ì‚¬ìš©ëŸ‰ í†µê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›”ê°„ ìš´ì˜ ë¹„ìš© ìƒì„¸ ê³„ì‚°"""

        # ì‚¬ìš©ì ì…ë ¥ ë°ì´í„°ì—ì„œ ê¸°ë³¸ ì§€í‘œ ì¶”ì¶œ
        queries_per_month = usage_stats["queries_per_month"]  # ì›”ê°„ ì¿¼ë¦¬ ìˆ˜
        avg_docs_per_query = usage_stats["avg_docs_per_query"]  # ì¿¼ë¦¬ë‹¹ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
        avg_tokens_per_doc = usage_stats["avg_tokens_per_doc"]  # ë¬¸ì„œë‹¹ í‰ê·  í† í° ìˆ˜

        costs = {}

        # 1. ì„ë² ë”© ë¹„ìš© ê³„ì‚° (ì²˜ìŒ í•œ ë²ˆë§Œ ìƒì„±, ê·¸ ë’¤ëŠ” ì¬ì‚¬ìš©)
        embedding_tokens = (
            queries_per_month * 100 +  # ì‚¬ìš©ì ì¿¼ë¦¬ ì„ë² ë”© (í‰ê·  100í† í°)
            usage_stats["new_docs_per_month"] * avg_tokens_per_doc  # ìƒˆë¡œ ì¶”ê°€ë˜ëŠ” ë¬¸ì„œ
        )
        costs["embedding"] = embedding_tokens * 0.0001 / 1000  # OpenAI ì„ë² ë”© API ìš”ê¸ˆ        # ë²¡í„° DB ë¹„ìš© (Qdrant ìì²´ í˜¸ìŠ¤íŒ…)
        costs["vector_db"] = 100  # ì›”ê°„ ì„œë²„ ë¹„ìš©

        # LLM ìƒì„± ë¹„ìš©
        context_tokens_per_query = avg_docs_per_query * avg_tokens_per_doc
        total_input_tokens = queries_per_month * (100 + context_tokens_per_query)
        total_output_tokens = queries_per_month * 200  # í‰ê·  ë‹µë³€ ê¸¸ì´

        costs["llm_input"] = total_input_tokens * 0.01 / 1000  # GPT-4 input
        costs["llm_output"] = total_output_tokens * 0.03 / 1000  # GPT-4 output

        # ì´ ë¹„ìš©
        costs["total"] = sum(costs.values())

        return costs
```

## 6. ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„

### 6.1 RAG ì‹œìŠ¤í…œ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### í•„ìˆ˜ ê¸°ìˆ  ìŠ¤íƒ ì¤€ë¹„ì‚¬í•­

- [ ] **ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ëª¨ë¸**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ í†µí•© ì²˜ë¦¬ (CLIP, LayoutLM)
- [ ] **OCR ì—”ì§„**: ì €í™”ì§ˆ ë„ë©´ ì²˜ë¦¬ (PaddleOCR, AWS Textract, Tesseract)
- [ ] **ì´ë¯¸ì§€ ì „ì²˜ë¦¬**: ë…¸ì´ì¦ˆ ì œê±°, ì„ ëª…í™”, ì´ì§„í™” íŒŒì´í”„ë¼ì¸
- [ ] **ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤**: ë©€í‹°ëª¨ë‹¬ ë°ì´í„° í™•ì¥ ê°€ëŠ¥í•œ ê²€ìƒ‰ ì„±ëŠ¥
- [ ] **ë©€í‹°ëª¨ë‹¬ LLM**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ë™ì‹œ ì²˜ë¦¬ (GPT-4V, Claude 3, Gemini Pro Vision)
- [ ] **PDF íŒŒì‹±**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì¶”ì¶œ ë° ë ˆì´ì•„ì›ƒ ë¶„ì„
- [ ] **CAD íŒŒì¼ ì§€ì›**: DWG, DXF íŒŒì¼ ì½ê¸° ë° ë ˆì´ì–´ë³„ ì •ë³´ ì¶”ì¶œ
- [ ] **í‰ê°€ ì‹œìŠ¤í…œ**: ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰/ìƒì„± í’ˆì§ˆ ì¸¡ì •

#### ë©€í‹°ëª¨ë‹¬ RAG íŠ¹í™” ìš”êµ¬ì‚¬í•­

- [ ] **ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„ **: ì €í•´ìƒë„ ë„ë©´ì˜ OCR ì •í™•ë„ í–¥ìƒ
- [ ] **ë„ë©´ í•´ì„ ëŠ¥ë ¥**: ê±´ì¶•/ê¸°ê³„ ë„ë©´ì˜ ê¸°í˜¸, ì¹˜ìˆ˜, ê³µì°¨ ì´í•´
- [ ] **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ë©”íƒ€ë°ì´í„° í†µí•© ê²€ìƒ‰
- [ ] **ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì •ë³´ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- [ ] **ì‹ ë¢°ë„ í‘œì‹œ**: OCR ê²°ê³¼ì˜ ì‹ ë¢°ë„ ë° ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ
- [ ] **ì „ë¬¸ê°€ ê²€í†  ì›Œí¬í”Œë¡œìš°**: ì¤‘ìš”í•œ ê¸°ìˆ  ì •ë³´ëŠ” ì „ë¬¸ê°€ í™•ì¸ í”„ë¡œì„¸ìŠ¤

#### ë¹„ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­

- [ ] **ë³´ì•ˆ**: ì ‘ê·¼ ì œì–´, ë°ì´í„° ìµëª…í™”, ê°ì‚¬ ë¡œê·¸
- [ ] **ì„±ëŠ¥**: ì‘ë‹µì‹œê°„ < 3ì´ˆ, ì²˜ë¦¬ëŸ‰ > 100 qps
- [ ] **ë¹„ìš©**: ì›” ìš´ì˜ë¹„ìš© ìµœì í™”, ROI ì¸¡ì •
- [ ] **ëª¨ë‹ˆí„°ë§**: í—¬ìŠ¤ ì²´í¬, ì•Œë¦¼, ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ
- [ ] **í™•ì¥ì„±**: íŠ¸ë˜í”½ ì¦ê°€ ëŒ€ì‘, ìˆ˜í‰ í™•ì¥

### 6.2 ì„±ê³µì ì¸ RAG êµ¬í˜„ì„ ìœ„í•œ í•µì‹¬ í¬ì¸íŠ¸

1. **ë‹¨ê³„ë³„ êµ¬í˜„**: MVPë¶€í„° ì‹œì‘í•´ì„œ ì ì§„ì  ê°œì„ 
2. **ë°ì´í„° í’ˆì§ˆ**: ê³ í’ˆì§ˆ ë¬¸ì„œ, ì •í™•í•œ ë©”íƒ€ë°ì´í„°
3. **ì‚¬ìš©ì ì¤‘ì‹¬**: ì‹¤ì œ ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ë° ìµœì í™”
4. **ì§€ì†ì  í‰ê°€**: A/B í…ŒìŠ¤íŠ¸, ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
5. **ìš´ì˜ ê³ ë ¤**: ëª¨ë‹ˆí„°ë§, ì•Œë¦¼, ì¥ì•  ëŒ€ì‘ í”„ë¡œì„¸ìŠ¤

### 6.3 í–¥í›„ ë°œì „ ë°©í–¥

```python
future_enhancements = {
    "ë©€í‹°ëª¨ë‹¬_RAG": {
        "description": "í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + í…Œì´ë¸” í†µí•© ê²€ìƒ‰",
        "technologies": ["CLIP", "LayoutLM", "Multimodal Embeddings"],
        "timeline": "6-12ê°œì›”"
    },
    "ì—ì´ì „íŠ¸_RAG": {
        "description": "ììœ¨ì  ì •ë³´ ìˆ˜ì§‘ ë° ì¶”ë¡  ì—ì´ì „íŠ¸",
        "technologies": ["AutoGPT", "LangGraph", "Tool Usage"],
        "timeline": "3-6ê°œì›”"
    },
    "ì‹¤ì‹œê°„_RAG": {
        "description": "ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì‹¤ì‹œê°„ ì²˜ë¦¬",
        "technologies": ["Kafka", "Delta Lake", "Stream Processing"],
        "timeline": "9-12ê°œì›”"
    }
}
```

RAG ì‹œìŠ¤í…œì€ ë‹¨ìˆœí•œ ê²€ìƒ‰-ìƒì„±ì„ ë„˜ì–´ì„œ ì§€ëŠ¥ì ì¸ ì •ë³´ ì²˜ë¦¬ í”Œë«í¼ìœ¼ë¡œ ì§„í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì„±ê³µì ì¸ êµ¬í˜„ì„ ìœ„í•´ì„œëŠ” ê¸°ìˆ ì  ì™„ì„±ë„ë¿ë§Œ ì•„ë‹ˆë¼ ì‚¬ìš©ì ê²½í—˜, ìš´ì˜ íš¨ìœ¨ì„±, ë¹„ìš© ìµœì í™”ë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ì¢…í•©ì  ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.

---

**ë‹¤ìŒ ë¬¸ì„œ**: [02-System-Architecture.md](./02-System-Architecture.md) - RAG ì‹œìŠ¤í…œì˜ êµ¬ì²´ì ì¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

## 10. ê²°ë¡ 

RAG ì‹œìŠ¤í…œì€ AIì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³  ë” ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” AI ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•˜ëŠ” í•µì‹¬ ê¸°ìˆ ì…ë‹ˆë‹¤. íŠ¹íˆ ê¸°ì—… í™˜ê²½ì—ì„œ ë‚´ë¶€ ì§€ì‹ì„ í™œìš©í•œ AI ì„œë¹„ìŠ¤ êµ¬ì¶•ì— ë§¤ìš° íš¨ê³¼ì ì…ë‹ˆë‹¤.
