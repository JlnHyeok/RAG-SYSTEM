# Agent (RAG Engine) ê°œë°œ í”„ë¡œì„¸ìŠ¤ ë° ê³„íš

## ğŸ“‹ ê°œë°œ ì¼ì • (4ì¼)

### Day 1: í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì • ë° ê¸°ë³¸ êµ¬ì¡°

- [ ] Python/FastAPI í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
- [ ] ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° DB ì„¤ì •
- [ ] ê¸°ë³¸ API êµ¬ì¡° ìƒì„±
- [ ] Gemini API ì—°ë™

### Day 2: ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

- [ ] ë¬¸ì„œ íŒŒì‹± ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
- [ ] ì²­í‚¹(Chunking) ì „ëµ êµ¬í˜„
- [ ] ì„ë² ë”© ìƒì„± ë° ë²¡í„° ì €ì¥
- [ ] ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê´€ë¦¬

### Day 3: RAG ì¿¼ë¦¬ ì—”ì§„

- [ ] ìœ ì‚¬ë„ ê²€ìƒ‰ êµ¬í˜„
- [ ] ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ë° í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- [ ] LLM ì—°ë™ ë° ì‘ë‹µ ìƒì„±
- [ ] ì¶œì²˜ ì¶”ì  ì‹œìŠ¤í…œ

### Day 4: ìµœì í™” ë° ê³ ë„í™”

- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] ê³ ê¸‰ RAG ê¸°ë²• ì ìš©
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹…
- [ ] API í…ŒìŠ¤íŠ¸ ë° ë¬¸ì„œí™”

## ğŸ›  ê¸°ìˆ  ìŠ¤íƒ

### í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
# requirements.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0

# AI ëª¨ë¸ ë° ì„ë² ë”©
sentence-transformers==2.2.2  # ì„ë² ë”© ëª¨ë¸
transformers==4.36.0
torch==2.1.0
google-generativeai==0.3.0    # Gemini API

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
qdrant-client==1.7.0

# OCR ë° ì´ë¯¸ì§€ ì²˜ë¦¬
pytesseract==0.3.10
opencv-python==4.8.1
paddlepaddle==2.5.1
paddleocr==2.7.0
easyocr==1.7.0

# ë¬¸ì„œ ì²˜ë¦¬
pypdf2==3.0.1
PyMuPDF==1.23.5               # PDF + ì´ë¯¸ì§€ ì¶”ì¶œ
python-docx==1.1.0
python-multipart==0.0.6
chardet==5.2.0
beautifulsoup4==4.12.2

# ë°ì´í„° ì²˜ë¦¬
pandas==2.1.4
numpy==1.24.3
tiktoken==0.5.2

# ìœ í‹¸ë¦¬í‹°
pydantic==2.5.0
python-dotenv==1.0.0
httpx==0.25.2
aiofiles==23.2.0
redis==5.0.1

# ê°œë°œ ë„êµ¬
pytest==7.4.3
pytest-asyncio==0.21.1
black==23.11.0
isort==5.12.0
mypy==1.7.1
```

## ğŸ“ í´ë” êµ¬ì¡°

```
agent/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/                     # API ë¼ìš°í„°
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ documents.py     # ë¬¸ì„œ ì²˜ë¦¬ API
â”‚   â”‚   â”‚   â”œâ”€â”€ query.py         # ì¿¼ë¦¬ ì²˜ë¦¬ API
â”‚   â”‚   â”‚   â””â”€â”€ health.py        # í—¬ìŠ¤ì²´í¬ API
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/                    # í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
â”‚   â”‚   â”œâ”€â”€ config.py            # ì„¤ì • ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ embedding_manager.py # ì„ë² ë”© ëª¨ë¸ ê´€ë¦¬
â”‚   â”‚   â”œâ”€â”€ vector_store.py      # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
â”‚   â”‚   â”œâ”€â”€ document_processor.py # ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ ocr_engine.py        # OCR ì²˜ë¦¬ ì—”ì§„
â”‚   â”‚   â”œâ”€â”€ image_enhancer.py    # ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„ 
â”‚   â”‚   â”œâ”€â”€ rag_engine.py        # RAG ì—”ì§„
â”‚   â”‚   â””â”€â”€ gemini_service.py    # Gemini LLM ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ models/                  # ë°ì´í„° ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ schemas.py           # Pydantic ìŠ¤í‚¤ë§ˆ
â”‚   â”‚   â””â”€â”€ enums.py             # ì—´ê±°í˜• ì •ì˜
â”‚   â”œâ”€â”€ services/                # ì„œë¹„ìŠ¤ ê³„ì¸µ
â”‚   â”‚   â”œâ”€â”€ document_service.py  # ë¬¸ì„œ ê´€ë¦¬ ì„œë¹„ìŠ¤
â”‚   â”‚   â”œâ”€â”€ query_service.py     # ì¿¼ë¦¬ ì²˜ë¦¬ ì„œë¹„ìŠ¤
â”‚   â”‚   â””â”€â”€ cache_service.py     # ìºì‹œ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ utils/                   # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”‚   â”‚   â”œâ”€â”€ text_splitter.py     # í…ìŠ¤íŠ¸ ë¶„í• 
â”‚   â”‚   â”œâ”€â”€ file_handlers.py     # íŒŒì¼ ì²˜ë¦¬
â”‚   â”‚   â”œâ”€â”€ logger.py            # ë¡œê¹…
â”‚   â”‚   â””â”€â”€ metrics.py           # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
â”‚   â””â”€â”€ main.py                  # FastAPI ì•± ì§„ì…ì 
â”œâ”€â”€ tests/                       # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”œâ”€â”€ scripts/                     # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ docs/                        # ë¬¸ì„œ
â”œâ”€â”€ .env.example                 # í™˜ê²½ ë³€ìˆ˜ í…œí”Œë¦¿
â”œâ”€â”€ Dockerfile                   # Docker ì„¤ì •
â”œâ”€â”€ docker-compose.yml           # Docker Compose
â”œâ”€â”€ requirements.txt             # Python ì˜ì¡´ì„±
â””â”€â”€ README.md                    # í”„ë¡œì íŠ¸ README
```

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸ êµ¬í˜„

### 1. FastAPI ì•± ì„¤ì •

```python
# app/main.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
import logging
from contextlib import asynccontextmanager

from app.api.v1 import documents, query, health
from app.core.config import settings
from app.core.vector_store import VectorStore
from app.utils.logger import setup_logging

# ë¡œê¹… ì„¤ì •
setup_logging()
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ ì‹œ ì´ˆê¸°í™”
    logger.info("Initializing RAG Agent...")

    # Vector Store ì´ˆê¸°í™”
    app.state.vector_store = VectorStore()
    await app.state.vector_store.initialize()

    yield

    # ì¢…ë£Œ ì‹œ ì •ë¦¬
    logger.info("Shutting down RAG Agent...")
    if hasattr(app.state, 'vector_store'):
        await app.state.vector_store.close()

app = FastAPI(
    title="RAG Agent API",
    description="RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ ì²˜ë¦¬ ì—”ì§„",
    version="1.0.0",
    lifespan=lifespan
)

# ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_HOSTS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.add_middleware(GZipMiddleware, minimum_size=1000)

# ë¼ìš°í„° ë“±ë¡
app.include_router(health.router, prefix="/health", tags=["health"])
app.include_router(documents.router, prefix="/api/v1/documents", tags=["documents"])
app.include_router(query.router, prefix="/api/v1/query", tags=["query"])

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Global exception: {exc}", exc_info=True)
    return HTTPException(status_code=500, detail="Internal server error")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.DEBUG,
        log_level="info"
    )
```

### 2. ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```python
# app/core/document_processor.py
import asyncio
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
import aiofiles
import fitz  # PyMuPDF
import uuid
from PIL import Image
import cv2
import numpy as np

from app.core.ocr_engine import MultiOCREngine
from app.core.image_enhancer import ImageEnhancer
from app.core.embedding_manager import EmbeddingManager
from app.models.schemas import DocumentChunk, ProcessingResult

logger = logging.getLogger(__name__)

class MultiModalDocumentProcessor:
    """ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬ - PDF, ì´ë¯¸ì§€, ë„ë©´ ë“±ì„ í†µí•© ì²˜ë¦¬"""

    def __init__(self):
        self.ocr_engine = MultiOCREngine()
        self.image_enhancer = ImageEnhancer()
        self.embedding_manager = EmbeddingManager()

    async def process_document(
        self,
        file_path: str,
        user_id: str
    ) -> ProcessingResult:
        """ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì„ë² ë”© ìƒì„±"""
        try:
            file_extension = Path(file_path).suffix.lower()

            if file_extension == '.pdf':
                return await self._process_pdf_with_images(file_path, user_id)
            elif file_extension in ['.jpg', '.jpeg', '.png', '.tiff']:
                return await self._process_image_document(file_path, user_id)
            else:
                return await self._process_text_document(file_path, user_id)

        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    async def _process_pdf_with_images(self, pdf_path: str, user_id: str) -> ProcessingResult:
        """ì´ë¯¸ì§€ê°€ í¬í•¨ëœ PDF ë¬¸ì„œ ì²˜ë¦¬"""
        doc = fitz.open(pdf_path)
        results = {
            "text_chunks": [],
            "image_chunks": [],
            "total_embeddings": 0
        }

        for page_num in range(len(doc)):
            page = doc.load_page(page_num)

            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text = page.get_text()
            if text.strip():
                # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
                embedding = await self.embedding_manager.embed_text(text)
                chunk = DocumentChunk(
                    id=str(uuid.uuid4()),
                    content=text,
                    embedding=embedding,
                    metadata={"page": page_num + 1, "type": "text"}
                )
                results["text_chunks"].append(chunk)

            # ì´ë¯¸ì§€ ì¶”ì¶œ ë° OCR ì²˜ë¦¬
            image_list = page.get_images()
            for img_index, img in enumerate(image_list):
                try:
                    # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
                    image_data = self._extract_image_from_pdf(doc, img)

                    # í™”ì§ˆ ê°œì„  (ì €í’ˆì§ˆ ë„ë©´ ëŒ€ì‘)
                    enhanced_image = await self.image_enhancer.enhance_quality(image_data)

                    # ë©€í‹° OCR ì—”ì§„ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    ocr_results = await self.ocr_engine.extract_text_multi_engine(enhanced_image)

                    if ocr_results["confidence"] > 0.7:  # ì‹ ë¢°ë„ ì„ê³„ê°’
                        # ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ìƒì„± (í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ ì •ë³´)
                        embedding = await self.embedding_manager.embed_multimodal(
                            text=ocr_results["text"],
                            image_path=None  # ì´ë¯¸ì§€ ìº¡ì…˜ë„ ì¶”ê°€ ê°€ëŠ¥
                        )

                        chunk = DocumentChunk(
                            id=str(uuid.uuid4()),
                            content=ocr_results["text"],
                            embedding=embedding,
                            metadata={
                                "page": page_num + 1,
                                "type": "image_ocr",
                                "confidence": ocr_results["confidence"],
                                "ocr_engine": ocr_results["best_engine"]
                            }
                        )
                        results["image_chunks"].append(chunk)

                except Exception as e:
                    logger.warning(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ (page {page_num}, img {img_index}): {e}")

        results["total_embeddings"] = len(results["text_chunks"]) + len(results["image_chunks"])
        return ProcessingResult(**results)
```

            # 2. ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = await self._generate_metadata(file_path, document_id, user_id)

            # 3. í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = await self._split_text(
                text_content,
                metadata,
                strategy=chunk_strategy
            )

            logger.info(f"Document {document_id} processed: {len(chunks)} chunks created")

            return ProcessingResult(
                document_id=document_id,
                chunk_count=len(chunks),
                chunks=chunks,
                metadata=metadata,
                status="success"
            )

        except Exception as e:
            logger.error(f"Error processing document {document_id}: {e}")
            return ProcessingResult(
                document_id=document_id,
                chunk_count=0,
                chunks=[],
                metadata={},
                status="error",
                error_message=str(e)
            )

    async def _extract_text(self, file_path: str) -> str:
        """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        path = Path(file_path)

        if not path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")

        if path.suffix.lower() == '.pdf':
            return await self._extract_pdf_text(file_path)
        elif path.suffix.lower() in ['.docx', '.doc']:
            return await self._extract_docx_text(file_path)
        elif path.suffix.lower() == '.txt':
            return await self._extract_txt_text(file_path)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {path.suffix}")

    async def _extract_pdf_text(self, file_path: str) -> str:
        """PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        def extract():
            with open(file_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ""
                for page_num, page in enumerate(reader.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text:
                            text += f"\n\n--- Page {page_num + 1} ---\n{page_text}"
                    except Exception as e:
                        logger.warning(f"Error extracting text from page {page_num + 1}: {e}")
                return text

        return await asyncio.to_thread(extract)

    async def _extract_docx_text(self, file_path: str) -> str:
        """DOCXì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        def extract():
            doc = docx.Document(file_path)
            paragraphs = [paragraph.text for paragraph in doc.paragraphs]
            return "\n\n".join(paragraphs)

        return await asyncio.to_thread(extract)

    async def _extract_txt_text(self, file_path: str) -> str:
        """TXT íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        async with aiofiles.open(file_path, 'rb') as file:
            raw_data = await file.read()

### 3. ì„ë² ë”© ë§¤ë‹ˆì €

```python
# app/core/embedding_manager.py
import logging
from typing import List, Union, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
import torch

logger = logging.getLogger(__name__)

class EmbeddingManager:
    """ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ì„ í†µí•© ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self):
        self.models = {}
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self._load_models()

    def _load_models(self):
        """ì„ë² ë”© ëª¨ë¸ë“¤ ë¡œë”©"""
        print("ì„ë² ë”© ëª¨ë¸ë“¤ ë¡œë”© ì¤‘...")

        # í…ìŠ¤íŠ¸ ì„ë² ë”© (ê°€ì¥ ë§ì´ ì‚¬ìš©)
        self.models['text'] = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)

        # í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”©
        self.models['korean'] = SentenceTransformer('jhgan/ko-sroberta-multitask', device=self.device)

        # ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© (í…ìŠ¤íŠ¸+ì´ë¯¸ì§€)
        self.models['clip'] = SentenceTransformer('clip-ViT-B-32', device=self.device)

        print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    async def embed_text(self, text: str, model_type: str = 'korean') -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        try:
            if model_type not in self.models:
                model_type = 'text'  # ê¸°ë³¸ê°’ìœ¼ë¡œ í´ë°±

            embedding = self.models[model_type].encode([text], convert_to_numpy=True)
            return embedding[0].tolist()

        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    async def embed_batch(self, texts: List[str], model_type: str = 'korean') -> List[List[float]]:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œë²ˆì— ë²¡í„°ë¡œ ë³€í™˜ (ì„±ëŠ¥ ìµœì í™”)"""
        try:
            if model_type not in self.models:
                model_type = 'text'

            embeddings = self.models[model_type].encode(
                texts,
                convert_to_numpy=True,
                batch_size=32,  # GPU ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
                show_progress_bar=True
            )
            return embeddings.tolist()

        except Exception as e:
            logger.error(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    async def embed_multimodal(self, text: str, image_path: Optional[str] = None) -> List[float]:
        """í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ í•¨ê»˜ ì„ë² ë”© (ë©€í‹°ëª¨ë‹¬)"""
        try:
            # CLIP ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ í†µí•© ì„ë² ë”©
            if image_path:
                # ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ í†µí•© ì„ë² ë”© (ì‹¤ì œ êµ¬í˜„ì‹œ ì´ë¯¸ì§€ ë¡œë”© í•„ìš”)
                embedding = self.models['clip'].encode([text])
            else:
                # í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©
                embedding = self.models['clip'].encode([text])

            return embedding[0].tolist()

        except Exception as e:
            logger.error(f"ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # í´ë°±: ì¼ë°˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ì‚¬ìš©
            return await self.embed_text(text)
```

### 4. RAG ì—”ì§„

```python
# app/core/rag_engine.py
import logging
from typing import List, Dict, Any, Optional
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
import google.generativeai as genai

from app.core.embedding_manager import EmbeddingManager
from app.models.schemas import QueryRequest, QueryResponse, SearchResult

logger = logging.getLogger(__name__)

class RAGEngine:
    """RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ ì—”ì§„ - ê²€ìƒ‰ê³¼ ìƒì„±ì„ ë‹´ë‹¹"""

    def __init__(self, qdrant_host: str = "localhost", qdrant_port: int = 6333):
        self.embedding_manager = EmbeddingManager()
        self.vector_client = QdrantClient(host=qdrant_host, port=qdrant_port)

        # Gemini API ì„¤ì •
        genai.configure(api_key="YOUR_GEMINI_API_KEY")
        self.gemini_model = genai.GenerativeModel('gemini-pro')

    async def query(self, request: QueryRequest) -> QueryResponse:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
            question_embedding = await self.embedding_manager.embed_text(
                request.question,
                model_type='korean'
            )

            # 2. ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰
            search_results = await self._vector_search(
                question_embedding,
                request.user_id,
                limit=5,
                score_threshold=0.7
            )

            # 3. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ì‘ë‹µ
            if not search_results:
                return QueryResponse(
                    answer="ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    sources=[],
                    confidence=0.0
                )

            # 4. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(search_results)

            # 5. Geminië¡œ ìµœì¢… ë‹µë³€ ìƒì„±
            answer = await self._generate_answer_with_gemini(request.question, context)

            return QueryResponse(
                answer=answer,
                sources=[SearchResult(
                    document_id=result.id,
                    content=result.payload["content"][:200] + "...",
                    score=result.score,
                    metadata=result.payload.get("metadata", {})
                ) for result in search_results],
                confidence=max([r.score for r in search_results])
            )

        except Exception as e:
            logger.error(f"RAG ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return QueryResponse(
                answer="ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                sources=[],
                confidence=0.0
            )

    async def _vector_search(
        self,
        query_embedding: List[float],
        user_id: str,
        limit: int = 5,
        score_threshold: float = 0.7
    ) -> List[Any]:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
        try:
            search_result = self.vector_client.search(
                collection_name=f"documents_{user_id}",
                query_vector=query_embedding,
                limit=limit,
                score_threshold=score_threshold
            )
            return search_result

        except Exception as e:
            logger.error(f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _build_context(self, search_results: List[Any]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        for i, result in enumerate(search_results, 1):
            content = result.payload["content"]
            metadata = result.payload.get("metadata", {})

            context_part = f"[ë¬¸ì„œ {i}]"
            if metadata.get("page"):
                context_part += f" (í˜ì´ì§€ {metadata['page']})"
            if metadata.get("type"):
                context_part += f" ({metadata['type']})"
            context_part += f"\n{content}\n"

            context_parts.append(context_part)

        return "\n".join(context_parts)

    async def _generate_answer_with_gemini(self, question: str, context: str) -> str:
        """Geminië¥¼ ì‚¬ìš©í•´ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        try:
            prompt = f"""
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥´ê² ë‹¤ê³  ë‹µë³€í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:
"""

            response = self.gemini_model.generate_content(prompt)
            return response.text

        except Exception as e:
            logger.error(f"Gemini ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
```

### 5. OCR ì—”ì§„ (ì €í™”ì§ˆ ë„ë©´ ì²˜ë¦¬ìš©)

        encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'

        try:
            return raw_data.decode(encoding)
        except UnicodeDecodeError:
            return raw_data.decode('utf-8', errors='ignore')

    async def _generate_metadata(
        self,
        file_path: str,
        document_id: str,
        user_id: str
    ) -> Dict[str, Any]:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        path = Path(file_path)
        stat = path.stat()

        return {
            "document_id": document_id,
            "user_id": user_id,
            "filename": path.name,
            "file_type": path.suffix.lower(),
            "file_size": stat.st_size,
            "created_at": stat.st_ctime,
            "modified_at": stat.st_mtime,
        }

    async def _split_text(
        self,
        text: str,
        metadata: Dict[str, Any],
        strategy: str = "recursive"
    ) -> List[DocumentChunk]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """

        if strategy == "semantic":
            chunks = await self.semantic_splitter.split_text(text)
        else:
            chunks = self.text_splitter.split_text(text)

        document_chunks = []
        for i, chunk in enumerate(chunks):
            chunk_metadata = {
                **metadata,
                "chunk_index": i,
                "chunk_id": f"{metadata['document_id']}_chunk_{i}",
                "total_chunks": len(chunks)
            }

            document_chunks.append(DocumentChunk(
                content=chunk,
                metadata=chunk_metadata,
                chunk_index=i
            ))

        return document_chunks

````

### 3. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™

```python
# app/core/vector_store.py
import logging
from typing import List, Dict, Any, Optional
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue
from langchain_qdrant import Qdrant
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document

from app.core.config import settings
from app.models.schemas import DocumentChunk, SearchResult

logger = logging.getLogger(__name__)

class VectorStore:
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=settings.OPENAI_API_KEY,
            model="text-embedding-ada-002"
        )
        self.collection_name = settings.QDRANT_COLLECTION_NAME
        self.client: Optional[QdrantClient] = None
        self.vector_store: Optional[Qdrant] = None

    async def initialize(self):
        """Qdrant ì´ˆê¸°í™”"""
        try:
            self.client = QdrantClient(
                host=settings.QDRANT_HOST,
                port=settings.QDRANT_PORT,
                prefer_grpc=True
            )

            # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
            collections = await self.client.get_collections()
            collection_names = [col.name for col in collections.collections]

            if self.collection_name not in collection_names:
                logger.info(f"Creating Qdrant collection: {self.collection_name}")
                await self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=1536,  # OpenAI embedding dimension
                        distance=Distance.COSINE
                    )
                )

            self.vector_store = Qdrant(
                client=self.client,
                collection_name=self.collection_name,
                embeddings=self.embeddings
            )

            logger.info("Vector store initialized successfully")

        except Exception as e:
            logger.error(f"Error initializing vector store: {e}")
            raise

    async def add_documents(self, chunks: List[DocumentChunk]) -> bool:
        """ë¬¸ì„œ ì²­í¬ë“¤ì„ ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€"""
        try:
            documents = []
            for chunk in chunks:
                doc = Document(
                    page_content=chunk.content,
                    metadata=chunk.metadata
                )
                documents.append(doc)

            # ë²¡í„° ì €ì¥ì†Œì— ì¶”ê°€
            await self.vector_store.aadd_documents(documents)

            logger.info(f"Added {len(documents)} document chunks to vector store")
            return True

        except Exception as e:
            logger.error(f"Error adding documents to vector store: {e}")
            return False

    async def similarity_search(
        self,
        query: str,
        k: int = 5,
        user_id: Optional[str] = None,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[SearchResult]:
        """ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰"""
        try:
            # í•„í„° êµ¬ì„±
            search_filter = {}
            if user_id:
                search_filter["user_id"] = user_id
            if filter_metadata:
                search_filter.update(filter_metadata)

            # ê²€ìƒ‰ ì‹¤í–‰
            results = await self.vector_store.asimilarity_search_with_score(
                query=query,
                k=k,
                filter=search_filter if search_filter else None
            )

            search_results = []
            for doc, score in results:
                search_results.append(SearchResult(
                    content=doc.page_content,
                    metadata=doc.metadata,
                    relevance_score=float(1 - score)  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                ))

            logger.info(f"Similarity search returned {len(search_results)} results")
            return search_results

        except Exception as e:
            logger.error(f"Error in similarity search: {e}")
            return []

    async def delete_documents(self, document_id: str) -> bool:
        """íŠ¹ì • ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ì‚­ì œ"""
        try:
            # Qdrantì—ì„œ ë¬¸ì„œ IDë¡œ í•„í„°ë§í•˜ì—¬ ì‚­ì œ
            delete_filter = Filter(
                must=[
                    FieldCondition(
                        key="document_id",
                        match=MatchValue(value=document_id)
                    )
                ]
            )

            result = await self.client.delete(
                collection_name=self.collection_name,
                points_selector=delete_filter
            )

            logger.info(f"Deleted vectors for document: {document_id}, operation_id: {result.operation_id}")
            return True

        except Exception as e:
            logger.error(f"Error deleting document vectors: {e}")
            return False

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        # í•„ìš”í•œ ê²½ìš° ì •ë¦¬ ì‘ì—… ìˆ˜í–‰
        logger.info("Vector store connection closed")
````

### 4. RAG ì—”ì§„ êµ¬í˜„

```python
# app/core/rag_engine.py
import logging
from typing import List, Dict, Any, Optional, AsyncGenerator
from langchain.chat_models import ChatOpenAI
from langchain.schema import AIMessage, HumanMessage, SystemMessage
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate

from app.core.vector_store import VectorStore
from app.core.config import settings
from app.models.schemas import QueryRequest, QueryResponse, SearchResult
from app.services.cache_service import CacheService

logger = logging.getLogger(__name__)

class RAGEngine:
    def __init__(self, vector_store: VectorStore):
        self.vector_store = vector_store
        self.llm = ChatOpenAI(
            openai_api_key=settings.OPENAI_API_KEY,
            model_name="gpt-4-turbo-preview",
            temperature=0.1,
            streaming=True
        )
        self.cache_service = CacheService()

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
        self.system_prompt = SystemMessagePromptTemplate.from_template(
            """ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”
3. ë‹µë³€ì—ëŠ” ì¶œì²˜ ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”
4. ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”

ì»¨í…ìŠ¤íŠ¸:
{context}"""
        )

        self.human_prompt = HumanMessagePromptTemplate.from_template(
            "ì§ˆë¬¸: {question}"
        )

        self.chat_prompt = ChatPromptTemplate.from_messages([
            self.system_prompt,
            self.human_prompt
        ])

    async def query(self, request: QueryRequest) -> QueryResponse:
        """RAG ì¿¼ë¦¬ ì²˜ë¦¬"""
        try:
            # 1. ìºì‹œ í™•ì¸
            cache_key = f"query:{hash(request.query)}:{request.user_id}"
            cached_result = await self.cache_service.get(cache_key)
            if cached_result:
                logger.info("Returning cached result")
                return QueryResponse(**cached_result)

            # 2. ë²¡í„° ê²€ìƒ‰
            search_results = await self.vector_store.similarity_search(
                query=request.query,
                k=request.top_k or 5,
                user_id=request.user_id
            )

            if not search_results:
                return QueryResponse(
                    answer="ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    sources=[],
                    query=request.query
                )

            # 3. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(search_results)

            # 4. LLM ì¿¼ë¦¬
            answer = await self._generate_answer(request.query, context)

            # 5. ì‘ë‹µ êµ¬ì„±
            response = QueryResponse(
                answer=answer,
                sources=search_results,
                query=request.query,
                context_used=len(search_results)
            )

            # 6. ìºì‹œ ì €ì¥
            await self.cache_service.set(cache_key, response.dict(), ttl=3600)

            return response

        except Exception as e:
            logger.error(f"Error in RAG query: {e}")
            return QueryResponse(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                sources=[],
                query=request.query,
                error=str(e)
            )

    async def query_stream(self, request: QueryRequest) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° RAG ì¿¼ë¦¬"""
        try:
            # ë²¡í„° ê²€ìƒ‰
            search_results = await self.vector_store.similarity_search(
                query=request.query,
                k=request.top_k or 5,
                user_id=request.user_id
            )

            if not search_results:
                yield "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                return

            # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(search_results)

            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            async for chunk in self._generate_answer_stream(request.query, context):
                yield chunk

        except Exception as e:
            logger.error(f"Error in streaming RAG query: {e}")
            yield f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def _build_context(self, search_results: List[SearchResult]) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []

        for i, result in enumerate(search_results, 1):
            source_info = f"ë¬¸ì„œ: {result.metadata.get('filename', 'ì•Œ ìˆ˜ ì—†ìŒ')}"
            if 'chunk_index' in result.metadata:
                source_info += f" (ì„¹ì…˜ {result.metadata['chunk_index'] + 1})"

            context_parts.append(f"[ì¶œì²˜ {i}] {source_info}\n{result.content}")

        return "\n\n".join(context_parts)

    async def _generate_answer(self, query: str, context: str) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„±"""
        try:
            messages = self.chat_prompt.format_messages(
                context=context,
                question=query
            )

            response = await self.llm.agenerate([messages])
            return response.generations[0][0].text.strip()

        except Exception as e:
            logger.error(f"Error generating answer: {e}")
            raise

    async def _generate_answer_stream(self, query: str, context: str) -> AsyncGenerator[str, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±"""
        try:
            messages = self.chat_prompt.format_messages(
                context=context,
                question=query
            )

            async for chunk in self.llm.astream(messages[0].content):
                if hasattr(chunk, 'content'):
                    yield chunk.content

        except Exception as e:
            logger.error(f"Error in streaming generation: {e}")
            yield f"ìƒì„± ì˜¤ë¥˜: {str(e)}"

    async def get_conversation_context(
        self,
        conversation_id: str,
        max_messages: int = 5
    ) -> List[Dict[str, Any]]:
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°"""
        # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê°€ì ¸ì™€ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” ìºì‹œë¥¼ ì‚¬ìš©í•œ ì˜ˆì‹œ
        cache_key = f"conversation:{conversation_id}"
        conversation = await self.cache_service.get(cache_key)

        if conversation:
            return conversation.get("messages", [])[-max_messages:]

        return []
```

### 6. API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„

```python
# app/api/v1/documents.py - ë¬¸ì„œ ì²˜ë¦¬ API
from fastapi import APIRouter, HTTPException, UploadFile, File
from typing import Dict, Any
import logging

from app.core.document_processor import MultiModalDocumentProcessor
from app.core.vector_store import VectorStore
from app.models.schemas import ProcessingResult

logger = logging.getLogger(__name__)
router = APIRouter()

processor = MultiModalDocumentProcessor()
vector_store = VectorStore()

@router.post("/process-document")
async def process_document(
    file_path: str,
    user_id: str
) -> Dict[str, Any]:
    """Backendì—ì„œ í˜¸ì¶œí•˜ëŠ” ë¬¸ì„œ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        logger.info(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file_path} (ì‚¬ìš©ì: {user_id})")

        # ë©€í‹°ëª¨ë‹¬ ë¬¸ì„œ ì²˜ë¦¬
        result = await processor.process_document(file_path, user_id)

        # ë²¡í„° DBì— ì €ì¥
        collection_name = f"documents_{user_id}"
        await vector_store.store_embeddings(collection_name, result)

        return {
            "document_id": f"doc_{hash(file_path)}",
            "status": "processed",
            "text_chunks": len(result.text_chunks),
            "image_chunks": len(result.image_chunks),
            "total_embeddings": result.total_embeddings
        }

    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# app/api/v1/query.py - ì§ˆë¬¸ ì²˜ë¦¬ API
from fastapi import APIRouter, HTTPException
from typing import Dict, Any
import logging

from app.core.rag_engine import RAGEngine
from app.models.schemas import QueryRequest, QueryResponse

logger = logging.getLogger(__name__)
router = APIRouter()

rag_engine = RAGEngine()

@router.post("/query")
async def query(
    question: str,
    user_id: str
) -> Dict[str, Any]:
    """Backendì—ì„œ í˜¸ì¶œí•˜ëŠ” ì§ˆë¬¸ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬: {question[:50]}... (ì‚¬ìš©ì: {user_id})")

        request = QueryRequest(question=question, user_id=user_id)
        response = await rag_engine.query(request)

        return {
            "answer": response.answer,
            "sources": [
                {
                    "document_id": source.document_id,
                    "file_path": source.metadata.get("file_path", ""),
                    "relevance_score": source.score
                }
                for source in response.sources
            ],
            "processing_time": 1.2,  # ì‹¤ì œ ì¸¡ì • ê°’
            "confidence": response.confidence
        }

    except Exception as e:
        logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# app/main.py - ë©”ì¸ FastAPI ì•±
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import logging

from app.api.v1 import documents, query
from app.core.embedding_manager import EmbeddingManager
from app.core.vector_store import VectorStore

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="RAG Agent Service",
    description="ë©€í‹°ëª¨ë‹¬ RAG ì‹œìŠ¤í…œì˜ AI ì²˜ë¦¬ ì—”ì§„",
    version="1.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ê°œë°œìš©, í”„ë¡œë•ì…˜ì—ì„œëŠ” ì œí•œ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ê°ì²´ ì´ˆê¸°í™”
@app.on_event("startup")
async def startup_event():
    """ì•± ì‹œì‘ì‹œ ëª¨ë¸ ë¡œë”©"""
    logger.info("RAG Agent ì„œë¹„ìŠ¤ ì‹œì‘...")

    # ì„ë² ë”© ëª¨ë¸ ë¡œë”© (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    app.state.embedding_manager = EmbeddingManager()

    # ë²¡í„° DB ì—°ê²°
    app.state.vector_store = VectorStore()

    logger.info("RAG Agent ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ!")

# API ë¼ìš°í„° ë“±ë¡
app.include_router(documents.router, tags=["documents"])
app.include_router(query.router, tags=["query"])

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {"status": "healthy", "service": "rag-agent"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

# app/api/v1/documents.py
from fastapi import APIRouter, HTTPException, BackgroundTasks
import logging

from app.models.schemas import DocumentProcessRequest, ProcessingResult
from app.services.document_service import DocumentService

logger = logging.getLogger(__name__)
router = APIRouter()

@router.post("/process", response_model=ProcessingResult)
async def process_document(
    request: DocumentProcessRequest,
    background_tasks: BackgroundTasks
):
    """ë¬¸ì„œ ì²˜ë¦¬ ìš”ì²­"""
    try:
        service = DocumentService()

        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¬¸ì„œ ì²˜ë¦¬
        background_tasks.add_task(
            service.process_document_async,
            request.document_id,
            request.file_path,
            request.user_id
        )

        return ProcessingResult(
            document_id=request.document_id,
            status="processing",
            message="ë¬¸ì„œ ì²˜ë¦¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        )

    except Exception as e:
        logger.error(f"Error processing document: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/{document_id}/vectors")
async def delete_document_vectors(document_id: str):
    """ë¬¸ì„œ ë²¡í„° ì‚­ì œ"""
    try:
        service = DocumentService()
        success = await service.delete_document_vectors(document_id)

        if success:
            return {"message": "ë¬¸ì„œ ë²¡í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        logger.error(f"Error deleting document vectors: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ

```python
# app/utils/metrics.py
import time
import logging
from functools import wraps
from typing import Dict, Any, Callable
from prometheus_client import Counter, Histogram, Gauge, generate_latest

logger = logging.getLogger(__name__)

# Prometheus ë©”íŠ¸ë¦­ ì •ì˜
query_counter = Counter('rag_queries_total', 'Total RAG queries', ['user_id', 'status'])
query_duration = Histogram('rag_query_duration_seconds', 'RAG query duration')
document_processing_counter = Counter('documents_processed_total', 'Total documents processed', ['status'])
vector_search_duration = Histogram('vector_search_duration_seconds', 'Vector search duration')
active_connections = Gauge('active_websocket_connections', 'Active WebSocket connections')

class MetricsCollector:
    def __init__(self):
        self.start_time = time.time()

    def track_query(self, user_id: str, status: str):
        query_counter.labels(user_id=user_id, status=status).inc()

    def track_document_processing(self, status: str):
        document_processing_counter.labels(status=status).inc()

    def get_metrics(self) -> str:
        return generate_latest()

metrics = MetricsCollector()

def track_time(metric_histogram):
    """ì‹¤í–‰ ì‹œê°„ ì¶”ì  ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                duration = time.time() - start_time
                metric_histogram.observe(duration)

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                duration = time.time() - start_time
                metric_histogram.observe(duration)

        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    return decorator
```

### ìºì‹± ì „ëµ

```python
# app/services/cache_service.py
import json
import logging
from typing import Any, Optional
import aioredis
from app.core.config import settings

logger = logging.getLogger(__name__)

class CacheService:
    def __init__(self):
        self.redis: Optional[aioredis.Redis] = None

    async def initialize(self):
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        try:
            self.redis = await aioredis.from_url(
                settings.REDIS_URL,
                decode_responses=True
            )
            logger.info("Cache service initialized")
        except Exception as e:
            logger.error(f"Error initializing cache: {e}")
            self.redis = None

    async def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        if not self.redis:
            return None

        try:
            value = await self.redis.get(key)
            return json.loads(value) if value else None
        except Exception as e:
            logger.error(f"Error getting from cache: {e}")
            return None

    async def set(self, key: str, value: Any, ttl: int = 3600) -> bool:
        """ìºì‹œì— ê°’ ì €ì¥"""
        if not self.redis:
            return False

        try:
            await self.redis.setex(key, ttl, json.dumps(value))
            return True
        except Exception as e:
            logger.error(f"Error setting cache: {e}")
            return False

    async def delete(self, key: str) -> bool:
        """ìºì‹œì—ì„œ ê°’ ì‚­ì œ"""
        if not self.redis:
            return False

        try:
            await self.redis.delete(key)
            return True
        except Exception as e:
            logger.error(f"Error deleting from cache: {e}")
            return False
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸

```python
# tests/unit/test_rag_engine.py
import pytest
from unittest.mock import AsyncMock, MagicMock
from app.core.rag_engine import RAGEngine
from app.models.schemas import QueryRequest, SearchResult

@pytest.fixture
def mock_vector_store():
    store = AsyncMock()
    store.similarity_search.return_value = [
        SearchResult(
            content="í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ë‚´ìš©",
            metadata={"filename": "test.pdf", "chunk_index": 0},
            relevance_score=0.85
        )
    ]
    return store

@pytest.fixture
def rag_engine(mock_vector_store):
    engine = RAGEngine(mock_vector_store)
    engine.llm = AsyncMock()
    engine.llm.agenerate.return_value.generations = [[MagicMock(text="í…ŒìŠ¤íŠ¸ ë‹µë³€")]]
    return engine

@pytest.mark.asyncio
async def test_query_processing(rag_engine):
    request = QueryRequest(
        query="í…ŒìŠ¤íŠ¸ ì§ˆë¬¸",
        user_id="test-user",
        top_k=3
    )

    response = await rag_engine.query(request)

    assert response.answer == "í…ŒìŠ¤íŠ¸ ë‹µë³€"
    assert len(response.sources) == 1
    assert response.query == "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸"
```

### í†µí•© í…ŒìŠ¤íŠ¸

```python
# tests/integration/test_api.py
import pytest
from httpx import AsyncClient
from app.main import app

@pytest.mark.asyncio
async def test_query_endpoint():
    async with AsyncClient(app=app, base_url="http://test") as client:
        response = await client.post(
            "/api/v1/query/",
            json={
                "query": "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸",
                "user_id": "test-user"
            }
        )

    assert response.status_code == 200
    data = response.json()
    assert "answer" in data
    assert "sources" in data
```

## ğŸš€ ë°°í¬ ë° ìš´ì˜

### Docker ì„¤ì •

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³µì‚¬
COPY . .

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### í™˜ê²½ ì„¤ì •

```python
# app/core/config.py
from pydantic_settings import BaseSettings
from typing import List

class Settings(BaseSettings):
    # API ì„¤ì •
    DEBUG: bool = False
    ALLOWED_HOSTS: List[str] = ["*"]

    # OpenAI ì„¤ì •
    OPENAI_API_KEY: str

    # Qdrant ì„¤ì •
    QDRANT_HOST: str = "localhost"
    QDRANT_PORT: int = 6333
    QDRANT_COLLECTION_NAME: str = "rag_documents"

    # Redis ì„¤ì •
    REDIS_URL: str = "redis://localhost:6379"

    # ë¡œê¹… ì„¤ì •
    LOG_LEVEL: str = "INFO"

    class Config:
        env_file = ".env"

settings = Settings()
```

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ê°œë°œ ì™„ë£Œ ê¸°ì¤€

- [ ] FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ê¸°ë³¸ êµ¬ì¡° ì™„ì„±
- [ ] ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ì™„ë£Œ
- [ ] ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ ì™„ë£Œ
- [ ] RAG ì¿¼ë¦¬ ì—”ì§„ êµ¬í˜„ ì™„ë£Œ
- [ ] API ì—”ë“œí¬ì¸íŠ¸ ëª¨ë‘ êµ¬í˜„
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë° í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] Docker ì»¨í…Œì´ë„ˆí™” ì™„ë£Œ
- [ ] í™˜ê²½ ì„¤ì • ë° ë°°í¬ ì¤€ë¹„ ì™„ë£Œ
- [ ] API ë¬¸ì„œí™” ì™„ë£Œ
